Installation
------------

KubeVirt is a virtualization add-on to Kubernetes and this guide assumes
that a Kubernetes cluster is already installed.

Requirements
~~~~~~~~~~~~

A few requirements need to be met before you can begin:

* https://kubernetes.io[Kubernetes] cluster
(https://github.com/openshift/origin[OpenShift], Tectonic)
* `kubectl` client utility
* `git`

Minimum Cluster Requirements
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Kubernetes 1.10 or later is required to run KubeVirt.

In addition it can be that feature gates need to be opened.

Runtime
+++++++

KubeVirt is currently support on the following container runtimes:

* docker
* crio (with runv)

Virtualization support
^^^^^^^^^^^^^^^^^^^^^^

There are several distributions of Kubernetes, you need to decide on one
and deploy it.

Hardware with virtualization support is recommended. You can use
virt-host-validate to ensure that your hosts are capable of running
virtualization workloads:

[source,bash]
----
$ virt-host-validate qemu
  QEMU: Checking for hardware virtualization                                 : PASS
  QEMU: Checking if device /dev/kvm exists                                   : PASS
  QEMU: Checking if device /dev/kvm is accessible                            : PASS
  QEMU: Checking if device /dev/vhost-net exists                             : PASS
  QEMU: Checking if device /dev/net/tun exists                               : PASS
...
----

Software emulation
++++++++++++++++++

If hardware virtualization is not available, then a
https://github.com/kubevirt/kubevirt/blob/master/docs/software-emulation.md[software
emulation fallback] can be enabled using:

....
$ kubectl create namespace kubevirt
$ kubectl create configmap -n kubevirt kubevirt-config \
    --from-literal debug.useEmulation=true
....

This ConfigMap needs to be created before deployment or the
virt-controller deployment has to be restarted.

Hugepages support
^^^^^^^^^^^^^^^^^

For hugepages support you need at least Kubernetes version `1.9`.

Enable feature-gate
+++++++++++++++++++

To enable hugepages on Kubernetes, check the
https://kubernetes.io/docs/tasks/manage-hugepages/scheduling-hugepages/[official
documentation].

To enable hugepages on OpenShift, check the
https://docs.openshift.org/3.9/scaling_performance/managing_hugepages.html#huge-pages-prerequisites[official
documentation].

Pre-allocate hugepages on a node
++++++++++++++++++++++++++++++++

To pre-allocate hugepages on boot time, you will need to specify
hugepages under kernel boot parameters `hugepagesz=2M hugepages=64` and
restart your machine.

You can find more about hugepages under
https://www.kernel.org/doc/Documentation/vm/hugetlbpage.txt[official
documentation].

Cluster side add-on deployment
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Core components
^^^^^^^^^^^^^^^

Once Kubernetes is deployed, you will need to deploy the KubeVirt
add-on.

The installation uses the KubeVirt operator, which manages lifecycle of all KubeVirt core components:

[source,bash]
----
$ export RELEASE=v0.17.0
# creates KubeVirt operator
$ kubectl apply -f https://github.com/kubevirt/kubevirt/releases/download/${RELEASE}/kubevirt-operator.yaml
# creates KubeVirt KV custom resource
$ kubectl apply -f https://github.com/kubevirt/kubevirt/releases/download/${RELEASE}/kubevirt-cr.yaml
# wait until all KubeVirt components is up
$ kubectl -n kubevirt wait kv kubevirt --for condition=Available
----
_____________________________
Note: Prior to release v0.20.0 the condition for the `kubectl wait` command was named "Ready" instead of "Available"
_____________________________

All new components will be deployed under the `kubevirt` namespace:

[source,bash]
----
kubectl get pods -n kubevirt
NAME                                           READY     STATUS        RESTARTS   AGE
virt-api-6d4fc3cf8a-b2ere                      1/1       Running       0          1m
virt-controller-5d9fc8cf8b-n5trt               1/1       Running       0          1m
virt-handler-vwdjx                             1/1       Running       0          1m
...
----

Restricting virt-handler DaemonSet
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

You can patch the `virt-handler` DaemonSet post-deployment to restrict it to a specific subset of nodes with
a nodeSelector. For example, to restrict the DaemonSet to only nodes with the "region=primary" label:

[source, bash]
----
kubectl patch ds/virt-handler -n kubevirt -p '{"spec": {"template": {"spec": {"nodeSelector": {"region": "primary"}}}}}'
----

Deploying on OpenShift
~~~~~~~~~~~~~~~~~~~~~~

The following
https://docs.openshift.com/container-platform/3.11/admin_guide/manage_scc.html[SCC]
needs to be added prior KubeVirt deployment:

[source,bash]
----
$ oc adm policy add-scc-to-user privileged -n kubevirt -z kubevirt-operator
----

Once privileges are granted, the KubeVirt can be deployed as described above.

Client side `virtctl` deployment
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Basic VirtualMachineInstance operations can be performed with the stock
`kubectl` utility. However, the `virtctl` binary utility is required to
use advanced features such as:

* Serial and graphical console access

It also provides convenience commands for:

* Starting and stopping VirtualMachineInstances
* Live migrating VirtualMachineInstances

There are two ways to get it:

* the most recent version of the tool can be retrieved from the
https://github.com/kubevirt/kubevirt/releases[official release page]
* it can be installed as a `kubectl` plugin using https://krew.dev/[krew]

===== Install `virtctl` with `krew`

It is required to https://github.com/kubernetes-sigs/krew/#installation[install `krew` plugin manager] beforehand.
If `krew` is installed, `virtctl` can be installed via `krew`:

[source,bash]
----
$ kubectl krew install virt
----

Then `virtctl` can be used as a kubectl plugin. For a list of available commands run:

[source,bash]
----
$ kubectl virt help
----

Every occurrence throughout this guide of

[source,bash]
----
$ ./virtctl <command>...
----

should then be read as

[source,bash]
----
$ kubectl virt <command>...
----


From Service Catalog as an APB
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

You can find KubeVirt in the OpenShift Service Catalog and install it
from there. In order to do that please follow the documentation in the
https://github.com/ansibleplaybookbundle/kubevirt-apb[KubeVirt APB
repository].

Using Ansible playbooks
^^^^^^^^^^^^^^^^^^^^^^^

The https://github.com/kubevirt/kubevirt-ansible[kubevirt-ansible]
project provides a collection of playbooks that installs KubeVirt and
itâ€™s related components on top of OpenShift or Kubernetes clusters.

Deploying from Source
~~~~~~~~~~~~~~~~~~~~~

See the
https://github.com/kubevirt/kubevirt/blob/master/docs/getting-started.md[Developer
Getting Started Guide] to understand how to build and deploy KubeVirt
from source.

Installing network plugins (optional)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

KubeVirt alone does not bring any additional network plugins, it just
allows user to utilize them. If you want to attach your VMs to multiple
networks (Multus CNI) or have full control over L2 (OVS CNI), you need
to deploy respective network plugins. For more information, refer to
https://github.com/kubevirt/ovs-cni/blob/master/docs/deployment-on-arbitrary-cluster.md[OVS
CNI installation guide].

______________________________________________________________________________________________________________________________________________________
Note: KubeVirt Ansible
https://github.com/kubevirt/kubevirt-ansible/tree/master/playbooks#network[network
playbook] installs these plugins by default.
______________________________________________________________________________________________________________________________________________________

Installing Web User Interface (optional)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
When the KubeVirt is installed on OpenShift, the Web User Interface can be used to administer the virtual
machines and other entities in the cluster in addition to the command line tools.

The Web UI is installed by default within the kubevirt-ansible flow as described above.

The Web UI URL can be retrieved from its Route object:
----
$ oc get route console -n kubevirt-web-ui
----

For manual installation, see https://github.com/kubevirt/web-ui-operator/blob/master/README.md[the web-ui-operator README] file.

For additional details, see the https://github.com/kubevirt/web-ui[Web User Interface project].

______________________________________________________________________________________________________________________________________________________
Note: Kubevirt Web UI ansible playbook can be found https://github.com/kubevirt/kubevirt-ansible/blob/master/playbooks/kubevirt_web_ui.yml[here].
______________________________________________________________________________________________________________________________________________________

Update
~~~~~~

Zero downtime rolling updates are supported starting with release `v0.17.0`
onward. Updating from any release prior to the KubeVirt `v0.17.0` release is
not supported.
 
Updates are triggered one of two ways.

1. By changing the imageTag value in the KubeVirt CR's spec.

For example, updating from `v0.17.0-alpha.1` to `v0.17.0` is as simple as
patching the KubeVirt CR with the `imageTag: v0.17.0` value. From there the
KubeVirt operator will begin the process of rolling out the new version of
KubeVirt. Existing VM/VMIs will remain uninterrupted both during and after
the update succeeds.

[source,bash]
----
$ kubectl patch kv kubevirt -n kubevirt --type=json -p '[{ "op": "add", "path": "/spec/imageTag", "value": "v0.17.0" }]'
----

2. Or, by updating the kubevirt operator if no imageTag value is set.

When no imageTag value is set in the kubevirt CR, the system assumes that the
version of KubeVirt is locked to the version of the operator. This means that
updating the operator will result in the underlying KubeVirt installation being
updated as well.

[source,bash]
----
$ export RELEASE=v0.17.0
$ kubectl apply -f https://github.com/kubevirt/kubevirt/releases/download/${RELEASE}/kubevirt-operator.yaml
----


The first way provides a fine granular approach where you have full control
over what version of KubeVirt is installed independently of what version of
the KubeVirt operator you might be running. The second approach allows you to
lock both the operator and operand to the same version.

Newer KubeVirt may require additional or extended RBAC rules. In this case, the #1 update method may fail,
because the virt-operator present in the cluster doesn't have these RBAC rules itself.
In this case, you need to update the `virt-operator` first, and then proceed to update kubevirt.
See https://github.com/kubevirt/kubevirt/issues/2533[this issue for more details].

Delete
~~~~~~

To delete the KubeVirt you should first to delete `KubeVirt` custom resource and then delete the KubeVirt operator:

[source,bash]
----
$ export RELEASE=v0.17.0
$ kubectl delete -f https://github.com/kubevirt/kubevirt/releases/download/${RELEASE}/kubevirt-cr.yaml
$ kubectl delete -f https://github.com/kubevirt/kubevirt/releases/download/${RELEASE}/kubevirt-operator.yaml
----

______________________________________________________________________________________________________________________________________________________
Note: If by mistake you deleted the operator first, the KV custom resource will stuck in the `Terminating` state,
to fix it, delete manually finalizer from the resource.

[source,bash]
----
$ kubectl -n kubevirt patch kv kubevirt --type=json -p '[{ "op": "remove", "path": "/metadata/finalizers" }]'
----

Note: If you find your namespace is still hanging in terminationstate this script will delete all objects inside of a namespace.
[source,bash]
```
# the namespace of kubevirt installation
export namespace=kubevirt
export labels=("operator.kubevirt.io" "operator.cdi.kubevirt.io" "kubevirt.io" "cdi.kubevirt.io")
export namespaces=(default ${namespace} "<other-namespaces that has kubevirst resources>")

kubectl -n ${namespace} delete kv kubevirt
kubectl -n ${namespace} patch kv kubevirt --type=json -p '[{ "op": "remove", "path": "/metadata/finalizers" }]'
kubectl get vmis --all-namespaces -o=custom-columns=NAME:.metadata.name,NAMESPACE:.metadata.namespace,FINALIZERS:.metadata.finalizers --no-headers | grep foregroundDeleteVirtualMachine | while read p; do
    arr=($p)
    name="${arr[0]}"
    namespace="${arr[1]}"
    kubectl patch vmi $name -n $namespace --type=json -p '[{ "op": "remove", "path": "/metadata/finalizers" }]'
done

for i in ${namespaces[@]}; do
    for label in ${labels[@]}; do
        kubectl -n ${i} delete deployment -l ${label}
        kubectl -n ${i} delete ds -l ${label}
        kubectl -n ${i} delete rs -l ${label}
        kubectl -n ${i} delete pods -l ${label}
        kubectl -n ${i} delete services -l ${label}
        kubectl -n ${i} delete pvc -l ${label}
        kubectl -n ${i} delete rolebinding -l ${label}
        kubectl -n ${i} delete roles -l ${label}
        kubectl -n ${i} delete serviceaccounts -l ${label}
        kubectl -n ${i} delete configmaps -l ${label}
        kubectl -n ${i} delete secrets -l ${label}
        kubectl -n ${i} delete jobs -l ${label}
    done
done

for label in ${labels[@]}; do
    kubectl delete validatingwebhookconfiguration -l ${label}
    kubectl delete pv -l ${label}
    kubectl delete clusterrolebinding -l ${label}
    kubectl delete clusterroles -l ${label}
    kubectl delete customresourcedefinitions -l ${label}
    kubectl delete scc -l ${label}
    kubectl delete apiservices -l ${label}

    kubectl get apiservices -l ${label} -o=custom-columns=NAME:.metadata.name,FINALIZERS:.metadata.finalizers --no-headers | grep foregroundDeletion | while read p; do
        arr=($p)
        name="${arr[0]}"
        kubectl -n ${i} patch apiservices $name --type=json -p '[{ "op": "remove", "path": "/metadata/finalizers" }]'
    done
done
```
______________________________________________________________________________________________________________________________________________________
