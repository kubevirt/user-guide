{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome \u00b6 This page is provided as the entrypoint to the different topics of this user-guide. Try it out \u00b6 Kubevirt on katacoda: https://www.katacoda.com/kubevirt Kubevirt on minikube: https://kubevirt.io/quickstart_minikube/ Kubevirt on kind: https://kubevirt.io/quickstart_kind/ Kubevirt on cloud providers: https://kubevirt.io/quickstart_cloud/ Getting help \u00b6 File a bug: https://github.com/kubevirt/kubevirt/issues Mailing list: https://groups.google.com/forum/#!forum/kubevirt-dev Slack: https://kubernetes.slack.com/messages/virtualization Developer \u00b6 Start contributing: Appendix/Contributing API Reference: http://kubevirt.io/api-reference/ Privacy \u00b6 Check our privacy policy at: https://kubevirt.io/privacy/ We do use https://netlify.com Open Source Plan for Rendering Pull Requests to the documentation repository","title":"Welcome"},{"location":"#welcome","text":"This page is provided as the entrypoint to the different topics of this user-guide.","title":"Welcome"},{"location":"#try-it-out","text":"Kubevirt on katacoda: https://www.katacoda.com/kubevirt Kubevirt on minikube: https://kubevirt.io/quickstart_minikube/ Kubevirt on kind: https://kubevirt.io/quickstart_kind/ Kubevirt on cloud providers: https://kubevirt.io/quickstart_cloud/","title":"Try it out"},{"location":"#getting-help","text":"File a bug: https://github.com/kubevirt/kubevirt/issues Mailing list: https://groups.google.com/forum/#!forum/kubevirt-dev Slack: https://kubernetes.slack.com/messages/virtualization","title":"Getting help"},{"location":"#developer","text":"Start contributing: Appendix/Contributing API Reference: http://kubevirt.io/api-reference/","title":"Developer"},{"location":"#privacy","text":"Check our privacy policy at: https://kubevirt.io/privacy/ We do use https://netlify.com Open Source Plan for Rendering Pull Requests to the documentation repository","title":"Privacy"},{"location":"architecture/","text":"Architecture \u00b6 KubeVirt is built using a service oriented architecture and a choreography pattern. Stack \u00b6 +---------------------+ | KubeVirt | ~~+---------------------+~~ | Orchestration (K8s) | +---------------------+ | Scheduling (K8s) | +---------------------+ | Container Runtime | ~~+---------------------+~~ | Operating System | +---------------------+ | Virtual(kvm) | ~~+---------------------+~~ | Physical | +---------------------+ Users requiring virtualization services are speaking to the Virtualization API (see below) which in turn is speaking to the Kubernetes cluster to schedule requested Virtual Machine Instances (VMIs). Scheduling, networking, and storage are all delegated to Kubernetes, while KubeVirt provides the virtualization functionality. Additional Services \u00b6 KubeVirt provides additional functionality to your Kubernetes cluster, to perform virtual machine management If we recall how Kubernetes is handling Pods, then we remember that Pods are created by posting a Pod specification to the Kubernetes API Server. This specification is then transformed into an object inside the API Server, this object is of a specific type or kind - that is how it's called in the specification. A Pod is of the type Pod . Controllers within Kubernetes know how to handle these Pod objects. Thus once a new Pod object is seen, those controllers perform the necessary actions to bring the Pod alive, and to match the required state. This same mechanism is used by KubeVirt. Thus KubeVirt delivers three things to provide the new functionality: Additional types - so called Custom Resource Definition (CRD) - are added to the Kubernetes API Additional controllers for cluster wide logic associated with these new types Additional daemons for node specific logic associated with new types Once all three steps have been completed, you are able to create new objects of these new types in Kubernetes (VMIs in our case) and the new controllers take care to get the VMIs scheduled on some host, and a daemon - the virt-handler - is taking care of a host - alongside the kubelet - to launch the VMI and configure it until it matches the required state. One final note; both controllers and daemons are running as Pods (or similar) on top of the Kubernetes cluster, and are not installed alongside it. The type is - as said before - even defined inside the Kubernetes API server. This allows users to speak to Kubernetes, but modify VMIs. The following diagram illustrates how the additional controllers and daemons communicate with Kubernetes and where the additional types are stored: And a simplified version: Application Layout \u00b6 Cluster KubeVirt Components virt-controller virt-handler libvirtd \u2026 KubeVirt Managed Pods VMI Foo VMI Bar \u2026 KubeVirt Custom Resources VirtualMachine (VM) Foo -> VirtualMachineInstance (VMI) Foo VirtualMachineInstanceReplicaSet (VMIRS) Bar -> VirtualMachineInstance (VMI) Bar VirtualMachineInstance (VMI) is the custom resource that represents the basic ephemeral building block of an instance. In a lot of cases this object won't be created directly by the user but by a high level resource. High level resources for VMI can be: * VirtualMachine (VM) - StateFul VM that can be stopped and started while keeping the VM data and state. * VirtualMachineInstanceReplicaSet (VMIRS) - Similar to pods ReplicaSet, a group of ephemeral VMIs with similar configuration defined in a template. Native Workloads \u00b6 KubeVirt is deployed on top of a Kubernetes cluster. This means that you can continue to run your Kubernetes-native workloads next to the VMIs managed through KubeVirt. Furthermore: if you can run native workloads, and you have KubeVirt installed, you should be able to run VM-based workloads, too. For example, Application Operators should not require additional permissions to use cluster features for VMs, compared to using that feature with a plain Pod. Security-wise, installing and using KubeVirt must not grant users any permission they do not already have regarding native workloads. For example, a non-privileged Application Operator must never gain access to a privileged Pod by using a KubeVirt feature. The Razor \u00b6 We love virtual machines, think that they are very important and work hard to make them easy to use in Kubernetes. But even more than VMs, we love good design and modular, reusable components. Quite frequently, we face a dilemma: should we solve a problem in KubeVirt in a way that is best optimized for VMs, or should we take a longer path and introduce the solution to Pod-based workloads too? To decide these dilemmas we came up with the KubeVirt Razor : \"If something is useful for Pods, we should not implement it only for VMs\". For example, we debated how we should connect VMs to external network resources. The quickest way seems to introduce KubeVirt-specific code, attaching a VM to a host bridge. However, we chose the longer path of integrating with Multus and CNI and improving them. VirtualMachine \u00b6 A VirtualMachine provides additional management capabilities to a VirtualMachineInstance inside the cluster. That includes: ABI stability Start/stop/restart capabilities on the controller level Offline configuration change with propagation on VirtualMachineInstance recreation Ensure that the VirtualMachineInstance is running if it should be running It focuses on a 1:1 relationship between the controller instance and a virtual machine instance. In many ways it is very similar to a StatefulSet with spec.replica set to 1 . How to use a VirtualMachine \u00b6 A VirtualMachine will make sure that a VirtualMachineInstance object with an identical name will be present in the cluster, if spec.running is set to true . Further it will make sure that a VirtualMachineInstance will be removed from the cluster if spec.running is set to false . There exists a field spec.runStrategy which can also be used to control the state of the associated VirtualMachineInstance object. To avoid confusing and contradictory states, these fields are mutually exclusive. An extended explanation of spec.runStrategy vs spec.running can be found in Run Strategies Starting and stopping \u00b6 After creating a VirtualMachine it can be switched on or off like this: # Start the virtual machine: virtctl start vm # Stop the virtual machine: virtctl stop vm kubectl can be used too: # Start the virtual machine: kubectl patch virtualmachine vm --type merge -p \\ '{\"spec\":{\"running\":true}}' # Stop the virtual machine: kubectl patch virtualmachine vm --type merge -p \\ '{\"spec\":{\"running\":false}}' Controller status \u00b6 Once a VirtualMachineInstance is created, its state will be tracked via status.created and status.ready fields of the VirtualMachine. If a VirtualMachineInstance exists in the cluster, status.created will equal true . If the VirtualMachineInstance is also ready, status.ready will equal true too. If a VirtualMachineInstance reaches a final state but the spec.running equals true , the VirtualMachine controller will set status.ready to false and re-create the VirtualMachineInstance. Additionally, the status.printableStatus field provides high-level summary information about the state of the VirtualMachine. This information is also displayed when listing VirtualMachines using the CLI: $ kubectl get virtualmachines NAME AGE STATUS VOLUME vm1 4m Running vm2 11s Stopped Here's the list of states currently supported and their meanings. Note that states may be added/removed in future releases, so caution should be used if consumed by automated programs. Stopped : The virtual machine is currently stopped and isn't expected to start. Provisioning : Cluster resources associated with the virtual machine (e.g., DataVolumes) are being provisioned and prepared. Starting : The virtual machine is being prepared for running. Running : The virtual machine is running. Paused : The virtual machine is paused. Migrating : The virtual machine is in the process of being migrated to another host. Stopping : The virtual machine is in the process of being stopped. Terminating : The virtual machine is in the process of deletion, as well as its associated resources (VirtualMachineInstance, DataVolumes, \u2026). Unknown : The state of the virtual machine could not be obtained, typically due to an error in communicating with the host on which it's running. Restarting \u00b6 A VirtualMachineInstance restart can be triggered by deleting the VirtualMachineInstance. This will also propagate configuration changes from the template in the VirtualMachine: # Restart the virtual machine (you delete the instance!): kubectl delete virtualmachineinstance vm To restart a VirtualMachine named vm using virtctl: $ virtctl restart vm This would perform a normal restart for the VirtualMachineInstance and would reschedule the VirtualMachineInstance on a new virt-launcher Pod To force restart a VirtualMachine named vm using virtctl: $ virtctl restart vm --force --grace-period=0 This would try to perform a normal restart, and would also delete the virt-launcher Pod of the VirtualMachineInstance with setting GracePeriodSeconds to the seconds passed in the command. Currently, only setting grace-period=0 is supported. Note: Force restart can cause data corruption, and should be used in cases of kernel panic or VirtualMachine being unresponsive to normal restarts. Fencing considerations \u00b6 A VirtualMachine will never restart or re-create a VirtualMachineInstance until the current instance of the VirtualMachineInstance is deleted from the cluster. Exposing as a Service \u00b6 A VirtualMachine can be exposed as a service. The actual service will be available once the VirtualMachineInstance starts without additional interaction. For example, exposing SSH port (22) as a ClusterIP service using virtctl after the VirtualMachine was created, but before it started: $ virtctl expose virtualmachine vmi-ephemeral --name vmiservice --port 27017 --target-port 22 All service exposure options that apply to a VirtualMachineInstance apply to a VirtualMachine. See Service Objects for more details. When to use a VirtualMachine \u00b6 When ABI stability is required between restarts \u00b6 A VirtualMachine makes sure that VirtualMachineInstance ABI configurations are consistent between restarts. A classical example are licenses which are bound to the firmware UUID of a virtual machine. The VirtualMachine makes sure that the UUID will always stay the same without the user having to take care of it. One of the main benefits is that a user can still make use of defaulting logic, although a stable ABI is needed. When config updates should be picked up on the next restart \u00b6 If the VirtualMachineInstance configuration should be modifiable inside the cluster and these changes should be picked up on the next VirtualMachineInstance restart. This means that no hotplug is involved. When you want to let the cluster manage your individual VirtualMachineInstance \u00b6 Kubernetes as a declarative system can help you to manage the VirtualMachineInstance. You tell it that you want this VirtualMachineInstance with your application running, the VirtualMachine will try to make sure it stays running. Note: The current belief is that if it is defined that the VirtualMachineInstance should be running, it should be running. This is different to many classical virtualization platforms, where VMs stay down if they were switched off. Restart policies may be added if needed. Please provide your use-case if you need this! Example \u00b6 apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachine metadata: labels: kubevirt.io/vm: vm-cirros name: vm-cirros spec: running: false template: metadata: labels: kubevirt.io/vm: vm-cirros spec: domain: devices: disks: - disk: bus: virtio name: containerdisk - disk: bus: virtio name: cloudinitdisk machine: type: \"\" resources: requests: memory: 64M terminationGracePeriodSeconds: 0 volumes: - name: containerdisk containerDisk: image: kubevirt/cirros-container-disk-demo:latest - cloudInitNoCloud: userDataBase64: IyEvYmluL3NoCgplY2hvICdwcmludGVkIGZyb20gY2xvdWQtaW5pdCB1c2VyZGF0YScK name: cloudinitdisk Saving this manifest into vm.yaml and submitting it to Kubernetes will create the controller instance: $ kubectl create -f vm.yaml virtualmachine \"vm-cirros\" created Since spec.running is set to false , no vmi will be created: $ kubectl get vmis No resources found. Let's start the VirtualMachine: $ virtctl start vm vm-cirros As expected, a VirtualMachineInstance called vm-cirros got created: $ kubectl describe vm vm-cirros Name: vm-cirros Namespace: default Labels: kubevirt.io/vm=vm-cirros Annotations: <none> API Version: kubevirt.io/v1alpha3 Kind: VirtualMachine Metadata: Cluster Name: Creation Timestamp: 2018-04-30T09:25:08Z Generation: 0 Resource Version: 6418 Self Link: /apis/kubevirt.io/v1alpha3/namespaces/default/virtualmachines/vm-cirros UID: 60043358-4c58-11e8-8653-525500d15501 Spec: Running: true Template: Metadata: Creation Timestamp: <nil> Labels: Kubevirt . Io / Ovmi: vm-cirros Spec: Domain: Devices: Disks: Disk: Bus: virtio Name: containerdisk Volume Name: containerdisk Disk: Bus: virtio Name: cloudinitdisk Volume Name: cloudinitdisk Machine: Type: Resources: Requests: Memory: 64M Termination Grace Period Seconds: 0 Volumes: Name: containerdisk Registry Disk: Image: kubevirt/cirros-registry-disk-demo:latest Cloud Init No Cloud: User Data Base 64: IyEvYmluL3NoCgplY2hvICdwcmludGVkIGZyb20gY2xvdWQtaW5pdCB1c2VyZGF0YScK Name: cloudinitdisk Status: Created: true Ready: true Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal SuccessfulCreate 15s virtualmachine-controller Created virtual machine: vm-cirros Kubectl commandline interactions \u00b6 Whenever you want to manipulate the VirtualMachine through the commandline you can use the kubectl command. The following are examples demonstrating how to do it. # Define a virtual machine: kubectl create -f vm.yaml # Start the virtual machine: kubectl patch virtualmachine vm --type merge -p \\ '{\"spec\":{\"running\":true}}' # Look at virtual machine status and associated events: kubectl describe virtualmachine vm # Look at the now created virtual machine instance status and associated events: kubectl describe virtualmachineinstance vm # Stop the virtual machine instance: kubectl patch virtualmachine vm --type merge -p \\ '{\"spec\":{\"running\":false}}' # Restart the virtual machine (you delete the instance!): kubectl delete virtualmachineinstance vm # Implicit cascade delete (first deletes the virtual machine and then the virtual machine instance) kubectl delete virtualmachine vm # Explicit cascade delete (first deletes the virtual machine and then the virtual machine instance) kubectl delete virtualmachine vm --cascade=true # Orphan delete (The running virtual machine is only detached, not deleted) # Recreating the virtual machine would lead to the adoption of the virtual machine instance kubectl delete virtualmachine vm --cascade=false","title":"Architecture"},{"location":"architecture/#architecture","text":"KubeVirt is built using a service oriented architecture and a choreography pattern.","title":"Architecture"},{"location":"architecture/#stack","text":"+---------------------+ | KubeVirt | ~~+---------------------+~~ | Orchestration (K8s) | +---------------------+ | Scheduling (K8s) | +---------------------+ | Container Runtime | ~~+---------------------+~~ | Operating System | +---------------------+ | Virtual(kvm) | ~~+---------------------+~~ | Physical | +---------------------+ Users requiring virtualization services are speaking to the Virtualization API (see below) which in turn is speaking to the Kubernetes cluster to schedule requested Virtual Machine Instances (VMIs). Scheduling, networking, and storage are all delegated to Kubernetes, while KubeVirt provides the virtualization functionality.","title":"Stack"},{"location":"architecture/#additional-services","text":"KubeVirt provides additional functionality to your Kubernetes cluster, to perform virtual machine management If we recall how Kubernetes is handling Pods, then we remember that Pods are created by posting a Pod specification to the Kubernetes API Server. This specification is then transformed into an object inside the API Server, this object is of a specific type or kind - that is how it's called in the specification. A Pod is of the type Pod . Controllers within Kubernetes know how to handle these Pod objects. Thus once a new Pod object is seen, those controllers perform the necessary actions to bring the Pod alive, and to match the required state. This same mechanism is used by KubeVirt. Thus KubeVirt delivers three things to provide the new functionality: Additional types - so called Custom Resource Definition (CRD) - are added to the Kubernetes API Additional controllers for cluster wide logic associated with these new types Additional daemons for node specific logic associated with new types Once all three steps have been completed, you are able to create new objects of these new types in Kubernetes (VMIs in our case) and the new controllers take care to get the VMIs scheduled on some host, and a daemon - the virt-handler - is taking care of a host - alongside the kubelet - to launch the VMI and configure it until it matches the required state. One final note; both controllers and daemons are running as Pods (or similar) on top of the Kubernetes cluster, and are not installed alongside it. The type is - as said before - even defined inside the Kubernetes API server. This allows users to speak to Kubernetes, but modify VMIs. The following diagram illustrates how the additional controllers and daemons communicate with Kubernetes and where the additional types are stored: And a simplified version:","title":"Additional Services"},{"location":"architecture/#application-layout","text":"Cluster KubeVirt Components virt-controller virt-handler libvirtd \u2026 KubeVirt Managed Pods VMI Foo VMI Bar \u2026 KubeVirt Custom Resources VirtualMachine (VM) Foo -> VirtualMachineInstance (VMI) Foo VirtualMachineInstanceReplicaSet (VMIRS) Bar -> VirtualMachineInstance (VMI) Bar VirtualMachineInstance (VMI) is the custom resource that represents the basic ephemeral building block of an instance. In a lot of cases this object won't be created directly by the user but by a high level resource. High level resources for VMI can be: * VirtualMachine (VM) - StateFul VM that can be stopped and started while keeping the VM data and state. * VirtualMachineInstanceReplicaSet (VMIRS) - Similar to pods ReplicaSet, a group of ephemeral VMIs with similar configuration defined in a template.","title":"Application Layout"},{"location":"architecture/#native-workloads","text":"KubeVirt is deployed on top of a Kubernetes cluster. This means that you can continue to run your Kubernetes-native workloads next to the VMIs managed through KubeVirt. Furthermore: if you can run native workloads, and you have KubeVirt installed, you should be able to run VM-based workloads, too. For example, Application Operators should not require additional permissions to use cluster features for VMs, compared to using that feature with a plain Pod. Security-wise, installing and using KubeVirt must not grant users any permission they do not already have regarding native workloads. For example, a non-privileged Application Operator must never gain access to a privileged Pod by using a KubeVirt feature.","title":"Native Workloads"},{"location":"architecture/#the-razor","text":"We love virtual machines, think that they are very important and work hard to make them easy to use in Kubernetes. But even more than VMs, we love good design and modular, reusable components. Quite frequently, we face a dilemma: should we solve a problem in KubeVirt in a way that is best optimized for VMs, or should we take a longer path and introduce the solution to Pod-based workloads too? To decide these dilemmas we came up with the KubeVirt Razor : \"If something is useful for Pods, we should not implement it only for VMs\". For example, we debated how we should connect VMs to external network resources. The quickest way seems to introduce KubeVirt-specific code, attaching a VM to a host bridge. However, we chose the longer path of integrating with Multus and CNI and improving them.","title":"The Razor"},{"location":"architecture/#virtualmachine","text":"A VirtualMachine provides additional management capabilities to a VirtualMachineInstance inside the cluster. That includes: ABI stability Start/stop/restart capabilities on the controller level Offline configuration change with propagation on VirtualMachineInstance recreation Ensure that the VirtualMachineInstance is running if it should be running It focuses on a 1:1 relationship between the controller instance and a virtual machine instance. In many ways it is very similar to a StatefulSet with spec.replica set to 1 .","title":"VirtualMachine"},{"location":"architecture/#how-to-use-a-virtualmachine","text":"A VirtualMachine will make sure that a VirtualMachineInstance object with an identical name will be present in the cluster, if spec.running is set to true . Further it will make sure that a VirtualMachineInstance will be removed from the cluster if spec.running is set to false . There exists a field spec.runStrategy which can also be used to control the state of the associated VirtualMachineInstance object. To avoid confusing and contradictory states, these fields are mutually exclusive. An extended explanation of spec.runStrategy vs spec.running can be found in Run Strategies","title":"How to use a VirtualMachine"},{"location":"architecture/#starting-and-stopping","text":"After creating a VirtualMachine it can be switched on or off like this: # Start the virtual machine: virtctl start vm # Stop the virtual machine: virtctl stop vm kubectl can be used too: # Start the virtual machine: kubectl patch virtualmachine vm --type merge -p \\ '{\"spec\":{\"running\":true}}' # Stop the virtual machine: kubectl patch virtualmachine vm --type merge -p \\ '{\"spec\":{\"running\":false}}'","title":"Starting and stopping"},{"location":"architecture/#controller-status","text":"Once a VirtualMachineInstance is created, its state will be tracked via status.created and status.ready fields of the VirtualMachine. If a VirtualMachineInstance exists in the cluster, status.created will equal true . If the VirtualMachineInstance is also ready, status.ready will equal true too. If a VirtualMachineInstance reaches a final state but the spec.running equals true , the VirtualMachine controller will set status.ready to false and re-create the VirtualMachineInstance. Additionally, the status.printableStatus field provides high-level summary information about the state of the VirtualMachine. This information is also displayed when listing VirtualMachines using the CLI: $ kubectl get virtualmachines NAME AGE STATUS VOLUME vm1 4m Running vm2 11s Stopped Here's the list of states currently supported and their meanings. Note that states may be added/removed in future releases, so caution should be used if consumed by automated programs. Stopped : The virtual machine is currently stopped and isn't expected to start. Provisioning : Cluster resources associated with the virtual machine (e.g., DataVolumes) are being provisioned and prepared. Starting : The virtual machine is being prepared for running. Running : The virtual machine is running. Paused : The virtual machine is paused. Migrating : The virtual machine is in the process of being migrated to another host. Stopping : The virtual machine is in the process of being stopped. Terminating : The virtual machine is in the process of deletion, as well as its associated resources (VirtualMachineInstance, DataVolumes, \u2026). Unknown : The state of the virtual machine could not be obtained, typically due to an error in communicating with the host on which it's running.","title":"Controller status"},{"location":"architecture/#restarting","text":"A VirtualMachineInstance restart can be triggered by deleting the VirtualMachineInstance. This will also propagate configuration changes from the template in the VirtualMachine: # Restart the virtual machine (you delete the instance!): kubectl delete virtualmachineinstance vm To restart a VirtualMachine named vm using virtctl: $ virtctl restart vm This would perform a normal restart for the VirtualMachineInstance and would reschedule the VirtualMachineInstance on a new virt-launcher Pod To force restart a VirtualMachine named vm using virtctl: $ virtctl restart vm --force --grace-period=0 This would try to perform a normal restart, and would also delete the virt-launcher Pod of the VirtualMachineInstance with setting GracePeriodSeconds to the seconds passed in the command. Currently, only setting grace-period=0 is supported. Note: Force restart can cause data corruption, and should be used in cases of kernel panic or VirtualMachine being unresponsive to normal restarts.","title":"Restarting"},{"location":"architecture/#fencing-considerations","text":"A VirtualMachine will never restart or re-create a VirtualMachineInstance until the current instance of the VirtualMachineInstance is deleted from the cluster.","title":"Fencing considerations"},{"location":"architecture/#exposing-as-a-service","text":"A VirtualMachine can be exposed as a service. The actual service will be available once the VirtualMachineInstance starts without additional interaction. For example, exposing SSH port (22) as a ClusterIP service using virtctl after the VirtualMachine was created, but before it started: $ virtctl expose virtualmachine vmi-ephemeral --name vmiservice --port 27017 --target-port 22 All service exposure options that apply to a VirtualMachineInstance apply to a VirtualMachine. See Service Objects for more details.","title":"Exposing as a Service"},{"location":"architecture/#when-to-use-a-virtualmachine","text":"","title":"When to use a VirtualMachine"},{"location":"architecture/#when-abi-stability-is-required-between-restarts","text":"A VirtualMachine makes sure that VirtualMachineInstance ABI configurations are consistent between restarts. A classical example are licenses which are bound to the firmware UUID of a virtual machine. The VirtualMachine makes sure that the UUID will always stay the same without the user having to take care of it. One of the main benefits is that a user can still make use of defaulting logic, although a stable ABI is needed.","title":"When ABI stability is required between restarts"},{"location":"architecture/#when-config-updates-should-be-picked-up-on-the-next-restart","text":"If the VirtualMachineInstance configuration should be modifiable inside the cluster and these changes should be picked up on the next VirtualMachineInstance restart. This means that no hotplug is involved.","title":"When config updates should be picked up on the next restart"},{"location":"architecture/#when-you-want-to-let-the-cluster-manage-your-individual-virtualmachineinstance","text":"Kubernetes as a declarative system can help you to manage the VirtualMachineInstance. You tell it that you want this VirtualMachineInstance with your application running, the VirtualMachine will try to make sure it stays running. Note: The current belief is that if it is defined that the VirtualMachineInstance should be running, it should be running. This is different to many classical virtualization platforms, where VMs stay down if they were switched off. Restart policies may be added if needed. Please provide your use-case if you need this!","title":"When you want to let the cluster manage your individual VirtualMachineInstance"},{"location":"architecture/#example","text":"apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachine metadata: labels: kubevirt.io/vm: vm-cirros name: vm-cirros spec: running: false template: metadata: labels: kubevirt.io/vm: vm-cirros spec: domain: devices: disks: - disk: bus: virtio name: containerdisk - disk: bus: virtio name: cloudinitdisk machine: type: \"\" resources: requests: memory: 64M terminationGracePeriodSeconds: 0 volumes: - name: containerdisk containerDisk: image: kubevirt/cirros-container-disk-demo:latest - cloudInitNoCloud: userDataBase64: IyEvYmluL3NoCgplY2hvICdwcmludGVkIGZyb20gY2xvdWQtaW5pdCB1c2VyZGF0YScK name: cloudinitdisk Saving this manifest into vm.yaml and submitting it to Kubernetes will create the controller instance: $ kubectl create -f vm.yaml virtualmachine \"vm-cirros\" created Since spec.running is set to false , no vmi will be created: $ kubectl get vmis No resources found. Let's start the VirtualMachine: $ virtctl start vm vm-cirros As expected, a VirtualMachineInstance called vm-cirros got created: $ kubectl describe vm vm-cirros Name: vm-cirros Namespace: default Labels: kubevirt.io/vm=vm-cirros Annotations: <none> API Version: kubevirt.io/v1alpha3 Kind: VirtualMachine Metadata: Cluster Name: Creation Timestamp: 2018-04-30T09:25:08Z Generation: 0 Resource Version: 6418 Self Link: /apis/kubevirt.io/v1alpha3/namespaces/default/virtualmachines/vm-cirros UID: 60043358-4c58-11e8-8653-525500d15501 Spec: Running: true Template: Metadata: Creation Timestamp: <nil> Labels: Kubevirt . Io / Ovmi: vm-cirros Spec: Domain: Devices: Disks: Disk: Bus: virtio Name: containerdisk Volume Name: containerdisk Disk: Bus: virtio Name: cloudinitdisk Volume Name: cloudinitdisk Machine: Type: Resources: Requests: Memory: 64M Termination Grace Period Seconds: 0 Volumes: Name: containerdisk Registry Disk: Image: kubevirt/cirros-registry-disk-demo:latest Cloud Init No Cloud: User Data Base 64: IyEvYmluL3NoCgplY2hvICdwcmludGVkIGZyb20gY2xvdWQtaW5pdCB1c2VyZGF0YScK Name: cloudinitdisk Status: Created: true Ready: true Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal SuccessfulCreate 15s virtualmachine-controller Created virtual machine: vm-cirros","title":"Example"},{"location":"architecture/#kubectl-commandline-interactions","text":"Whenever you want to manipulate the VirtualMachine through the commandline you can use the kubectl command. The following are examples demonstrating how to do it. # Define a virtual machine: kubectl create -f vm.yaml # Start the virtual machine: kubectl patch virtualmachine vm --type merge -p \\ '{\"spec\":{\"running\":true}}' # Look at virtual machine status and associated events: kubectl describe virtualmachine vm # Look at the now created virtual machine instance status and associated events: kubectl describe virtualmachineinstance vm # Stop the virtual machine instance: kubectl patch virtualmachine vm --type merge -p \\ '{\"spec\":{\"running\":false}}' # Restart the virtual machine (you delete the instance!): kubectl delete virtualmachineinstance vm # Implicit cascade delete (first deletes the virtual machine and then the virtual machine instance) kubectl delete virtualmachine vm # Explicit cascade delete (first deletes the virtual machine and then the virtual machine instance) kubectl delete virtualmachine vm --cascade=true # Orphan delete (The running virtual machine is only detached, not deleted) # Recreating the virtual machine would lead to the adoption of the virtual machine instance kubectl delete virtualmachine vm --cascade=false","title":"Kubectl commandline interactions"},{"location":"latest_release_notes/","text":"Latest release notes \u00b6 v0.38.1 \u00b6 Released on: Mon Feb 8 19:00:24 2021 +0000 [PR #4870][qinqon] Bump k8s deps to 0.20.2 [PR #4571][yuvalturg] Added os, workflow and flavor labels to the kubevirt_vmi_phase_count metric [PR #4659][salanki] Fixed an issue where non-root users inside a guest could not write to a Virtio-FS mount. [PR #4844][xpivarc] Fixed limits/requests to accept int again [PR #4850][rmohr] virtio-scsi now respects the useTransitionalVirtio flag instead of assigning a virtio version depending on the machine layout [PR #4672][vladikr] allow increasing logging verbosity of infra components in KubeVirt CR [PR #4838][rmohr] Fix an issue where it may not be able to update the KubeVirt CR after creation for up to minutes due to certificate propagation delays [PR #4806][rmohr] Make the mutating webhooks for VMIs and VMs required to avoid letting entities into the cluster which are not properly defaulted [PR #4779][brybacki] Error message on virtctl image-upload to WaitForFirstConsumer DV [PR #4749][davidvossel] KUBEVIRT_CLIENT_GO_SCHEME_REGISTRATION_VERSION env var for specifying exactly what client-go scheme version is registered [PR #4772][jean-edouard] Faster VMI phase transitions thanks to an increased number of VMI watch threads in virt-controller [PR #4730][rmohr] Add spec.domain.devices.useVirtioTransitional boolean to support virtio-transitional for old guests v0.38.0 \u00b6 Released on: Mon Feb 8 13:15:32 2021 +0000 [PR #4870][qinqon] Bump k8s deps to 0.20.2 [PR #4571][yuvalturg] Added os, workflow and flavor labels to the kubevirt_vmi_phase_count metric [PR #4659][salanki] Fixed an issue where non-root users inside a guest could not write to a Virtio-FS mount. [PR #4844][xpivarc] Fixed limits/requests to accept int again [PR #4850][rmohr] virtio-scsi now respects the useTransitionalVirtio flag instead of assigning a virtio version depending on the machine layout [PR #4672][vladikr] allow increasing logging verbosity of infra components in KubeVirt CR [PR #4838][rmohr] Fix an issue where it may not be able to update the KubeVirt CR after creation for up to minutes due to certificate propagation delays [PR #4806][rmohr] Make the mutating webhooks for VMIs and VMs required to avoid letting entities into the cluster which are not properly defaulted [PR #4779][brybacki] Error message on virtctl image-upload to WaitForFirstConsumer DV [PR #4749][davidvossel] KUBEVIRT_CLIENT_GO_SCHEME_REGISTRATION_VERSION env var for specifying exactly what client-go scheme version is registered [PR #4772][jean-edouard] Faster VMI phase transitions thanks to an increased number of VMI watch threads in virt-controller [PR #4730][rmohr] Add spec.domain.devices.useVirtioTransitional boolean to support virtio-transitional for old guests v0.37.2 \u00b6 Released on: Wed Jan 27 17:49:36 2021 +0000 [PR #4872][kubevirt-bot] Add spec.domain.devices.useVirtioTransitional boolean to support virtio-transitional for old guests [PR #4855][kubevirt-bot] Fix an issue where it may not be able to update the KubeVirt CR after creation for up to minutes due to certificate propagation delays v0.37.1 \u00b6 Released on: Thu Jan 21 16:20:52 2021 +0000 [PR #4842][kubevirt-bot] KUBEVIRT_CLIENT_GO_SCHEME_REGISTRATION_VERSION env var for specifying exactly what client-go scheme version is registered v0.37.0 \u00b6 Released on: Mon Jan 18 17:57:03 2021 +0000 [PR #4654][AlonaKaplan] Introduce virt-launcher DHCPv6 server. [PR #4669][kwiesmueller] Add nodeSelector to kubevirt components restricting them to run on linux nodes only. [PR #4648][davidvossel] Update libvirt base container to be based of packages in rhel-av 8.3 [PR #4653][qinqon] Allow configure cloud-init with networkData only. [PR #4644][ashleyschuett] Operator validation webhook will deny updates to the workloads object of the KubeVirt CR if there are running VMIs [PR #3349][davidvossel] KubeVirt v1 GA api [PR #4645][maiqueb] Re-introduce the CAP_NET_ADMIN, to allow migration of VMs already having it. [PR #4546][yuhaohaoyu] Failure detection and handling for VM with EFI Insecure Boot in KubeVirt environments where EFI Insecure Boot is not supported by design. [PR #4625][awels] virtctl upload now shows error when specifying access mode of ReadOnlyMany [PR #4396][xpivarc] KubeVirt is now explainable! [PR #4517][danielBelenky] Fix guest agent reporting. v0.36.2 \u00b6 Released on: Mon Feb 22 10:20:40 2021 -0500 v0.36.1 \u00b6 Released on: Tue Jan 19 12:30:33 2021 +0100 v0.36.0 \u00b6 Released on: Wed Dec 16 14:30:37 2020 +0000 [PR #4667][kubevirt-bot] Update libvirt base container to be based of packages in rhel-av 8.3 [PR #4634][kubevirt-bot] Failure detection and handling for VM with EFI Insecure Boot in KubeVirt environments where EFI Insecure Boot is not supported by design. [PR #4647][kubevirt-bot] Re-introduce the CAP_NET_ADMIN, to allow migration of VMs already having it. [PR #4627][kubevirt-bot] Fix guest agent reporting. [PR #4458][awels] It is now possible to hotplug DataVolume and PVC volumes into a running Virtual Machine. [PR #4025][brybacki] Adds a special handling for DataVolumes in WaitForFirstConsumer state to support CDI's delayed binding mode. [PR #4217][mfranczy] Set only an IP address for interfaces reported by qemu-guest-agent. Previously that was CIDR. [PR #4195][davidvossel] AccessCredentials API for dynamic user/password and ssh public key injection [PR #4335][oshoval] VMI status displays SRIOV interfaces with their network name only when they have originally [PR #4408][andreabolognani] This version of KubeVirt includes upgraded virtualization technology based on libvirt 6.6.0 and QEMU 5.1.0. [PR #4514][ArthurSens] domain label removed from metric kubevirt_vmi_memory_unused_bytes [PR #4542][danielBelenky] Fix double migration on node evacuation [PR #4506][maiqueb] Remove CAP_NET_ADMIN from the virt-launcher pod. [PR #4501][AlonaKaplan] CAP_NET_RAW removed from virt-launcher. [PR #4488][salanki] Disable Virtio-FS metadata cache to prevent OOM conditions on the host. [PR #3937][vladikr] Generalize host devices assignment. Provides an interface between kubevirt and external device plugins. Provides a mechanism for accesslisting host devices. [PR #4443][rmohr] All kubevirt webhooks support now dry-runs. v0.35.0 \u00b6 Released on: Mon Nov 9 13:08:27 2020 +0000 [PR #4409][vladikr] Increase the static memory overhead by 10Mi [PR #4272][maiqueb] Add ip-family to the virtctl expose command. [PR #4398][rmohr] VMIs reflect deleted stuck virt-launcher pods with the \"PodTerminating\" reason in the ready condition. The VMIRs detects this reason and immediately creates replacement VMIs. [PR #4393][salanki] Disable legacy service links in virt-launcher Pods to speed up Pod instantiation and decrease Kubelet load in namespaces with many services. [PR #2935][maiqueb] Add the macvtap bind mechanism. [PR #4132][mstarostik] fixes a bug that prevented unique device name allocation when configuring both SCSI and SATA drives [PR #3257][xpivarc] Added support of kubectl explain for Kubevirt resources. [PR #4288][ezrasilvera] Adding DownwardAPI volumes type [PR #4233][maya-r] Update base image used for pods to Fedora 31. [PR #4192][xpivarc] We now run gosec in Kubevirt [PR #4328][stu-gott] Version 2.x QEMU guest agents are supported. [PR #4289][AlonaKaplan] Masquerade binding - set the virt-launcher pod interface MTU on the bridge. [PR #4300][maiqueb] Update the NetworkInterfaceMultiqueue openAPI documentation to better specify its semantics within KubeVirt. [PR #4277][awels] PVCs populated by DVs are now allowed as volumes. [PR #4265][dhiller] Fix virtctl help text when running as a plugin [PR #4273][dhiller] Only run Travis build for PRs against release branches v0.34.2 \u00b6 Released on: Tue Nov 17 08:13:22 2020 -0500 v0.34.1 \u00b6 Released on: Mon Nov 16 08:22:56 2020 -0500 v0.34.0 \u00b6 Released on: Wed Oct 7 13:59:50 2020 +0300 [PR #4315][kubevirt-bot] PVCs populated by DVs are now allowed as volumes. [PR #3837][jean-edouard] VM interfaces with no bootOrder will no longer be candidates for boot when using the BIOS bootloader, as documented [PR #3879][ashleyschuett] KubeVirt should now be configured through the KubeVirt CR configuration key. The usage of the kubevirt-config configMap will be deprecated in the future. [PR #4074][stu-gott] Fixed bug preventing non-admin users from pausing/unpausing VMs [PR #4252][rhrazdil] Fixes https://bugzilla.redhat.com/show_bug.cgi?id=1853911 [PR #4016][ashleyschuett] Allow for post copy VMI migrations [PR #4235][davidvossel] Fixes timeout failure that occurs when pulling large containerDisk images [PR #4263][rmohr] Add readiness and liveness probes to virt-handler, to clearly indicate readiness [PR #4248][maiqueb] always compile KubeVirt with selinux support on pure go builds. [PR #4012][danielBelenky] Added support for the eviction API for VMIs with eviction strategy. This enables VMIs to be live-migrated when the node is drained or when the descheduler wants to move a VMI to a different node. [PR #4075][ArthurSens] Metric kubevirt_vmi_vcpu_seconds' state label is now exposed as a human-readable state instead of an integer [PR #4162][vladikr] introduce a cpuAllocationRatio config parameter to normalize the number of CPUs requested for a pod, based on the number of vCPUs [PR #4177][maiqueb] Use vishvananda/netlink instead of songgao/water to create tap devices. [PR #4092][stu-gott] Allow specifying nodeSelectors, affinity and tolerations to control where KubeVirt components will run [PR #3927][ArthurSens] Adds new metric kubevirt_vmi_memory_unused_bytes [PR #3493][vladikr] virtio-fs is being added as experimental, protected by a feature-gate that needs to be enabled in the kubevirt config by the administrator [PR #4193][mhenriks] Add snapshot.kubevirt.io to admin/edit/view roles [PR #4149][qinqon] Bump kubevirtci to k8s-1.19 [PR #3471][crobinso] Allow hiding that the VM is running on KVM, so that Nvidia graphics cards can be passed through [PR #4115][phoracek] Add conformance automation and manifest publishing [PR #3733][davidvossel] each PRs description. [PR #4082][mhenriks] VirtualMachineRestore API and implementation [PR #4154][davidvossel] Fixes issue with Service endpoints not being updated properly in place during KubeVirt updates. [PR #3289][vatsalparekh] Add option to run only VNC Proxy in virtctl [PR #4027][alicefr] Added memfd as default memory backend for hugepages. This introduces the new annotation kubevirt.io/memfd to disable memfd as default and fallback to the previous behavior. [PR #3612][ashleyschuett] Adds customizeComponents to the kubevirt api [PR #4029][cchengleo] Fix an issue which prevented virt-operator from installing monitoring resources in custom namespaces. [PR #4031][rmohr] Initial support for sonobuoy for conformance testing v0.33.0 \u00b6 Released on: Tue Sep 15 14:46:00 2020 +0000 [PR #3226][vatsalparekh] Added tests to verify custom pciAddress slots and function [PR #4048][davidvossel] Improved reliability for failed migration retries [PR #3585][mhenriks] \"virtctl image-upload pvc ...\" will create the PVC if it does not exist [PR #3945][xpivarc] KubeVirt is now being built with Go1.13.14 [PR #3845][ArthurSens] action required: The domain label from VMI metrics is being removed and may break dashboards that use the domain label to identify VMIs. Use name and namespace labels instead [PR #4011][dhiller] ppc64le arch has been disabled for the moment, see https://github.com/kubevirt/kubevirt/issues/4037 [PR #3875][stu-gott] Resources created by KubeVirt are now labelled more clearly in terms of relationship and role. [PR #3791][ashleyschuett] make node as kubevirt.io/schedulable=false on virt-handler restart [PR #3998][vladikr] the local provider is usable again. [PR #3290][maiqueb] Have virt-handler (KubeVirt agent) create the tap devices on behalf of the virt-launchers. [PR #3957][AlonaKaplan] virt-launcher support Ipv6 on dual stack cluster. [PR #3952][davidvossel] Fixes rare situation where vmi may not properly terminate if failure occurs before domain starts. [PR #3973][xpivarc] Fixes VMs with clock.timezone set. [PR #3923][danielBelenky] Add support to configure QEMU I/O mode for VMIs [PR #3889][rmohr] The status fields for our CRDs are now protected on normal PATCH and PUT operations.The /status subresource is now used where possible for status updates. [PR #3568][xpivarc] Guest swap metrics available v0.32.0 \u00b6 Released on: Tue Aug 11 19:21:56 2020 +0000 [PR #3921][vladikr] use correct memory units in libvirt xml [PR #3893][davidvossel] Adds recurring period that rsyncs virt-launcher domains with virt-handler [PR #3880][sgarbour] Better error message when input parameters are not the expected number of parameters for each argument. Help menu will popup in case the number of parameters is incorrect. [PR #3785][xpivarc] Vcpu wait metrics available [PR #3642][vatsalparekh] Add a way to update VMI Status with latest Pod IP for Masquerade bindings [PR #3636][ArthurSens] Adds kubernetes metadata.labels as VMI metrics' label [PR #3825][awels] Virtctl now prints error messages from the response body on upload errors. [PR #3830][davidvossel] Fixes re-establishing domain notify client connections when domain notify server restarts due to an error event. [PR #3778][danielBelenky] Do not emit a SyncFailed event if we fail to sync a VMI in a final state [PR #3803][andreabolognani] Not sure what to write here (see above) [PR #2694][rmohr] Use native go libraries for selinux to not rely on python-selinux tools like semanage, which are not always present. [PR #3692][victortoso] QEMU logs can now be fetched from outside the pod [PR #3738][enp0s3] Restrict creation of VMI if it has labels that are used internally by Kubevirt components. [PR #3725][danielBelenky] The tests binary is now part of the release and can be consumed from the GitHub release page. [PR #3684][rmohr] Log if critical devices, like kvm, which virt-handler wants to expose are not present on the node. [PR #3166][petrkotas] Introduce new virtctl commands: [PR #3708][andreabolognani] Make qemu work on GCE by pulling in a fix for https://bugzilla.redhat.com/show_bug.cgi?id=1822682 v0.31.0 \u00b6 Released on: Thu Jul 9 16:08:18 2020 +0300 [PR 3690][davidvossel] Update go-grpc dependency to v1.30.0 in order to improve stability [PR 3628][AlonaKaplan] Avoid virt-handler crash in case of virt-launcher network configuration error [PR 3635][jean-edouard] The \"HostDisk\" feature gate has to be enabled to use hostDisks [PR 3641][vatsalparekh] Reverts kubevirt/kubevirt#3488 because CI seems to have merged it without all tests passing [PR 3488][vatsalparekh] Add a way to update VMI Status with latest Pod IP for Masquerade bindings [PR 3406][tomob] If a PVC was created by a DataVolume, it cannot be used as a Volume Source for a VM. The owning DataVolume has to be used instead. [PR 3566][kraxel] added: TigerVNC support for linux & windows [PR 3529][jean-edouard] Enabling EFI will also enable Secure Boot, which requires SMM to be enabled. [PR 3455][ashleyschuett] Add KubevirtConfiguration, MigrationConfiguration, DeveloperConfiguration and NetworkConfiguration to API-types [PR 3520][rmohr] Fix hot-looping on the VMI sync-condition if errors happen during the Scheduled phase of a VMI [PR 3220][mhenriks] API and controller/webhook for VirtualMachineSnapshots v0.30.7 \u00b6 Released on: Mon Oct 26 11:57:21 2020 -0400 v0.30.6 \u00b6 Released on: Wed Aug 12 10:55:31 2020 +0200 v0.30.5 \u00b6 Released on: Fri Jul 17 05:26:37 2020 -0400 v0.30.4 \u00b6 Released on: Fri Jul 10 07:44:00 2020 -0400 v0.30.3 \u00b6 Released on: Tue Jun 30 17:39:42 2020 -0400 v0.30.2 \u00b6 Released on: Thu Jun 25 17:05:59 2020 -0400 v0.30.1 \u00b6 Released on: Tue Jun 16 13:10:17 2020 -0400 v0.30.0 \u00b6 Released on: Fri Jun 5 12:19:57 2020 +0200 Tests: Many more test fixes Security: Introduce a custom SELinux policy for virt-launcher More user friendly IPv6 default CIDR for IPv6 addresses Fix OpenAPI compatibility issues by switching to openapi-gen Improved support for EFI boot (configurable OVMF path and test fixes) Improved VMI IP reporting Support propagation of annotations from VMI to pods Support for more fine grained (NET_RAW( capability granting to virt-launcher Support for eventual consistency with DataVolumes v0.29.2 \u00b6 Released on: Mon May 25 21:15:30 2020 +0200 v0.29.1 \u00b6 Released on: Tue May 19 10:03:27 2020 +0200 v0.29.0 \u00b6 Released on: Wed May 6 15:01:57 2020 +0200 Tests: Many many test fixes Tests: Many more test fixes CI: Add lane with SELinux enabled CI: Drop PPC64 support for now Drop Genie support Drop the use of hostPaths in the virt-launcher for improved security Support priority classes for important components Support IPv6 over masquerade binding Support certificate rotations based on shared secrets Support for VM ready condition Support for advanced node labelling (supported CPU Families and machine types) v0.28.0 \u00b6 Released on: Thu Apr 9 23:01:29 2020 +0200 CI: Try to discover flaky tests before merge Fix the use of priorityClasses Fix guest memory overhead calculation Fix SR-IOV device overhead requirements Fix loading of tun module during virt-handler initialization Fixes for several test cases Fixes to support running with container_t Support for renaming a VM Support ioEmulator thread pinning Support a couple of alerts for virt-handler Support for filesystem listing using the guest agent Support for retrieving data from the guest agent Support for device role tagging Support for assigning devices to the PCI root bus Support for guest overhead override Rewrite container-disk in C to in order to reduce it's memory footprint v0.27.0 \u00b6 Released on: Fri Mar 6 22:40:34 2020 +0100 Support for more guest agent informations in the API Support setting priorityClasses on VMs Support for additional control plane alerts via prometheus Support for io and emulator thread pinning Support setting a custom SELinux type for the launcher Support to perform network configurations from handler instead of launcher Support to opt-out of auto attaching the serial console Support for different uninstall strategies for data protection Fix to let qemu run in the qemu group Fix guest agent connectivity check after i.e. live migrations v0.26.5 \u00b6 Released on: Tue Apr 14 15:07:04 2020 -0400 v0.26.4 \u00b6 Released on: Mon Mar 30 03:43:48 2020 +0200 v0.26.3 \u00b6 Released on: Tue Mar 10 08:57:27 2020 -0400 v0.26.2 \u00b6 Released on: Tue Mar 3 12:31:56 2020 -0500 v0.26.1 \u00b6 Released on: Fri Feb 14 20:42:46 2020 +0100 v0.26.0 \u00b6 Released on: Fri Feb 7 09:40:07 2020 +0100 Fix incorrect ownerReferences to avoid VMs getting GCed Fixes for several tests Fix greedy permissions around Secrets by delegating them to kubelet Fix OOM infra pod by increasing it's memory request Clarify device support around live migrations Support for an uninstall strategy to protect workloads during uninstallation Support for more prometheus metrics and alert rules Support for testing SRIOV connectivity in functional tests Update Kubernetes client-go to 1.16.4 FOSSA fixes and status v0.25.0 \u00b6 Released on: Mon Jan 13 20:37:15 2020 +0100 CI: Support for Kubernetes 1.17 Support emulator thread pinning Support virtctl restart --force Support virtctl migrate to trigger live migrations from the CLI v0.24.0 \u00b6 Released on: Tue Dec 3 15:34:34 2019 +0100 CI: Support for Kubernetes 1.15 CI: Support for Kubernetes 1.16 Add and fix a couple of test cases Support for pause and unpausing VMs Update of libvirt to 5.6.0 Fix bug related to parallel scraping of Prometheus endpoints Fix to reliably test VNC v0.23.3 \u00b6 Released on: Tue Jan 21 13:17:20 2020 -0500 v0.23.2 \u00b6 Released on: Fri Jan 10 10:36:36 2020 -0500 v0.23.1 \u00b6 Released on: Thu Nov 28 09:36:41 2019 +0100 v0.23.0 \u00b6 Released on: Mon Nov 4 16:42:54 2019 +0100 Guest OS Information is available under the VMI status now Updated to Go 1.12.8 and latest bazel Updated go-yaml to v2.2.4, which has a ddos vulnerability fixed Cleaned up and fixed CRD scheme registration Several bug fixes Many CI improvements (e.g. more logs in case of test failures) v0.22.0 \u00b6 Released on: Thu Oct 10 18:55:08 2019 +0200 Support for Nvidia GPUs and vGPUs exposed by Nvidia Kubevirt Device Plugin. VMIs now successfully start if they get a 0xfe prefixed MAC address assigned from the pod network Removed dependency on host semanage in SELinux Permissive mode Some changes as result of entering the CNCF sandbox (DCO check, FOSSA check, best practice badge) Many bug fixes and improvements in several areas CI: Introduced a OKD 4 test lane CI: Many improved tests resulting in less flakiness v0.21.0 \u00b6 Released on: Mon Sep 9 09:59:08 2019 +0200 CI: Support for Kubernetes 1.14 Many bug fixes in several areas Support for virtctl migrate Support configurable number of controller threads Support to opt-out of bridge binding for podNetwork Support for OpenShift Prometheus monitoring Support for setting more SMBIOS fields Improved containerDisk memory usage and speed Fix CRI-O memory limit Drop spc_t from launcher Add feature gates to security sensitive features v0.20.8 \u00b6 Released on: Thu Oct 3 12:03:40 2019 +0200 v0.20.7 \u00b6 Released on: Fri Sep 27 15:21:56 2019 +0200 v0.20.6 \u00b6 Released on: Wed Sep 11 06:09:47 2019 -0400 v0.20.5 \u00b6 Released on: Thu Sep 5 17:48:59 2019 +0200 v0.20.4 \u00b6 Released on: Mon Sep 2 18:55:35 2019 +0200 v0.20.3 \u00b6 Released on: Tue Aug 27 16:58:15 2019 +0200 v0.20.2 \u00b6 Released on: Tue Aug 20 15:51:07 2019 +0200 v0.20.1 \u00b6 Released on: Fri Aug 9 19:48:17 2019 +0200 Container disks are now secure and they are not copied anymore on every start. Old container disks can still be used in the same secure way, but new container disks can't be used on older kubevirt releases Create specific SecurityContextConstraints on OKD instead of using the privileged SCC Added clone authorization check for DataVolumes with PVC source The sidecar feature is feature-gated now Use container image shasums instead of tags for KubeVirt deployments Protect control plane components against voluntary evictions with a PodDisruptionBudget of MinAvailable=1 Replaced hardcoded virtctl by using the basename of the call, this enables nicer output when installed via krew plugin package manager Added RNG device to all Fedora VMs in tests and examples (newer kernels might block bootimg while waiting for entropy) The virtual memory is now set to match the memory limit, if memory limit is specified and guest memory is not Support nftable for CoreOS Added a block-volume flag to the virtctl image-upload command Improved virtctl console/vnc data flow Removed DataVolumes feature gate in favor of auto-detecting CDI support Removed SR-IOV feature gate, it is enabled by default now VMI-related metrics have been renamed from kubevirt_vm_ to kubevirt_vmi_ to better reflect their purpose Added metric to report the VMI count Improved integration with HCO by adding a CSV generator tool and modified KubeVirt CR conditions CI Improvements: Added dedicated SR-IOV test lane Improved log gathering Reduced amount of flaky tests v0.20.0 \u00b6 Released on: Fri Aug 9 16:42:41 2019 +0200 container Disks are now secure and they are not copied anymore on every start. Old container Disks can still be used in the same secure way, but new container Disks can't be used on older kubevirt releases Create specific SecurityContextConstraints on OKD instead of using the privileged SCC Added clone authorization check for DataVolumes with PVC source The sidecar feature is feature-gated now Use container image shasum's instead of tags for KubeVirt deployments Protect control plane components against voluntary evictions with a PodDisruptionBudget of MinAvailable=1 Replaced hardcoded virtctl by using the basename of the call, this enables nicer output when installed via krew plugin package manager Added RNG device to all Fedora VMs in tests and examples (newer kernels might block boot img while waiting for entropy) The virtual memory is now set to match the memory limit, if memory limit is specified and guest memory is not Support nftable for CoreOS Added a block-volume flag to the virtctl image-upload command Improved virtctl console/vnc data flow Removed DataVolumes feature gate in favor of auto-detecting CDI support Removed SR-IOV feature gate, it is enabled by default now VMI-related metrics have been renamed from kubevirt_vm_ to kubevirt_vmi_ to better reflect their purpose Added metric to report the VMI count Improved integration with HCO by adding a CSV generator tool and modified KubeVirt CR conditions CI Improvements: Added dedicated SR-IOV test lane Improved log gathering Reduced amount of flaky tests v0.19.0 \u00b6 Released on: Fri Jul 5 12:52:16 2019 +0200 Fixes when run on kind Fixes for sub-resource RBAC Limit pod network interface bindings Many additional bug fixes in many areas Additional test cases for updates, disk types, live migration with NFS Additional test cases for memory over-commit, block storage, cpu manager, headless mode Improvements around HyperV Improved error handling for runStrategies Improved update procedure Improved network metrics reporting (packets and errors) Improved guest overhead calculation Improved SR-IOV test suite Support for live migration auto-converge Support for config-drive disks Support for setting a pullPolicy con container Disks Support for unprivileged VMs when using SR-IOV Introduction of a project security policy v0.18.1 \u00b6 Released on: Thu Jun 13 12:00:56 2019 +0200 v0.18.0 \u00b6 Released on: Wed Jun 5 22:25:09 2019 +0200 Build: Use of go modules CI: Support for Kubernetes 1.13 Countless test cases fixes and additions Several smaller bug fixes Improved upgrade documentation v0.17.4 \u00b6 Released on: Tue Jun 25 07:49:12 2019 -0400 v0.17.3 \u00b6 Released on: Wed Jun 19 12:00:45 2019 -0400 v0.17.2 \u00b6 Released on: Wed Jun 5 08:12:04 2019 -0400 v0.17.1 \u00b6 Released on: Tue Jun 4 14:41:10 2019 -0400 v0.17.0 \u00b6 Released on: Mon May 6 16:18:01 2019 +0200 Several test case additions Improved virt-controller node distribution Improved support between version migrations Support for a configurable MachineType default Support for live-migration of a VM on node taints Support for VM swap metrics Support for versioned virt-launcher / virt-handler communication Support for HyperV flags Support for different VM run strategies (i.e manual and rerunOnFailure) Several fixes for live-migration (TLS support, protected pods) v0.16.3 \u00b6 Released on: Thu May 2 23:51:08 2019 +0200 v0.16.2 \u00b6 Released on: Fri Apr 26 12:24:33 2019 +0200 v0.16.1 \u00b6 Released on: Tue Apr 23 19:31:19 2019 +0200 v0.16.0 \u00b6 Released on: Fri Apr 5 23:18:22 2019 +0200 Bazel fixes Initial work to support upgrades (not finalized) Initial support for HyperV features Support propagation of MAC addresses to multus Support live migration cancellation Support for table input devices Support for generating OLM metadata Support for triggering VM live migration on node taints v0.15.0 \u00b6 Released on: Tue Mar 5 10:35:08 2019 +0100 CI: Several fixes Fix configurable number of KVM devices Narrow virt-handler permissions Use bazel for development builds Support for live migration with shared and non-shared disks Support for live migration progress tracking Support for EFI boot Support for libvirt 5.0 Support for extra DHCP options Support for a hook to manipulate cloud-init metadata Support setting a VM serial number Support for exposing infra and VM metrics Support for a tablet input device Support for extra CPU flags Support for ignition metadata Support to set a default CPU model Update to go 1.11.5 v0.14.0 \u00b6 Released on: Mon Feb 4 22:04:14 2019 +0100 CI: Several stabilizing fixes docs: Document the KubeVirt Razor build: golang update Update to Kubernetes 1.12 Update CDI Support for Ready and Created Operator conditions Support (basic) EFI Support for generating cloud-init network-config v0.13.7 \u00b6 Released on: Mon Oct 28 17:02:35 2019 -0400 v0.13.6 \u00b6 Released on: Wed Sep 25 17:19:44 2019 +0200 v0.13.5 \u00b6 Released on: Thu Aug 1 11:25:00 2019 -0400 v0.13.4 \u00b6 Released on: Thu Aug 1 09:52:35 2019 -0400 v0.13.3 \u00b6 Released on: Mon Feb 4 15:46:48 2019 -0500 v0.13.2 \u00b6 Released on: Thu Jan 24 23:24:06 2019 +0100 v0.13.1 \u00b6 Released on: Thu Jan 24 11:16:20 2019 +0100 v0.13.0 \u00b6 Released on: Tue Jan 15 08:26:25 2019 +0100 CI: Fix virt-api race API: Remove volumeName from disks v0.12.0 \u00b6 Released on: Fri Jan 11 22:22:02 2019 +0100 Introduce a KubeVirt Operator for KubeVirt life-cycle management Introduce dedicated kubevirt namespace Support VMI ready conditions Support vCPU threads and sockets Support scale and HPA for VMIRs Support to pass NTP related DHCP options Support guest IP address reporting via qemu guest agent Support for live migration with shared storage Support scheduling of VMs based on CPU family Support masquerade network interface binding v0.11.1 \u00b6 Released on: Thu Dec 13 10:21:56 2018 +0200 v0.11.0 \u00b6 Released on: Thu Dec 6 10:15:51 2018 +0100 API: registryDisk got renamed to containerDisk CI: User OKD 3.11 Fix: Tolerate if the PVC has less capacity than expected Aligned to use ownerReferences Update to libvirt-4.10.0 Support for VNC on MAC OSX Support for network SR-IOV interfaces Support for custom DHCP options Support for VM restarts via a custom endpoint Support for liveness and readiness probes v0.10.0 \u00b6 Released on: Thu Nov 8 15:21:34 2018 +0100 Support for vhost-net Support for block multi-queue Support for custom PCI addresses for virtio devices Support for deploying KubeVirt to a custom namespace Support for ServiceAccount token disks Support for multus backed networks Support for genie backed networks Support for kuryr backed networks Support for block PVs Support for configurable disk device caches Support for pinned IO threads Support for virtio net multi-queue Support for image upload (depending on CDI) Support for custom entity lists with more VM details (custom columns) Support for IP and MAC address reporting of all vNICs Basic support for guest agent status reporting More structured logging Better libvirt error reporting Stricter CR validation Better ownership references Several test improvements v0.9.6 \u00b6 Released on: Thu Nov 22 17:14:18 2018 +0100 v0.9.5 \u00b6 Released on: Thu Nov 8 09:57:48 2018 +0100 v0.9.4 \u00b6 Released on: Wed Nov 7 08:22:14 2018 -0500 v0.9.3 \u00b6 Released on: Mon Oct 22 09:04:02 2018 -0400 v0.9.2 \u00b6 Released on: Thu Oct 18 12:14:09 2018 +0200 v0.9.1 \u00b6 Released on: Fri Oct 5 09:01:51 2018 +0200 v0.9.0 \u00b6 Released on: Thu Oct 4 14:42:28 2018 +0200 CI: NetworkPolicy tests CI: Support for an external provider (use a preconfigured cluster for tests) Fix virtctl console issues with CRI-O Support to initialize empty PVs Support for basic CPU pinning Support for setting IO Threads Support for block volumes Move preset logic to mutating webhook Introduce basic metrics reporting using prometheus metrics Many stabilizing fixes in many places v0.8.0 \u00b6 Released on: Thu Sep 6 14:25:22 2018 +0200 Support for DataVolume Support for a subprotocol for web browser terminals Support for virtio-rng Support disconnected VMs Support for setting host model Support for host CPU passthrough Support setting a vNICs mac and PCI address Support for memory over-commit Support booting from network devices Use less devices by default, aka disable unused ones Improved VMI shutdown status More logging to improve debugability A lot of small fixes, including typos and documentation fixes Race detection in tests Hook improvements Update to use Fedora 28 (includes updates of dependencies like libvirt and qemu) Move CI to support Kubernetes 1.11 v0.7.0 \u00b6 Released on: Wed Jul 4 17:41:33 2018 +0200 CI: Move test storage to hostPath CI: Add support for Kubernetes 1.10.4 CI: Improved network tests for multiple-interfaces CI: Drop Origin 3.9 support CI: Add test for testing templates on Origin VM to VMI rename VM affinity and anti-affinity Add awareness for multiple networks Add hugepage support Add device-plugin based kvm Add support for setting the network interface model Add (basic and initial) Kubernetes compatible networking approach (SLIRP) Add role aggregation for our roles Add support for setting a disks serial number Add support for specifying the CPU model Add support for setting an network interfaces MAC address Relocate binaries for FHS conformance Logging improvements Template fixes Fix OpenShift CRD validation virtctl: Improve vnc logging improvements virtctl: Add expose virtctl: Use PATCH instead of PUT v0.6.4 \u00b6 Released on: Tue Aug 21 17:29:28 2018 +0300 v0.6.3 \u00b6 Released on: Mon Jul 30 16:14:22 2018 +0200 v0.6.2 \u00b6 Released on: Wed Jul 4 17:49:37 2018 +0200 Binary relocation for packaging QEMU Process detection Role aggregation CPU Model selection VM Rename fix v0.6.1 \u00b6 Released on: Mon Jun 18 17:07:48 2018 -0400 v0.6.0 \u00b6 Released on: Mon Jun 11 09:30:28 2018 +0200 A range of flakiness reducing test fixes Vagrant setup got deprecated Updated Docker and CentOS versions Add Kubernetes 1.10.3 to test matrix A couple of ginkgo concurrency fixes A couple of spelling fixes A range if infra updates Use /dev/kvm if possible, otherwise fallback to emulation Add default view/edit/admin RBAC Roles Network MTU fixes CD-ROM drives are now read-only Secrets can now be correctly referenced on VMs Add disk boot ordering Add virtctl version Add virtctl expose Fix virtual machine memory calculations Add basic virtual machine Network API v0.5.0 \u00b6 Released on: Fri May 4 18:25:32 2018 +0200 Better controller health signaling Better virtctl error messages Improvements to enable CRI-O support Run CI on stable OpenShift Add test coverage for multiple PVCs Improved controller life-cycle guarantees Add Webhook validation Add tests coverage for node eviction OfflineVirtualMachine status improvements RegistryDisk API update v0.4.1 \u00b6 Released on: Thu Apr 12 11:46:09 2018 +0200 VM shutdown fixes and tests Functional test for CRD validation Windows VM test DHCP link-local change v0.4.0 \u00b6 Released on: Fri Apr 6 16:40:31 2018 +0200 Fix several networking issues Add and enable OpenShift support to CI Add conditional Windows tests (if an image is present) Add subresources for console access virtctl config alignment with kubectl Fix API reference generation Stable UUIDs for OfflineVirtualMachines Build virtctl for MacOS and Windows Set default architecture to x86_64 Major improvement to the CI infrastructure (all containerized) virtctl convenience functions for starting and stopping a VM v0.3.0 \u00b6 Released on: Thu Mar 8 10:21:57 2018 +0100 Kubernetes compatible networking Kubernetes compatible PV based storage VirtualMachinePresets support OfflineVirtualMachine support RBAC improvements Switch to q35 machine type by default A large number of test and CI fixes Ephemeral disk support v0.2.0 \u00b6 Released on: Fri Jan 5 16:30:45 2018 +0100 VM launch and shutdown flow improvements VirtualMachine API redesign Removal of HAProxy Redesign of VNC/Console access Initial support for different vagrant providers v0.1.0 \u00b6 Released on: Fri Dec 8 20:43:06 2017 +0100 Many API improvements for a proper OpenAPI reference Add watchdog support Drastically improve the deployment on non-vagrant setups Dropped nodeSelectors Separated inner component deployment from edge component deployment Created separate manifests for developer, test, and release deployments Moved components to kube-system namespace Improved and unified flag parsing v0.0.4 \u00b6 Released on: Tue Nov 7 11:51:45 2017 +0100 Add support for node affinity to VM.Spec Add OpenAPI specification Drop swagger 1.2 specification virt-launcher refactoring Leader election mechanism for virt-controller Move from glide to dep for dependency management Improve virt-handler synchronization loops Add support for running the functional tests on oVirt infrastructure Several tests fixes (spice, cleanup, ...) Add console test tool Improve libvirt event notification v0.0.3 \u00b6 Released on: Fri Oct 6 10:21:16 2017 +0200 Containerized binary builds Socket based container detection cloud-init support Container based ephemeral disk support Basic RBAC profile client-go updates Rename of VM to VirtualMachine Introduction of VirtualMachineReplicaSet Improved migration events Improved API documentation v0.0.2 \u00b6 Released on: Mon Sep 4 21:12:46 2017 +0200 Usage of CRDs Moved libvirt to a pod Introduction of virtctl Use glide instead of govendor Container based ephemeral disks Contributing guide improvements Support for Kubernetes Namespaces","title":"Latest release notes"},{"location":"latest_release_notes/#latest-release-notes","text":"","title":"Latest release notes"},{"location":"latest_release_notes/#v0381","text":"Released on: Mon Feb 8 19:00:24 2021 +0000 [PR #4870][qinqon] Bump k8s deps to 0.20.2 [PR #4571][yuvalturg] Added os, workflow and flavor labels to the kubevirt_vmi_phase_count metric [PR #4659][salanki] Fixed an issue where non-root users inside a guest could not write to a Virtio-FS mount. [PR #4844][xpivarc] Fixed limits/requests to accept int again [PR #4850][rmohr] virtio-scsi now respects the useTransitionalVirtio flag instead of assigning a virtio version depending on the machine layout [PR #4672][vladikr] allow increasing logging verbosity of infra components in KubeVirt CR [PR #4838][rmohr] Fix an issue where it may not be able to update the KubeVirt CR after creation for up to minutes due to certificate propagation delays [PR #4806][rmohr] Make the mutating webhooks for VMIs and VMs required to avoid letting entities into the cluster which are not properly defaulted [PR #4779][brybacki] Error message on virtctl image-upload to WaitForFirstConsumer DV [PR #4749][davidvossel] KUBEVIRT_CLIENT_GO_SCHEME_REGISTRATION_VERSION env var for specifying exactly what client-go scheme version is registered [PR #4772][jean-edouard] Faster VMI phase transitions thanks to an increased number of VMI watch threads in virt-controller [PR #4730][rmohr] Add spec.domain.devices.useVirtioTransitional boolean to support virtio-transitional for old guests","title":"v0.38.1"},{"location":"latest_release_notes/#v0380","text":"Released on: Mon Feb 8 13:15:32 2021 +0000 [PR #4870][qinqon] Bump k8s deps to 0.20.2 [PR #4571][yuvalturg] Added os, workflow and flavor labels to the kubevirt_vmi_phase_count metric [PR #4659][salanki] Fixed an issue where non-root users inside a guest could not write to a Virtio-FS mount. [PR #4844][xpivarc] Fixed limits/requests to accept int again [PR #4850][rmohr] virtio-scsi now respects the useTransitionalVirtio flag instead of assigning a virtio version depending on the machine layout [PR #4672][vladikr] allow increasing logging verbosity of infra components in KubeVirt CR [PR #4838][rmohr] Fix an issue where it may not be able to update the KubeVirt CR after creation for up to minutes due to certificate propagation delays [PR #4806][rmohr] Make the mutating webhooks for VMIs and VMs required to avoid letting entities into the cluster which are not properly defaulted [PR #4779][brybacki] Error message on virtctl image-upload to WaitForFirstConsumer DV [PR #4749][davidvossel] KUBEVIRT_CLIENT_GO_SCHEME_REGISTRATION_VERSION env var for specifying exactly what client-go scheme version is registered [PR #4772][jean-edouard] Faster VMI phase transitions thanks to an increased number of VMI watch threads in virt-controller [PR #4730][rmohr] Add spec.domain.devices.useVirtioTransitional boolean to support virtio-transitional for old guests","title":"v0.38.0"},{"location":"latest_release_notes/#v0372","text":"Released on: Wed Jan 27 17:49:36 2021 +0000 [PR #4872][kubevirt-bot] Add spec.domain.devices.useVirtioTransitional boolean to support virtio-transitional for old guests [PR #4855][kubevirt-bot] Fix an issue where it may not be able to update the KubeVirt CR after creation for up to minutes due to certificate propagation delays","title":"v0.37.2"},{"location":"latest_release_notes/#v0371","text":"Released on: Thu Jan 21 16:20:52 2021 +0000 [PR #4842][kubevirt-bot] KUBEVIRT_CLIENT_GO_SCHEME_REGISTRATION_VERSION env var for specifying exactly what client-go scheme version is registered","title":"v0.37.1"},{"location":"latest_release_notes/#v0370","text":"Released on: Mon Jan 18 17:57:03 2021 +0000 [PR #4654][AlonaKaplan] Introduce virt-launcher DHCPv6 server. [PR #4669][kwiesmueller] Add nodeSelector to kubevirt components restricting them to run on linux nodes only. [PR #4648][davidvossel] Update libvirt base container to be based of packages in rhel-av 8.3 [PR #4653][qinqon] Allow configure cloud-init with networkData only. [PR #4644][ashleyschuett] Operator validation webhook will deny updates to the workloads object of the KubeVirt CR if there are running VMIs [PR #3349][davidvossel] KubeVirt v1 GA api [PR #4645][maiqueb] Re-introduce the CAP_NET_ADMIN, to allow migration of VMs already having it. [PR #4546][yuhaohaoyu] Failure detection and handling for VM with EFI Insecure Boot in KubeVirt environments where EFI Insecure Boot is not supported by design. [PR #4625][awels] virtctl upload now shows error when specifying access mode of ReadOnlyMany [PR #4396][xpivarc] KubeVirt is now explainable! [PR #4517][danielBelenky] Fix guest agent reporting.","title":"v0.37.0"},{"location":"latest_release_notes/#v0362","text":"Released on: Mon Feb 22 10:20:40 2021 -0500","title":"v0.36.2"},{"location":"latest_release_notes/#v0361","text":"Released on: Tue Jan 19 12:30:33 2021 +0100","title":"v0.36.1"},{"location":"latest_release_notes/#v0360","text":"Released on: Wed Dec 16 14:30:37 2020 +0000 [PR #4667][kubevirt-bot] Update libvirt base container to be based of packages in rhel-av 8.3 [PR #4634][kubevirt-bot] Failure detection and handling for VM with EFI Insecure Boot in KubeVirt environments where EFI Insecure Boot is not supported by design. [PR #4647][kubevirt-bot] Re-introduce the CAP_NET_ADMIN, to allow migration of VMs already having it. [PR #4627][kubevirt-bot] Fix guest agent reporting. [PR #4458][awels] It is now possible to hotplug DataVolume and PVC volumes into a running Virtual Machine. [PR #4025][brybacki] Adds a special handling for DataVolumes in WaitForFirstConsumer state to support CDI's delayed binding mode. [PR #4217][mfranczy] Set only an IP address for interfaces reported by qemu-guest-agent. Previously that was CIDR. [PR #4195][davidvossel] AccessCredentials API for dynamic user/password and ssh public key injection [PR #4335][oshoval] VMI status displays SRIOV interfaces with their network name only when they have originally [PR #4408][andreabolognani] This version of KubeVirt includes upgraded virtualization technology based on libvirt 6.6.0 and QEMU 5.1.0. [PR #4514][ArthurSens] domain label removed from metric kubevirt_vmi_memory_unused_bytes [PR #4542][danielBelenky] Fix double migration on node evacuation [PR #4506][maiqueb] Remove CAP_NET_ADMIN from the virt-launcher pod. [PR #4501][AlonaKaplan] CAP_NET_RAW removed from virt-launcher. [PR #4488][salanki] Disable Virtio-FS metadata cache to prevent OOM conditions on the host. [PR #3937][vladikr] Generalize host devices assignment. Provides an interface between kubevirt and external device plugins. Provides a mechanism for accesslisting host devices. [PR #4443][rmohr] All kubevirt webhooks support now dry-runs.","title":"v0.36.0"},{"location":"latest_release_notes/#v0350","text":"Released on: Mon Nov 9 13:08:27 2020 +0000 [PR #4409][vladikr] Increase the static memory overhead by 10Mi [PR #4272][maiqueb] Add ip-family to the virtctl expose command. [PR #4398][rmohr] VMIs reflect deleted stuck virt-launcher pods with the \"PodTerminating\" reason in the ready condition. The VMIRs detects this reason and immediately creates replacement VMIs. [PR #4393][salanki] Disable legacy service links in virt-launcher Pods to speed up Pod instantiation and decrease Kubelet load in namespaces with many services. [PR #2935][maiqueb] Add the macvtap bind mechanism. [PR #4132][mstarostik] fixes a bug that prevented unique device name allocation when configuring both SCSI and SATA drives [PR #3257][xpivarc] Added support of kubectl explain for Kubevirt resources. [PR #4288][ezrasilvera] Adding DownwardAPI volumes type [PR #4233][maya-r] Update base image used for pods to Fedora 31. [PR #4192][xpivarc] We now run gosec in Kubevirt [PR #4328][stu-gott] Version 2.x QEMU guest agents are supported. [PR #4289][AlonaKaplan] Masquerade binding - set the virt-launcher pod interface MTU on the bridge. [PR #4300][maiqueb] Update the NetworkInterfaceMultiqueue openAPI documentation to better specify its semantics within KubeVirt. [PR #4277][awels] PVCs populated by DVs are now allowed as volumes. [PR #4265][dhiller] Fix virtctl help text when running as a plugin [PR #4273][dhiller] Only run Travis build for PRs against release branches","title":"v0.35.0"},{"location":"latest_release_notes/#v0342","text":"Released on: Tue Nov 17 08:13:22 2020 -0500","title":"v0.34.2"},{"location":"latest_release_notes/#v0341","text":"Released on: Mon Nov 16 08:22:56 2020 -0500","title":"v0.34.1"},{"location":"latest_release_notes/#v0340","text":"Released on: Wed Oct 7 13:59:50 2020 +0300 [PR #4315][kubevirt-bot] PVCs populated by DVs are now allowed as volumes. [PR #3837][jean-edouard] VM interfaces with no bootOrder will no longer be candidates for boot when using the BIOS bootloader, as documented [PR #3879][ashleyschuett] KubeVirt should now be configured through the KubeVirt CR configuration key. The usage of the kubevirt-config configMap will be deprecated in the future. [PR #4074][stu-gott] Fixed bug preventing non-admin users from pausing/unpausing VMs [PR #4252][rhrazdil] Fixes https://bugzilla.redhat.com/show_bug.cgi?id=1853911 [PR #4016][ashleyschuett] Allow for post copy VMI migrations [PR #4235][davidvossel] Fixes timeout failure that occurs when pulling large containerDisk images [PR #4263][rmohr] Add readiness and liveness probes to virt-handler, to clearly indicate readiness [PR #4248][maiqueb] always compile KubeVirt with selinux support on pure go builds. [PR #4012][danielBelenky] Added support for the eviction API for VMIs with eviction strategy. This enables VMIs to be live-migrated when the node is drained or when the descheduler wants to move a VMI to a different node. [PR #4075][ArthurSens] Metric kubevirt_vmi_vcpu_seconds' state label is now exposed as a human-readable state instead of an integer [PR #4162][vladikr] introduce a cpuAllocationRatio config parameter to normalize the number of CPUs requested for a pod, based on the number of vCPUs [PR #4177][maiqueb] Use vishvananda/netlink instead of songgao/water to create tap devices. [PR #4092][stu-gott] Allow specifying nodeSelectors, affinity and tolerations to control where KubeVirt components will run [PR #3927][ArthurSens] Adds new metric kubevirt_vmi_memory_unused_bytes [PR #3493][vladikr] virtio-fs is being added as experimental, protected by a feature-gate that needs to be enabled in the kubevirt config by the administrator [PR #4193][mhenriks] Add snapshot.kubevirt.io to admin/edit/view roles [PR #4149][qinqon] Bump kubevirtci to k8s-1.19 [PR #3471][crobinso] Allow hiding that the VM is running on KVM, so that Nvidia graphics cards can be passed through [PR #4115][phoracek] Add conformance automation and manifest publishing [PR #3733][davidvossel] each PRs description. [PR #4082][mhenriks] VirtualMachineRestore API and implementation [PR #4154][davidvossel] Fixes issue with Service endpoints not being updated properly in place during KubeVirt updates. [PR #3289][vatsalparekh] Add option to run only VNC Proxy in virtctl [PR #4027][alicefr] Added memfd as default memory backend for hugepages. This introduces the new annotation kubevirt.io/memfd to disable memfd as default and fallback to the previous behavior. [PR #3612][ashleyschuett] Adds customizeComponents to the kubevirt api [PR #4029][cchengleo] Fix an issue which prevented virt-operator from installing monitoring resources in custom namespaces. [PR #4031][rmohr] Initial support for sonobuoy for conformance testing","title":"v0.34.0"},{"location":"latest_release_notes/#v0330","text":"Released on: Tue Sep 15 14:46:00 2020 +0000 [PR #3226][vatsalparekh] Added tests to verify custom pciAddress slots and function [PR #4048][davidvossel] Improved reliability for failed migration retries [PR #3585][mhenriks] \"virtctl image-upload pvc ...\" will create the PVC if it does not exist [PR #3945][xpivarc] KubeVirt is now being built with Go1.13.14 [PR #3845][ArthurSens] action required: The domain label from VMI metrics is being removed and may break dashboards that use the domain label to identify VMIs. Use name and namespace labels instead [PR #4011][dhiller] ppc64le arch has been disabled for the moment, see https://github.com/kubevirt/kubevirt/issues/4037 [PR #3875][stu-gott] Resources created by KubeVirt are now labelled more clearly in terms of relationship and role. [PR #3791][ashleyschuett] make node as kubevirt.io/schedulable=false on virt-handler restart [PR #3998][vladikr] the local provider is usable again. [PR #3290][maiqueb] Have virt-handler (KubeVirt agent) create the tap devices on behalf of the virt-launchers. [PR #3957][AlonaKaplan] virt-launcher support Ipv6 on dual stack cluster. [PR #3952][davidvossel] Fixes rare situation where vmi may not properly terminate if failure occurs before domain starts. [PR #3973][xpivarc] Fixes VMs with clock.timezone set. [PR #3923][danielBelenky] Add support to configure QEMU I/O mode for VMIs [PR #3889][rmohr] The status fields for our CRDs are now protected on normal PATCH and PUT operations.The /status subresource is now used where possible for status updates. [PR #3568][xpivarc] Guest swap metrics available","title":"v0.33.0"},{"location":"latest_release_notes/#v0320","text":"Released on: Tue Aug 11 19:21:56 2020 +0000 [PR #3921][vladikr] use correct memory units in libvirt xml [PR #3893][davidvossel] Adds recurring period that rsyncs virt-launcher domains with virt-handler [PR #3880][sgarbour] Better error message when input parameters are not the expected number of parameters for each argument. Help menu will popup in case the number of parameters is incorrect. [PR #3785][xpivarc] Vcpu wait metrics available [PR #3642][vatsalparekh] Add a way to update VMI Status with latest Pod IP for Masquerade bindings [PR #3636][ArthurSens] Adds kubernetes metadata.labels as VMI metrics' label [PR #3825][awels] Virtctl now prints error messages from the response body on upload errors. [PR #3830][davidvossel] Fixes re-establishing domain notify client connections when domain notify server restarts due to an error event. [PR #3778][danielBelenky] Do not emit a SyncFailed event if we fail to sync a VMI in a final state [PR #3803][andreabolognani] Not sure what to write here (see above) [PR #2694][rmohr] Use native go libraries for selinux to not rely on python-selinux tools like semanage, which are not always present. [PR #3692][victortoso] QEMU logs can now be fetched from outside the pod [PR #3738][enp0s3] Restrict creation of VMI if it has labels that are used internally by Kubevirt components. [PR #3725][danielBelenky] The tests binary is now part of the release and can be consumed from the GitHub release page. [PR #3684][rmohr] Log if critical devices, like kvm, which virt-handler wants to expose are not present on the node. [PR #3166][petrkotas] Introduce new virtctl commands: [PR #3708][andreabolognani] Make qemu work on GCE by pulling in a fix for https://bugzilla.redhat.com/show_bug.cgi?id=1822682","title":"v0.32.0"},{"location":"latest_release_notes/#v0310","text":"Released on: Thu Jul 9 16:08:18 2020 +0300 [PR 3690][davidvossel] Update go-grpc dependency to v1.30.0 in order to improve stability [PR 3628][AlonaKaplan] Avoid virt-handler crash in case of virt-launcher network configuration error [PR 3635][jean-edouard] The \"HostDisk\" feature gate has to be enabled to use hostDisks [PR 3641][vatsalparekh] Reverts kubevirt/kubevirt#3488 because CI seems to have merged it without all tests passing [PR 3488][vatsalparekh] Add a way to update VMI Status with latest Pod IP for Masquerade bindings [PR 3406][tomob] If a PVC was created by a DataVolume, it cannot be used as a Volume Source for a VM. The owning DataVolume has to be used instead. [PR 3566][kraxel] added: TigerVNC support for linux & windows [PR 3529][jean-edouard] Enabling EFI will also enable Secure Boot, which requires SMM to be enabled. [PR 3455][ashleyschuett] Add KubevirtConfiguration, MigrationConfiguration, DeveloperConfiguration and NetworkConfiguration to API-types [PR 3520][rmohr] Fix hot-looping on the VMI sync-condition if errors happen during the Scheduled phase of a VMI [PR 3220][mhenriks] API and controller/webhook for VirtualMachineSnapshots","title":"v0.31.0"},{"location":"latest_release_notes/#v0307","text":"Released on: Mon Oct 26 11:57:21 2020 -0400","title":"v0.30.7"},{"location":"latest_release_notes/#v0306","text":"Released on: Wed Aug 12 10:55:31 2020 +0200","title":"v0.30.6"},{"location":"latest_release_notes/#v0305","text":"Released on: Fri Jul 17 05:26:37 2020 -0400","title":"v0.30.5"},{"location":"latest_release_notes/#v0304","text":"Released on: Fri Jul 10 07:44:00 2020 -0400","title":"v0.30.4"},{"location":"latest_release_notes/#v0303","text":"Released on: Tue Jun 30 17:39:42 2020 -0400","title":"v0.30.3"},{"location":"latest_release_notes/#v0302","text":"Released on: Thu Jun 25 17:05:59 2020 -0400","title":"v0.30.2"},{"location":"latest_release_notes/#v0301","text":"Released on: Tue Jun 16 13:10:17 2020 -0400","title":"v0.30.1"},{"location":"latest_release_notes/#v0300","text":"Released on: Fri Jun 5 12:19:57 2020 +0200 Tests: Many more test fixes Security: Introduce a custom SELinux policy for virt-launcher More user friendly IPv6 default CIDR for IPv6 addresses Fix OpenAPI compatibility issues by switching to openapi-gen Improved support for EFI boot (configurable OVMF path and test fixes) Improved VMI IP reporting Support propagation of annotations from VMI to pods Support for more fine grained (NET_RAW( capability granting to virt-launcher Support for eventual consistency with DataVolumes","title":"v0.30.0"},{"location":"latest_release_notes/#v0292","text":"Released on: Mon May 25 21:15:30 2020 +0200","title":"v0.29.2"},{"location":"latest_release_notes/#v0291","text":"Released on: Tue May 19 10:03:27 2020 +0200","title":"v0.29.1"},{"location":"latest_release_notes/#v0290","text":"Released on: Wed May 6 15:01:57 2020 +0200 Tests: Many many test fixes Tests: Many more test fixes CI: Add lane with SELinux enabled CI: Drop PPC64 support for now Drop Genie support Drop the use of hostPaths in the virt-launcher for improved security Support priority classes for important components Support IPv6 over masquerade binding Support certificate rotations based on shared secrets Support for VM ready condition Support for advanced node labelling (supported CPU Families and machine types)","title":"v0.29.0"},{"location":"latest_release_notes/#v0280","text":"Released on: Thu Apr 9 23:01:29 2020 +0200 CI: Try to discover flaky tests before merge Fix the use of priorityClasses Fix guest memory overhead calculation Fix SR-IOV device overhead requirements Fix loading of tun module during virt-handler initialization Fixes for several test cases Fixes to support running with container_t Support for renaming a VM Support ioEmulator thread pinning Support a couple of alerts for virt-handler Support for filesystem listing using the guest agent Support for retrieving data from the guest agent Support for device role tagging Support for assigning devices to the PCI root bus Support for guest overhead override Rewrite container-disk in C to in order to reduce it's memory footprint","title":"v0.28.0"},{"location":"latest_release_notes/#v0270","text":"Released on: Fri Mar 6 22:40:34 2020 +0100 Support for more guest agent informations in the API Support setting priorityClasses on VMs Support for additional control plane alerts via prometheus Support for io and emulator thread pinning Support setting a custom SELinux type for the launcher Support to perform network configurations from handler instead of launcher Support to opt-out of auto attaching the serial console Support for different uninstall strategies for data protection Fix to let qemu run in the qemu group Fix guest agent connectivity check after i.e. live migrations","title":"v0.27.0"},{"location":"latest_release_notes/#v0265","text":"Released on: Tue Apr 14 15:07:04 2020 -0400","title":"v0.26.5"},{"location":"latest_release_notes/#v0264","text":"Released on: Mon Mar 30 03:43:48 2020 +0200","title":"v0.26.4"},{"location":"latest_release_notes/#v0263","text":"Released on: Tue Mar 10 08:57:27 2020 -0400","title":"v0.26.3"},{"location":"latest_release_notes/#v0262","text":"Released on: Tue Mar 3 12:31:56 2020 -0500","title":"v0.26.2"},{"location":"latest_release_notes/#v0261","text":"Released on: Fri Feb 14 20:42:46 2020 +0100","title":"v0.26.1"},{"location":"latest_release_notes/#v0260","text":"Released on: Fri Feb 7 09:40:07 2020 +0100 Fix incorrect ownerReferences to avoid VMs getting GCed Fixes for several tests Fix greedy permissions around Secrets by delegating them to kubelet Fix OOM infra pod by increasing it's memory request Clarify device support around live migrations Support for an uninstall strategy to protect workloads during uninstallation Support for more prometheus metrics and alert rules Support for testing SRIOV connectivity in functional tests Update Kubernetes client-go to 1.16.4 FOSSA fixes and status","title":"v0.26.0"},{"location":"latest_release_notes/#v0250","text":"Released on: Mon Jan 13 20:37:15 2020 +0100 CI: Support for Kubernetes 1.17 Support emulator thread pinning Support virtctl restart --force Support virtctl migrate to trigger live migrations from the CLI","title":"v0.25.0"},{"location":"latest_release_notes/#v0240","text":"Released on: Tue Dec 3 15:34:34 2019 +0100 CI: Support for Kubernetes 1.15 CI: Support for Kubernetes 1.16 Add and fix a couple of test cases Support for pause and unpausing VMs Update of libvirt to 5.6.0 Fix bug related to parallel scraping of Prometheus endpoints Fix to reliably test VNC","title":"v0.24.0"},{"location":"latest_release_notes/#v0233","text":"Released on: Tue Jan 21 13:17:20 2020 -0500","title":"v0.23.3"},{"location":"latest_release_notes/#v0232","text":"Released on: Fri Jan 10 10:36:36 2020 -0500","title":"v0.23.2"},{"location":"latest_release_notes/#v0231","text":"Released on: Thu Nov 28 09:36:41 2019 +0100","title":"v0.23.1"},{"location":"latest_release_notes/#v0230","text":"Released on: Mon Nov 4 16:42:54 2019 +0100 Guest OS Information is available under the VMI status now Updated to Go 1.12.8 and latest bazel Updated go-yaml to v2.2.4, which has a ddos vulnerability fixed Cleaned up and fixed CRD scheme registration Several bug fixes Many CI improvements (e.g. more logs in case of test failures)","title":"v0.23.0"},{"location":"latest_release_notes/#v0220","text":"Released on: Thu Oct 10 18:55:08 2019 +0200 Support for Nvidia GPUs and vGPUs exposed by Nvidia Kubevirt Device Plugin. VMIs now successfully start if they get a 0xfe prefixed MAC address assigned from the pod network Removed dependency on host semanage in SELinux Permissive mode Some changes as result of entering the CNCF sandbox (DCO check, FOSSA check, best practice badge) Many bug fixes and improvements in several areas CI: Introduced a OKD 4 test lane CI: Many improved tests resulting in less flakiness","title":"v0.22.0"},{"location":"latest_release_notes/#v0210","text":"Released on: Mon Sep 9 09:59:08 2019 +0200 CI: Support for Kubernetes 1.14 Many bug fixes in several areas Support for virtctl migrate Support configurable number of controller threads Support to opt-out of bridge binding for podNetwork Support for OpenShift Prometheus monitoring Support for setting more SMBIOS fields Improved containerDisk memory usage and speed Fix CRI-O memory limit Drop spc_t from launcher Add feature gates to security sensitive features","title":"v0.21.0"},{"location":"latest_release_notes/#v0208","text":"Released on: Thu Oct 3 12:03:40 2019 +0200","title":"v0.20.8"},{"location":"latest_release_notes/#v0207","text":"Released on: Fri Sep 27 15:21:56 2019 +0200","title":"v0.20.7"},{"location":"latest_release_notes/#v0206","text":"Released on: Wed Sep 11 06:09:47 2019 -0400","title":"v0.20.6"},{"location":"latest_release_notes/#v0205","text":"Released on: Thu Sep 5 17:48:59 2019 +0200","title":"v0.20.5"},{"location":"latest_release_notes/#v0204","text":"Released on: Mon Sep 2 18:55:35 2019 +0200","title":"v0.20.4"},{"location":"latest_release_notes/#v0203","text":"Released on: Tue Aug 27 16:58:15 2019 +0200","title":"v0.20.3"},{"location":"latest_release_notes/#v0202","text":"Released on: Tue Aug 20 15:51:07 2019 +0200","title":"v0.20.2"},{"location":"latest_release_notes/#v0201","text":"Released on: Fri Aug 9 19:48:17 2019 +0200 Container disks are now secure and they are not copied anymore on every start. Old container disks can still be used in the same secure way, but new container disks can't be used on older kubevirt releases Create specific SecurityContextConstraints on OKD instead of using the privileged SCC Added clone authorization check for DataVolumes with PVC source The sidecar feature is feature-gated now Use container image shasums instead of tags for KubeVirt deployments Protect control plane components against voluntary evictions with a PodDisruptionBudget of MinAvailable=1 Replaced hardcoded virtctl by using the basename of the call, this enables nicer output when installed via krew plugin package manager Added RNG device to all Fedora VMs in tests and examples (newer kernels might block bootimg while waiting for entropy) The virtual memory is now set to match the memory limit, if memory limit is specified and guest memory is not Support nftable for CoreOS Added a block-volume flag to the virtctl image-upload command Improved virtctl console/vnc data flow Removed DataVolumes feature gate in favor of auto-detecting CDI support Removed SR-IOV feature gate, it is enabled by default now VMI-related metrics have been renamed from kubevirt_vm_ to kubevirt_vmi_ to better reflect their purpose Added metric to report the VMI count Improved integration with HCO by adding a CSV generator tool and modified KubeVirt CR conditions CI Improvements: Added dedicated SR-IOV test lane Improved log gathering Reduced amount of flaky tests","title":"v0.20.1"},{"location":"latest_release_notes/#v0200","text":"Released on: Fri Aug 9 16:42:41 2019 +0200 container Disks are now secure and they are not copied anymore on every start. Old container Disks can still be used in the same secure way, but new container Disks can't be used on older kubevirt releases Create specific SecurityContextConstraints on OKD instead of using the privileged SCC Added clone authorization check for DataVolumes with PVC source The sidecar feature is feature-gated now Use container image shasum's instead of tags for KubeVirt deployments Protect control plane components against voluntary evictions with a PodDisruptionBudget of MinAvailable=1 Replaced hardcoded virtctl by using the basename of the call, this enables nicer output when installed via krew plugin package manager Added RNG device to all Fedora VMs in tests and examples (newer kernels might block boot img while waiting for entropy) The virtual memory is now set to match the memory limit, if memory limit is specified and guest memory is not Support nftable for CoreOS Added a block-volume flag to the virtctl image-upload command Improved virtctl console/vnc data flow Removed DataVolumes feature gate in favor of auto-detecting CDI support Removed SR-IOV feature gate, it is enabled by default now VMI-related metrics have been renamed from kubevirt_vm_ to kubevirt_vmi_ to better reflect their purpose Added metric to report the VMI count Improved integration with HCO by adding a CSV generator tool and modified KubeVirt CR conditions CI Improvements: Added dedicated SR-IOV test lane Improved log gathering Reduced amount of flaky tests","title":"v0.20.0"},{"location":"latest_release_notes/#v0190","text":"Released on: Fri Jul 5 12:52:16 2019 +0200 Fixes when run on kind Fixes for sub-resource RBAC Limit pod network interface bindings Many additional bug fixes in many areas Additional test cases for updates, disk types, live migration with NFS Additional test cases for memory over-commit, block storage, cpu manager, headless mode Improvements around HyperV Improved error handling for runStrategies Improved update procedure Improved network metrics reporting (packets and errors) Improved guest overhead calculation Improved SR-IOV test suite Support for live migration auto-converge Support for config-drive disks Support for setting a pullPolicy con container Disks Support for unprivileged VMs when using SR-IOV Introduction of a project security policy","title":"v0.19.0"},{"location":"latest_release_notes/#v0181","text":"Released on: Thu Jun 13 12:00:56 2019 +0200","title":"v0.18.1"},{"location":"latest_release_notes/#v0180","text":"Released on: Wed Jun 5 22:25:09 2019 +0200 Build: Use of go modules CI: Support for Kubernetes 1.13 Countless test cases fixes and additions Several smaller bug fixes Improved upgrade documentation","title":"v0.18.0"},{"location":"latest_release_notes/#v0174","text":"Released on: Tue Jun 25 07:49:12 2019 -0400","title":"v0.17.4"},{"location":"latest_release_notes/#v0173","text":"Released on: Wed Jun 19 12:00:45 2019 -0400","title":"v0.17.3"},{"location":"latest_release_notes/#v0172","text":"Released on: Wed Jun 5 08:12:04 2019 -0400","title":"v0.17.2"},{"location":"latest_release_notes/#v0171","text":"Released on: Tue Jun 4 14:41:10 2019 -0400","title":"v0.17.1"},{"location":"latest_release_notes/#v0170","text":"Released on: Mon May 6 16:18:01 2019 +0200 Several test case additions Improved virt-controller node distribution Improved support between version migrations Support for a configurable MachineType default Support for live-migration of a VM on node taints Support for VM swap metrics Support for versioned virt-launcher / virt-handler communication Support for HyperV flags Support for different VM run strategies (i.e manual and rerunOnFailure) Several fixes for live-migration (TLS support, protected pods)","title":"v0.17.0"},{"location":"latest_release_notes/#v0163","text":"Released on: Thu May 2 23:51:08 2019 +0200","title":"v0.16.3"},{"location":"latest_release_notes/#v0162","text":"Released on: Fri Apr 26 12:24:33 2019 +0200","title":"v0.16.2"},{"location":"latest_release_notes/#v0161","text":"Released on: Tue Apr 23 19:31:19 2019 +0200","title":"v0.16.1"},{"location":"latest_release_notes/#v0160","text":"Released on: Fri Apr 5 23:18:22 2019 +0200 Bazel fixes Initial work to support upgrades (not finalized) Initial support for HyperV features Support propagation of MAC addresses to multus Support live migration cancellation Support for table input devices Support for generating OLM metadata Support for triggering VM live migration on node taints","title":"v0.16.0"},{"location":"latest_release_notes/#v0150","text":"Released on: Tue Mar 5 10:35:08 2019 +0100 CI: Several fixes Fix configurable number of KVM devices Narrow virt-handler permissions Use bazel for development builds Support for live migration with shared and non-shared disks Support for live migration progress tracking Support for EFI boot Support for libvirt 5.0 Support for extra DHCP options Support for a hook to manipulate cloud-init metadata Support setting a VM serial number Support for exposing infra and VM metrics Support for a tablet input device Support for extra CPU flags Support for ignition metadata Support to set a default CPU model Update to go 1.11.5","title":"v0.15.0"},{"location":"latest_release_notes/#v0140","text":"Released on: Mon Feb 4 22:04:14 2019 +0100 CI: Several stabilizing fixes docs: Document the KubeVirt Razor build: golang update Update to Kubernetes 1.12 Update CDI Support for Ready and Created Operator conditions Support (basic) EFI Support for generating cloud-init network-config","title":"v0.14.0"},{"location":"latest_release_notes/#v0137","text":"Released on: Mon Oct 28 17:02:35 2019 -0400","title":"v0.13.7"},{"location":"latest_release_notes/#v0136","text":"Released on: Wed Sep 25 17:19:44 2019 +0200","title":"v0.13.6"},{"location":"latest_release_notes/#v0135","text":"Released on: Thu Aug 1 11:25:00 2019 -0400","title":"v0.13.5"},{"location":"latest_release_notes/#v0134","text":"Released on: Thu Aug 1 09:52:35 2019 -0400","title":"v0.13.4"},{"location":"latest_release_notes/#v0133","text":"Released on: Mon Feb 4 15:46:48 2019 -0500","title":"v0.13.3"},{"location":"latest_release_notes/#v0132","text":"Released on: Thu Jan 24 23:24:06 2019 +0100","title":"v0.13.2"},{"location":"latest_release_notes/#v0131","text":"Released on: Thu Jan 24 11:16:20 2019 +0100","title":"v0.13.1"},{"location":"latest_release_notes/#v0130","text":"Released on: Tue Jan 15 08:26:25 2019 +0100 CI: Fix virt-api race API: Remove volumeName from disks","title":"v0.13.0"},{"location":"latest_release_notes/#v0120","text":"Released on: Fri Jan 11 22:22:02 2019 +0100 Introduce a KubeVirt Operator for KubeVirt life-cycle management Introduce dedicated kubevirt namespace Support VMI ready conditions Support vCPU threads and sockets Support scale and HPA for VMIRs Support to pass NTP related DHCP options Support guest IP address reporting via qemu guest agent Support for live migration with shared storage Support scheduling of VMs based on CPU family Support masquerade network interface binding","title":"v0.12.0"},{"location":"latest_release_notes/#v0111","text":"Released on: Thu Dec 13 10:21:56 2018 +0200","title":"v0.11.1"},{"location":"latest_release_notes/#v0110","text":"Released on: Thu Dec 6 10:15:51 2018 +0100 API: registryDisk got renamed to containerDisk CI: User OKD 3.11 Fix: Tolerate if the PVC has less capacity than expected Aligned to use ownerReferences Update to libvirt-4.10.0 Support for VNC on MAC OSX Support for network SR-IOV interfaces Support for custom DHCP options Support for VM restarts via a custom endpoint Support for liveness and readiness probes","title":"v0.11.0"},{"location":"latest_release_notes/#v0100","text":"Released on: Thu Nov 8 15:21:34 2018 +0100 Support for vhost-net Support for block multi-queue Support for custom PCI addresses for virtio devices Support for deploying KubeVirt to a custom namespace Support for ServiceAccount token disks Support for multus backed networks Support for genie backed networks Support for kuryr backed networks Support for block PVs Support for configurable disk device caches Support for pinned IO threads Support for virtio net multi-queue Support for image upload (depending on CDI) Support for custom entity lists with more VM details (custom columns) Support for IP and MAC address reporting of all vNICs Basic support for guest agent status reporting More structured logging Better libvirt error reporting Stricter CR validation Better ownership references Several test improvements","title":"v0.10.0"},{"location":"latest_release_notes/#v096","text":"Released on: Thu Nov 22 17:14:18 2018 +0100","title":"v0.9.6"},{"location":"latest_release_notes/#v095","text":"Released on: Thu Nov 8 09:57:48 2018 +0100","title":"v0.9.5"},{"location":"latest_release_notes/#v094","text":"Released on: Wed Nov 7 08:22:14 2018 -0500","title":"v0.9.4"},{"location":"latest_release_notes/#v093","text":"Released on: Mon Oct 22 09:04:02 2018 -0400","title":"v0.9.3"},{"location":"latest_release_notes/#v092","text":"Released on: Thu Oct 18 12:14:09 2018 +0200","title":"v0.9.2"},{"location":"latest_release_notes/#v091","text":"Released on: Fri Oct 5 09:01:51 2018 +0200","title":"v0.9.1"},{"location":"latest_release_notes/#v090","text":"Released on: Thu Oct 4 14:42:28 2018 +0200 CI: NetworkPolicy tests CI: Support for an external provider (use a preconfigured cluster for tests) Fix virtctl console issues with CRI-O Support to initialize empty PVs Support for basic CPU pinning Support for setting IO Threads Support for block volumes Move preset logic to mutating webhook Introduce basic metrics reporting using prometheus metrics Many stabilizing fixes in many places","title":"v0.9.0"},{"location":"latest_release_notes/#v080","text":"Released on: Thu Sep 6 14:25:22 2018 +0200 Support for DataVolume Support for a subprotocol for web browser terminals Support for virtio-rng Support disconnected VMs Support for setting host model Support for host CPU passthrough Support setting a vNICs mac and PCI address Support for memory over-commit Support booting from network devices Use less devices by default, aka disable unused ones Improved VMI shutdown status More logging to improve debugability A lot of small fixes, including typos and documentation fixes Race detection in tests Hook improvements Update to use Fedora 28 (includes updates of dependencies like libvirt and qemu) Move CI to support Kubernetes 1.11","title":"v0.8.0"},{"location":"latest_release_notes/#v070","text":"Released on: Wed Jul 4 17:41:33 2018 +0200 CI: Move test storage to hostPath CI: Add support for Kubernetes 1.10.4 CI: Improved network tests for multiple-interfaces CI: Drop Origin 3.9 support CI: Add test for testing templates on Origin VM to VMI rename VM affinity and anti-affinity Add awareness for multiple networks Add hugepage support Add device-plugin based kvm Add support for setting the network interface model Add (basic and initial) Kubernetes compatible networking approach (SLIRP) Add role aggregation for our roles Add support for setting a disks serial number Add support for specifying the CPU model Add support for setting an network interfaces MAC address Relocate binaries for FHS conformance Logging improvements Template fixes Fix OpenShift CRD validation virtctl: Improve vnc logging improvements virtctl: Add expose virtctl: Use PATCH instead of PUT","title":"v0.7.0"},{"location":"latest_release_notes/#v064","text":"Released on: Tue Aug 21 17:29:28 2018 +0300","title":"v0.6.4"},{"location":"latest_release_notes/#v063","text":"Released on: Mon Jul 30 16:14:22 2018 +0200","title":"v0.6.3"},{"location":"latest_release_notes/#v062","text":"Released on: Wed Jul 4 17:49:37 2018 +0200 Binary relocation for packaging QEMU Process detection Role aggregation CPU Model selection VM Rename fix","title":"v0.6.2"},{"location":"latest_release_notes/#v061","text":"Released on: Mon Jun 18 17:07:48 2018 -0400","title":"v0.6.1"},{"location":"latest_release_notes/#v060","text":"Released on: Mon Jun 11 09:30:28 2018 +0200 A range of flakiness reducing test fixes Vagrant setup got deprecated Updated Docker and CentOS versions Add Kubernetes 1.10.3 to test matrix A couple of ginkgo concurrency fixes A couple of spelling fixes A range if infra updates Use /dev/kvm if possible, otherwise fallback to emulation Add default view/edit/admin RBAC Roles Network MTU fixes CD-ROM drives are now read-only Secrets can now be correctly referenced on VMs Add disk boot ordering Add virtctl version Add virtctl expose Fix virtual machine memory calculations Add basic virtual machine Network API","title":"v0.6.0"},{"location":"latest_release_notes/#v050","text":"Released on: Fri May 4 18:25:32 2018 +0200 Better controller health signaling Better virtctl error messages Improvements to enable CRI-O support Run CI on stable OpenShift Add test coverage for multiple PVCs Improved controller life-cycle guarantees Add Webhook validation Add tests coverage for node eviction OfflineVirtualMachine status improvements RegistryDisk API update","title":"v0.5.0"},{"location":"latest_release_notes/#v041","text":"Released on: Thu Apr 12 11:46:09 2018 +0200 VM shutdown fixes and tests Functional test for CRD validation Windows VM test DHCP link-local change","title":"v0.4.1"},{"location":"latest_release_notes/#v040","text":"Released on: Fri Apr 6 16:40:31 2018 +0200 Fix several networking issues Add and enable OpenShift support to CI Add conditional Windows tests (if an image is present) Add subresources for console access virtctl config alignment with kubectl Fix API reference generation Stable UUIDs for OfflineVirtualMachines Build virtctl for MacOS and Windows Set default architecture to x86_64 Major improvement to the CI infrastructure (all containerized) virtctl convenience functions for starting and stopping a VM","title":"v0.4.0"},{"location":"latest_release_notes/#v030","text":"Released on: Thu Mar 8 10:21:57 2018 +0100 Kubernetes compatible networking Kubernetes compatible PV based storage VirtualMachinePresets support OfflineVirtualMachine support RBAC improvements Switch to q35 machine type by default A large number of test and CI fixes Ephemeral disk support","title":"v0.3.0"},{"location":"latest_release_notes/#v020","text":"Released on: Fri Jan 5 16:30:45 2018 +0100 VM launch and shutdown flow improvements VirtualMachine API redesign Removal of HAProxy Redesign of VNC/Console access Initial support for different vagrant providers","title":"v0.2.0"},{"location":"latest_release_notes/#v010","text":"Released on: Fri Dec 8 20:43:06 2017 +0100 Many API improvements for a proper OpenAPI reference Add watchdog support Drastically improve the deployment on non-vagrant setups Dropped nodeSelectors Separated inner component deployment from edge component deployment Created separate manifests for developer, test, and release deployments Moved components to kube-system namespace Improved and unified flag parsing","title":"v0.1.0"},{"location":"latest_release_notes/#v004","text":"Released on: Tue Nov 7 11:51:45 2017 +0100 Add support for node affinity to VM.Spec Add OpenAPI specification Drop swagger 1.2 specification virt-launcher refactoring Leader election mechanism for virt-controller Move from glide to dep for dependency management Improve virt-handler synchronization loops Add support for running the functional tests on oVirt infrastructure Several tests fixes (spice, cleanup, ...) Add console test tool Improve libvirt event notification","title":"v0.0.4"},{"location":"latest_release_notes/#v003","text":"Released on: Fri Oct 6 10:21:16 2017 +0200 Containerized binary builds Socket based container detection cloud-init support Container based ephemeral disk support Basic RBAC profile client-go updates Rename of VM to VirtualMachine Introduction of VirtualMachineReplicaSet Improved migration events Improved API documentation","title":"v0.0.3"},{"location":"latest_release_notes/#v002","text":"Released on: Mon Sep 4 21:12:46 2017 +0200 Usage of CRDs Moved libvirt to a pod Introduction of virtctl Use glide instead of govendor Container based ephemeral disks Contributing guide improvements Support for Kubernetes Namespaces","title":"v0.0.2"},{"location":"web_console/","text":"Web Console \u00b6 Note By deploying KubeVirt on top of OpenShift the user can benefit from the OpenShift web console functionality. Managing Virtual Machines \u00b6 Creating a Virtual Machine with the Interactive Wizard \u00b6 The web console features an interactive wizard that guides you through Basic Settings , Networking , and Storage screens to simplify the process of creating virtual machines. All required fields are marked with a * . The wizard prevents you from moving to the next screen until the required fields have been completed. NICs and storage disks can be created and attached to virtual machines after they have been created. *Bootable Disk *. If either URL or Container are selected as the Provision Source in the Basic Settings screen, a rootdisk disk is created and attached to the virtual machine as the Bootable Disk . You can modify the rootdisk but you cannot remove it. A Bootable Disk is not required for virtual machines provisioned from a PXE source if there are no disks attached to the virtual machine. If one or more disks are attached to the virtual machine, you must select one as the Bootable Disk . Click Workloads > Virtual Machines from the side menu. Click Create Virtual Machine and select Create with Wizard . Fill in all required Basic Settings . Selecting a Template automatically fills in these fields. Click Next to progress to the Networking screen. A nic0 NIC is attached by default. (Optional) Click Create NIC to create additional NICs. (Optional) You can remove any or all NICs by clicking the button and selecting Remove NIC . A virtual machine does not need a NIC attached to be created. NICs can be created after the virtual machine has been created. Click Next to progress to the Storage screen. (Optional) Click Create Disk to create additional disks. These disks can be removed by clicking the button and selecting Remove Disk . (Optional) Click on a disk to modify available fields. Click the button to save the update. (Optional) Click Attach Disk to choose an available disk from the Select Storage drop-down list. Click Create Virtual Machine > . The Results screen displays the JSON configuration file for the virtual machine. The virtual machine should now be listed in Workloads > Virtual Machines . Creating a Virtual Machine Using a YAML Configuration File \u00b6 A virtual machine can also be created by writing or pasting a YAML configuration file in the web console in the Workloads > Virtual Machines screen. A valid example virtual machine configuration is provided by default whenever you open the YAML edit screen. If your YAML configuration is invalid when you click Create , an error message indicates the parameter in which the error occurs. Only one error is shown at a time. Note: Navigating away from the YAML screen while editing cancels any changes to the configuration you have made. Click Workloads > Virtual Machines from the side menu. Click Create Virtual Machine and select Create from YAML . Write or paste your virtual machine configuration in the editable window. Alternatively, use the example virtual machine provided by default in the YAML screen. (Optional) Click Download to download the YAML configuration file to your local machine in its present state. Click Create to create the virtual machine. The virtual machine should now be listed in Workloads > Virtual Machines . Editing a Virtual Machine \u00b6 You can edit some values of a virtual machine in the web console, either by editing the YAML directly , or from the Virtual Machine Overview screen. When editing from the Virtual Machine Overview screen, the virtual machine must be Off . Click Workloads > Virtual Machines from the side menu. Select a Virtual Machine. Click Edit to make editable fields available. You can change the Flavor , but only to Custom , which provides additional fields for CPU and Memory . Click Save . The updated values are shown after the operation is processed. Editing the YAML of a Virtual Machine \u00b6 You can edit the YAML configuration of a virtual machine directly within the web console. Not all parameters can be updated. If you edit values that cannot be changed and click Save , an error message indicates the parameter that was not able to be updated. The YAML configuration can be edited while the virtual machine is Running , however the changes will only take effect after the virtual machine has been restarted. Note: Navigating away from the YAML screen while editing cancels any changes to the configuration you have made. Click Workloads > Virtual Machine from the side menu. Select a virtual machine. Click the YAML tab to display the editable configuration. (Optional) You can click Download to download the YAML file to your local machine in its current state. Edit the file and click Save . A confirmation message shows that the modification has been successful, including the updated version number for the object. Viewing the Events of a Virtual Machine \u00b6 You can view the events stream for a running virtual machine from the Virtual Machine Details screen of the web console. The button pauses the events stream. The button continues a paused events stream. Click Workloads > Virtual Machines from the side menu. Select a virtual machine. Click Events to view all events for the virtual machine. Deleting a Virtual Machine \u00b6 Deleting a virtual machine permanently removes it from the cluster. Delete a virtual machine using the button of the virtual machine in the Workloads > Virtual Machines list, or using the button of the Virtual Machine Details screen. Click Workloads > Virtual Machines from the side menu. Click the button of the virtual machine you wish to delete and select Delete Virtual Machine . Alternatively, click the virtual machine name to open the Virtual Machine Details screen and click > Delete Virtual Machine . In the confirmation pop-up window, click Delete to permanently delete the virtual machine. Controlling Virtual Machines \u00b6 Starting a Virtual Machine \u00b6 Virtual machines can be started from the button found on each virtual machine in the Workloads > Virtual Machines list, or from the button on the Virtual Machine Details screen. Click Workloads > Virtual Machine from the side menu. Click the button of the virtual machine and select Start Virtual Machine . Alternatively, click the virtual machine name to open the Virtual Machine Details screen and click and select Start Virtual Machine . In the confirmation pop-up window, click Start to start the virtual machine. Stopping a Virtual Machine \u00b6 A running virtual machine can be stopped using the button found on each virtual machine in the Workloads > Virtual Machines list, or from the button on the Virtual Machine Details screen. Click Workloads > Virtual Machine from the side menu. Click the button of the virtual machine and select Stop Virtual Machine . Alternatively, click the virtual machine name to open the Virtual Machine Details screen and click and select Stop Virtual Machine . In the confirmation pop-up window, click Stop to stop the virtual machine. Restarting a Virtual Machine \u00b6 A running virtual machine can be restarted from the button found on each virtual machine in the Workloads > Virtual Machines list, or from the button on the Virtual Machine Details screen. Important: Do not restart a virtual machine while it has a status of Importing . This will result in an error for the virtual machine and is a known issue . Click Workloads > Virtual Machine from the side menu. Click the button of the virtual machine and select Restart Virtual Machine . Alternatively, click the virtual machine name to open the Virtual Machine Details screen, click , and select Restart Virtual Machine . In the confirmation pop-up window, click Restart to restart the virtual machine. Live Migrating a Virtual Machine \u00b6 Virtual machines can be live migrated to a different node from the button found on each virtual machine in the Workloads > Virtual Machines list, or from the button on the Virtual Machine Details screen. Click Workloads > Virtual Machine from the side menu. Click the button of the virtual machine and select Migrate Virtual Machine . Alternatively, click the virtual machine name to open the Virtual Machine Details screen and click and select Migrate Virtual Machine . In the confirmation pop-up window, click Migrate to migrate the virtual machine. Cancelling a Virtual Machine Live Migration \u00b6 A live migration of the virtual machine can be cancelled using the button found on each virtual machine in the Workloads > Virtual Machines list, or from the button on the Cancel Virtual Machine Migration screen. Click Workloads > Virtual Machine from the side menu. Click the button of the virtual machine and select Cancel Virtual Machine Migration . Alternatively, click the virtual machine name to open the Virtual Machine Details screen and click and select Cancel Virtual Machine Migration . In the confirmation pop-up window, click Cancel Migration to cancel the virtual machine live migration. Accessing Virtual Machine Consoles \u00b6 Virtual Machine Console Sessions \u00b6 You can connect to the VNC, Serial, and RDP consoles of a running virtual machine from the Consoles tab in the Virtual Machine Details screen of the web console. There are three choices available: the graphical VNC Console , the Serial Console , and Desktop Viewer which can be used to connect to the RDP Console (Windows only). The VNC Console opens by default whenever you navigate to the Consoles tab. You can switch between the consoles using the VNC Console | Serial Console | Desktop Viewer drop-down list. Console sessions remain active in the background unless they are disconnected. When the Disconnect before switching checkbox is active and you switch consoles, the current console session is disconnected and a new session with the selected console connects to the virtual machine. This ensures only one console session is open at a time. Options for the VNC Console . The Send Key button lists key combinations to send to the virtual machine. Options for the Serial Console . Use the Disconnect button to manually disconnect the Serial Console session from the virtual machine. Use the Reconnect button to manually open a Serial Console session to the virtual machine. Options for the Desktop Viewer . The Network Interface drop-down allows you to choose the network interface for the VM to which you want to connect. Connecting to the Serial Console \u00b6 Connect to the Serial Console of a running virtual machine from the Consoles tab in the Virtual Machine Details screen of the web console. Click Workloads > Virtual Machines from the side menu. Select a virtual machine. Click Consoles . The VNC console opens by default. Click the VNC Console drop-down list and select Serial Console . Connecting to the VNC Console \u00b6 Connect to the VNC console of a running virtual machine from the Consoles tab in the Virtual Machine Details screen of the web console. Click Workloads > Virtual Machines from the side menu. Select a virtual machine. Click Consoles . The VNC console opens by default. Using Desktop Viewer to connect to RDP Console (Windows only) \u00b6 The Desktop Viewer option can be used to connect to the RDP console of a running Windows virtual machine. Click Workloads > Virtual Machines from the side menu. Select a virtual machine. Click Consoles . The VNC console opens by default. Select Desktop Viewer in the drop-down list. Choose the network interface you want to connect to from the Network Interface drop-down. If the L2 network interface is chosen, you will need to install qemu-guest-agent on the VM. If the pod network interface is chosen, you will need to create a service to expose port 3389 (RDP) on the VM. Follow the on-screen instructions to do so. Click Launch Remote Desktop to download the RDP file, which can then be opened in the RDP client of your choice. Managing Virtual Machine NICs \u00b6 Creating a NIC for a Virtual Machine \u00b6 Create and attach additional NICs to a virtual machine from the web console. Click Workloads > Virtual Machines from the side menu. Select a virtual machine. Click Network Interfaces to display the NICs already attached to the virtual machine. Click Create NIC to create a new slot in the list. Fill in the NAME , NETWORK , MAC ADDRESS , and BINDING METHOD details for the new NIC. Click the button to save and attach the NIC to the virtual machine. Deleting a NIC from a Virtual Machine \u00b6 Deleting a NIC from a virtual machine detaches and permanently deletes the NIC. Click Workloads > Virtual Machines from the side menu. Select a virtual machine. Click Network Interfaces to display the NICs already attached to the virtual machine. Click the button of the NIC you wish to delete and select Delete . In the confirmation pop-up window, click Delete to detach and delete the NIC. Managing Virtual Machine Disks \u00b6 Creating a Disk for a Virtual Machine \u00b6 Create and attach additional storage disks to a virtual machine from the web console. Click Workloads > Virtual Machines from the side menu. Select a virtual machine. Click Disks to display the disks already attached to the virtual machine. Click Create Disk to create a new slot in the list. Fill in the NAME , SIZE , and optional STORAGE CLASS details for the new disk. Click the button to save and attach the disk to the virtual machine. Deleting a Disk from a Virtual Machine \u00b6 Deleting a disk from a virtual machine detaches and permanently deletes the disk. Click Workloads > Virtual Machines from the side menu. Select a virtual machine. Click Disks to display the disks already attached to the virtual machine. Click the button of the disk you wish to delete and select Delete . Click Confirm to detach and delete the disk. Virtual Machine Templates in the Web Console \u00b6 Creating a Virtual Machine Template with the Interactive Wizard \u00b6 Virtual machine templates are an easy way to create multiple virtual machines with similar configuration. After a template is created, reference the template when creating virtual machines . The web console features an interactive wizard that guides you through Basic Settings , Networking , and Storage screens to simplify the process of creating virtual machine templates. All required fields are marked with a * . The wizard prevents you from moving to the next screen until the required fields have been completed. NICs and storage disks can be created and attached to virtual machines after they have been created. *Bootable Disk *. If either URL or Container are selected as the Provision Source in the Basic Settings screen, a rootdisk disk is created and attached to virtual machines as the Bootable Disk . You can modify the rootdisk but you cannot remove it. A Bootable Disk is not required for virtual machines provisioned from a PXE source if there are no disks attached to the virtual machine. If one or more disks are attached to the virtual machine, you must select one as the Bootable Disk . Click Workloads > Virtual Machine Templates from the side menu. Click Create Template and select Create with Wizard . Fill in all required Basic Settings . Click Next to progress to the Networking screen. An nic0 NIC is attached by default. (Optional) Click Create NIC to create additional NICs. (Optional) You can remove any or all NICs by clicking the button and selecting Remove NIC . Virtual machines created from a template do not need a NIC attached. NICs can be created after a virtual machine has been created. Click Next to progress to the Storage screen. (Optional) Click Create Disk to create additional disks. These disks can be removed by clicking the button and selecting Remove Disk . (Optional) Click on a disk to modify available fields. Click the button to save the update. (Optional) Click Attach Disk to choose an available disk from the Select Storage drop-down list. Click Create Virtual Machine Template > . The Results screen displays the JSON configuration file for the virtual machine template. The template should now be listed in Workloads > Virtual Machine Templates . Editing the YAML of a Virtual Machine Template \u00b6 You can edit the YAML configuration of a virtual machine template directly within the web console. Not all parameters can be updated. If you edit values that cannot be changed and click Save , an error message shows, indicating the parameter that was not able to be updated. Note: Navigating away from the YAML screen while editing cancels any changes to the configuration you have made. Click Workloads > Virtual Machine Template from the side menu. Select a template. Click the YAML tab to display the editable configuration. (Optional) You can click Download to download the YAML file to your local machine in its current state. Edit the file and click Save . A confirmation message shows the modification has been successful, including the updated version number for the object. Deleting a Virtual Machine Template \u00b6 Deleting a virtual machine template permanently removes it from the cluster. Delete a virtual machine template using the button found on each template in the Workloads > Virtual Machines Templates list, or using the button of the Virtual Machine Templates Details screen. Click Workloads > Virtual Machine Templates from the side menu. Click the button of the template you wish to delete and select Delete Template . Alternatively, click the template name to open the Virtual Machine Template Details screen and click > Delete Template . In the confirmation pop-up window, click Delete to permanently delete the template. Reference \u00b6 Virtual Machine Wizard Fields \u00b6 Name Parameter Description Name The name of the virtual machine * Alphanumeric characters only (63 characters max) Description Optional description field Template Template from which to create the virtual machine * Selecting a template will automatically fill other fields Provision Source PXE URL Provision virtual machine from an image available from an HTTP or S3 endpoint Container Provision virtual machine from a bootable operating system container located in a registry accessible from the cluster * Example: _kubevirt/cirros-registry-disk-demo_ Operating System A list of operating systems available in the cluster * This is the primary operating system for the virtual machine Flavor Presets that determine the amount of CPU and memory allocated to the virtual machine Workload Profile A general configuration that balances performance and compatibility for a broad range of workloads highperformance The virtual machine has a more efficient configuration optimized for high performance loads Start virtual machine on creation Select this checkbox to automatically start the virtual machine upon creation cloud-init Select this checkbox to enable the cloud-init fields Virtual Machine Template Wizard Fields \u00b6 Name Parameter Description Name The name of the virtual machine * Alphanumeric characters only (63 characters max) Description Optional description field Provision Source PXE Template Template from which to create the virtual machine * Selecting a template will automatically fill other fields. Provision Source Provision virtual machine from PXE menu * Requires a PXE-capable NIC in the cluster URL Provision virtual machines from an image available from a HTTP or S3 endpoint. Container Provision virtual machines from a bootable operating system container located in a registry accessible from the cluster * Example: _kubevirt/cirros-registry-disk-demo_ Operating System A list of operating systems available in the cluster. This is the primary operating system for the virtual machine. Flavor small, medium, large, tiny, Custom Presets that determine the amount of CPU and memory allocated to the virtual machine Workload Profile generic A general configuration that balances performance and compatibility for a broad range of workloads highperformance Virtual machines have a more efficient configuration optimized for high performance loads cloud-init Select this checkbox to enable the cloud-init fields Cloud-init Fields \u00b6 Name Description Hostname Sets a specific hostname for the virtual machine Authenticated SSH Keys The user's public key * This will be copied to ~/.ssh/authorized_keys on the virtual machine Use custom script Replaces other options with a textbox into which you can paste a custom cloud-init script Networking Fields \u00b6 Name Description Create NIC Create a new NIC for the virtual machine NIC NAME Name for the NIC MAC ADDRESS MAC address for the network interface * If a MAC address is not specified an ephemeral address is generated for the session NETWORK CONFIGURATION List of available NetworkAttachmentDefinition objects BINDING METHOD List of available binding methods For pod networking, the possible values are masquerade, bridge, and sriov For other networks, the possible values are bridge and sriov PXE NIC List of PXE-capable networks * Only visible if PXE has been selected as the Provision Source Storage Fields \u00b6 Name Description Create Disk Create a new disk for the virtual machine Attach Disk Select an existing disk from a list of available PVCs to attach to the virtual machine DISK NAME Name of the disk SIZE (GiB) Size in GiB of the disk STORAGE CLASS Name of the underlying StorageClass Bootable Disk List of available disks from which the virtual machine will boot * This is locked to rootdisk if the Provision Source of the virtual machine is URL or Container Virtual Machine Actions \u00b6 Action Available in state Description Start Virtual Machine Off Start the virtual machine Stop Virtual Machine Running or Other Stop the virtual machine Restart Virtual Machine Running or Other Restart the running virtual machine Delete Virtual Machine All Permanently delete the virtual machine from the cluster Migrate Virtual Machine Running Live migrates the virtual machine to an another node Cancel Virtual Machine Migration Migrating Cancels the migration of the virtual machine PXE Booting with a Specified MAC Address \u00b6 Network booting allows a computer to boot and load an operating system or other program without requiring a locally attached storage device. For example, you can use it to choose your desired OS image from a PXE server when deploying a new host. Configure a PXE network on the cluster: Create NetworkAttachmentDefinition of PXE network pxe-net-conf : apiVersion: \"k8s.cni.cncf.io/v1\" kind: NetworkAttachmentDefinition metadata: name: pxe-net-conf spec: config: '{ \"cniVersion\": \"0.3.1\", \"type\": \"ovs\", \"bridge\": \"br1\" }' Note: In this example, the virtual machine instance (VMI) will be attached through a trunk port to the Open vSwitch bridge <br1> . Create Open vSwitch bridge <br1> and connect it to interface <eth1> , which is connected to a network that allows for PXE booting: $ ovs-vsctl add-br br1 $ ovs-vsctl add-port br1 eth1 $ ovs-vsctl show 8d004495-ea9a-44e1-b00c-3b65648dae5f Bridge br1 Port br1 Interface br1 type: internal Port \"eth1\" Interface \"eth1\" ovs_version: \"2.8.1\" Note: This bridge must be configured on all nodes. If it is only available on a subset of nodes, make sure that VMIs have nodeSelector constraints in place. Edit the virtual machine instance configuration file to include the details of the interface and network. Specify the network and MAC address if required by the PXE server. A value is assigned automatically if the MAC address is not specified; however, note that at this time, MAC addresses that are assigned automatically are not persistent. Ensure that bootOrder is set to 1 so that the interface boots first. In this example, the interface is connected to a network called <pxe-net> : interfaces: - masquerade: {} name: default - bridge: {} name: pxe-net macAddress: de:00:00:00:00:de bootOrder: 1 Note: Boot order is global for interfaces and disks. Assign a boot device number to the disk to ensure proper booting after OS provisioning. Set the disk bootOrder value to 2 : devices: disks: - disk: bus: virtio name: containerdisk bootOrder: 2 Specify that the network is connected to the previously created NetworkAttachmentDefinition . In this scenario, <pxe-net> is connected to the NetworkAttachmentDefinition called <pxe-net-conf> : networks: - name: default pod: {} - name: pxe-net multus: networkName: pxe-net-conf Create the virtual machine instance: $ oc create -f vmi-pxe-boot.yaml virtualmachineinstance.kubevirt.io \"vmi-pxe-boot\" created Wait for the virtual machine instance to run: $ oc get vmi vmi-pxe-boot -o yaml | grep -i phase phase: Running View the virtual machine instance using VNC: $ virtctl vnc vmi-pxe-boot Watch the boot screen to verify that the PXE boot is successful. Log in to the VMI: $ virtctl console vmi-pxe-boot Verify the interfaces and MAC address on the VM, and that the interface connected to the bridge has the specified MAC address. In this case, we used eth1 for the PXE boot, without an IP address. The other $ ip addr ... 3. eth1: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether de:00:00:00:00:de brd ff:ff:ff:ff:ff:ff","title":"Web Console"},{"location":"web_console/#web-console","text":"Note By deploying KubeVirt on top of OpenShift the user can benefit from the OpenShift web console functionality.","title":"Web Console"},{"location":"web_console/#managing-virtual-machines","text":"","title":"Managing Virtual Machines"},{"location":"web_console/#creating-a-virtual-machine-with-the-interactive-wizard","text":"The web console features an interactive wizard that guides you through Basic Settings , Networking , and Storage screens to simplify the process of creating virtual machines. All required fields are marked with a * . The wizard prevents you from moving to the next screen until the required fields have been completed. NICs and storage disks can be created and attached to virtual machines after they have been created. *Bootable Disk *. If either URL or Container are selected as the Provision Source in the Basic Settings screen, a rootdisk disk is created and attached to the virtual machine as the Bootable Disk . You can modify the rootdisk but you cannot remove it. A Bootable Disk is not required for virtual machines provisioned from a PXE source if there are no disks attached to the virtual machine. If one or more disks are attached to the virtual machine, you must select one as the Bootable Disk . Click Workloads > Virtual Machines from the side menu. Click Create Virtual Machine and select Create with Wizard . Fill in all required Basic Settings . Selecting a Template automatically fills in these fields. Click Next to progress to the Networking screen. A nic0 NIC is attached by default. (Optional) Click Create NIC to create additional NICs. (Optional) You can remove any or all NICs by clicking the button and selecting Remove NIC . A virtual machine does not need a NIC attached to be created. NICs can be created after the virtual machine has been created. Click Next to progress to the Storage screen. (Optional) Click Create Disk to create additional disks. These disks can be removed by clicking the button and selecting Remove Disk . (Optional) Click on a disk to modify available fields. Click the button to save the update. (Optional) Click Attach Disk to choose an available disk from the Select Storage drop-down list. Click Create Virtual Machine > . The Results screen displays the JSON configuration file for the virtual machine. The virtual machine should now be listed in Workloads > Virtual Machines .","title":"Creating a Virtual Machine with the Interactive Wizard"},{"location":"web_console/#creating-a-virtual-machine-using-a-yaml-configuration-file","text":"A virtual machine can also be created by writing or pasting a YAML configuration file in the web console in the Workloads > Virtual Machines screen. A valid example virtual machine configuration is provided by default whenever you open the YAML edit screen. If your YAML configuration is invalid when you click Create , an error message indicates the parameter in which the error occurs. Only one error is shown at a time. Note: Navigating away from the YAML screen while editing cancels any changes to the configuration you have made. Click Workloads > Virtual Machines from the side menu. Click Create Virtual Machine and select Create from YAML . Write or paste your virtual machine configuration in the editable window. Alternatively, use the example virtual machine provided by default in the YAML screen. (Optional) Click Download to download the YAML configuration file to your local machine in its present state. Click Create to create the virtual machine. The virtual machine should now be listed in Workloads > Virtual Machines .","title":"Creating a Virtual Machine Using a YAML Configuration File"},{"location":"web_console/#editing-a-virtual-machine","text":"You can edit some values of a virtual machine in the web console, either by editing the YAML directly , or from the Virtual Machine Overview screen. When editing from the Virtual Machine Overview screen, the virtual machine must be Off . Click Workloads > Virtual Machines from the side menu. Select a Virtual Machine. Click Edit to make editable fields available. You can change the Flavor , but only to Custom , which provides additional fields for CPU and Memory . Click Save . The updated values are shown after the operation is processed.","title":"Editing a Virtual Machine"},{"location":"web_console/#editing-the-yaml-of-a-virtual-machine","text":"You can edit the YAML configuration of a virtual machine directly within the web console. Not all parameters can be updated. If you edit values that cannot be changed and click Save , an error message indicates the parameter that was not able to be updated. The YAML configuration can be edited while the virtual machine is Running , however the changes will only take effect after the virtual machine has been restarted. Note: Navigating away from the YAML screen while editing cancels any changes to the configuration you have made. Click Workloads > Virtual Machine from the side menu. Select a virtual machine. Click the YAML tab to display the editable configuration. (Optional) You can click Download to download the YAML file to your local machine in its current state. Edit the file and click Save . A confirmation message shows that the modification has been successful, including the updated version number for the object.","title":"Editing the YAML of a Virtual Machine"},{"location":"web_console/#viewing-the-events-of-a-virtual-machine","text":"You can view the events stream for a running virtual machine from the Virtual Machine Details screen of the web console. The button pauses the events stream. The button continues a paused events stream. Click Workloads > Virtual Machines from the side menu. Select a virtual machine. Click Events to view all events for the virtual machine.","title":"Viewing the Events of a Virtual Machine"},{"location":"web_console/#deleting-a-virtual-machine","text":"Deleting a virtual machine permanently removes it from the cluster. Delete a virtual machine using the button of the virtual machine in the Workloads > Virtual Machines list, or using the button of the Virtual Machine Details screen. Click Workloads > Virtual Machines from the side menu. Click the button of the virtual machine you wish to delete and select Delete Virtual Machine . Alternatively, click the virtual machine name to open the Virtual Machine Details screen and click > Delete Virtual Machine . In the confirmation pop-up window, click Delete to permanently delete the virtual machine.","title":"Deleting a Virtual Machine"},{"location":"web_console/#controlling-virtual-machines","text":"","title":"Controlling Virtual Machines"},{"location":"web_console/#starting-a-virtual-machine","text":"Virtual machines can be started from the button found on each virtual machine in the Workloads > Virtual Machines list, or from the button on the Virtual Machine Details screen. Click Workloads > Virtual Machine from the side menu. Click the button of the virtual machine and select Start Virtual Machine . Alternatively, click the virtual machine name to open the Virtual Machine Details screen and click and select Start Virtual Machine . In the confirmation pop-up window, click Start to start the virtual machine.","title":"Starting a Virtual Machine"},{"location":"web_console/#stopping-a-virtual-machine","text":"A running virtual machine can be stopped using the button found on each virtual machine in the Workloads > Virtual Machines list, or from the button on the Virtual Machine Details screen. Click Workloads > Virtual Machine from the side menu. Click the button of the virtual machine and select Stop Virtual Machine . Alternatively, click the virtual machine name to open the Virtual Machine Details screen and click and select Stop Virtual Machine . In the confirmation pop-up window, click Stop to stop the virtual machine.","title":"Stopping a Virtual Machine"},{"location":"web_console/#restarting-a-virtual-machine","text":"A running virtual machine can be restarted from the button found on each virtual machine in the Workloads > Virtual Machines list, or from the button on the Virtual Machine Details screen. Important: Do not restart a virtual machine while it has a status of Importing . This will result in an error for the virtual machine and is a known issue . Click Workloads > Virtual Machine from the side menu. Click the button of the virtual machine and select Restart Virtual Machine . Alternatively, click the virtual machine name to open the Virtual Machine Details screen, click , and select Restart Virtual Machine . In the confirmation pop-up window, click Restart to restart the virtual machine.","title":"Restarting a Virtual Machine"},{"location":"web_console/#live-migrating-a-virtual-machine","text":"Virtual machines can be live migrated to a different node from the button found on each virtual machine in the Workloads > Virtual Machines list, or from the button on the Virtual Machine Details screen. Click Workloads > Virtual Machine from the side menu. Click the button of the virtual machine and select Migrate Virtual Machine . Alternatively, click the virtual machine name to open the Virtual Machine Details screen and click and select Migrate Virtual Machine . In the confirmation pop-up window, click Migrate to migrate the virtual machine.","title":"Live Migrating a Virtual Machine"},{"location":"web_console/#cancelling-a-virtual-machine-live-migration","text":"A live migration of the virtual machine can be cancelled using the button found on each virtual machine in the Workloads > Virtual Machines list, or from the button on the Cancel Virtual Machine Migration screen. Click Workloads > Virtual Machine from the side menu. Click the button of the virtual machine and select Cancel Virtual Machine Migration . Alternatively, click the virtual machine name to open the Virtual Machine Details screen and click and select Cancel Virtual Machine Migration . In the confirmation pop-up window, click Cancel Migration to cancel the virtual machine live migration.","title":"Cancelling a Virtual Machine Live Migration"},{"location":"web_console/#accessing-virtual-machine-consoles","text":"","title":"Accessing Virtual Machine Consoles"},{"location":"web_console/#virtual-machine-console-sessions","text":"You can connect to the VNC, Serial, and RDP consoles of a running virtual machine from the Consoles tab in the Virtual Machine Details screen of the web console. There are three choices available: the graphical VNC Console , the Serial Console , and Desktop Viewer which can be used to connect to the RDP Console (Windows only). The VNC Console opens by default whenever you navigate to the Consoles tab. You can switch between the consoles using the VNC Console | Serial Console | Desktop Viewer drop-down list. Console sessions remain active in the background unless they are disconnected. When the Disconnect before switching checkbox is active and you switch consoles, the current console session is disconnected and a new session with the selected console connects to the virtual machine. This ensures only one console session is open at a time. Options for the VNC Console . The Send Key button lists key combinations to send to the virtual machine. Options for the Serial Console . Use the Disconnect button to manually disconnect the Serial Console session from the virtual machine. Use the Reconnect button to manually open a Serial Console session to the virtual machine. Options for the Desktop Viewer . The Network Interface drop-down allows you to choose the network interface for the VM to which you want to connect.","title":"Virtual Machine Console Sessions"},{"location":"web_console/#connecting-to-the-serial-console","text":"Connect to the Serial Console of a running virtual machine from the Consoles tab in the Virtual Machine Details screen of the web console. Click Workloads > Virtual Machines from the side menu. Select a virtual machine. Click Consoles . The VNC console opens by default. Click the VNC Console drop-down list and select Serial Console .","title":"Connecting to the Serial Console"},{"location":"web_console/#connecting-to-the-vnc-console","text":"Connect to the VNC console of a running virtual machine from the Consoles tab in the Virtual Machine Details screen of the web console. Click Workloads > Virtual Machines from the side menu. Select a virtual machine. Click Consoles . The VNC console opens by default.","title":"Connecting to the VNC Console"},{"location":"web_console/#using-desktop-viewer-to-connect-to-rdp-console-windows-only","text":"The Desktop Viewer option can be used to connect to the RDP console of a running Windows virtual machine. Click Workloads > Virtual Machines from the side menu. Select a virtual machine. Click Consoles . The VNC console opens by default. Select Desktop Viewer in the drop-down list. Choose the network interface you want to connect to from the Network Interface drop-down. If the L2 network interface is chosen, you will need to install qemu-guest-agent on the VM. If the pod network interface is chosen, you will need to create a service to expose port 3389 (RDP) on the VM. Follow the on-screen instructions to do so. Click Launch Remote Desktop to download the RDP file, which can then be opened in the RDP client of your choice.","title":"Using Desktop Viewer to connect to RDP Console (Windows only)"},{"location":"web_console/#managing-virtual-machine-nics","text":"","title":"Managing Virtual Machine NICs"},{"location":"web_console/#creating-a-nic-for-a-virtual-machine","text":"Create and attach additional NICs to a virtual machine from the web console. Click Workloads > Virtual Machines from the side menu. Select a virtual machine. Click Network Interfaces to display the NICs already attached to the virtual machine. Click Create NIC to create a new slot in the list. Fill in the NAME , NETWORK , MAC ADDRESS , and BINDING METHOD details for the new NIC. Click the button to save and attach the NIC to the virtual machine.","title":"Creating a NIC for a Virtual Machine"},{"location":"web_console/#deleting-a-nic-from-a-virtual-machine","text":"Deleting a NIC from a virtual machine detaches and permanently deletes the NIC. Click Workloads > Virtual Machines from the side menu. Select a virtual machine. Click Network Interfaces to display the NICs already attached to the virtual machine. Click the button of the NIC you wish to delete and select Delete . In the confirmation pop-up window, click Delete to detach and delete the NIC.","title":"Deleting a NIC from a Virtual Machine"},{"location":"web_console/#managing-virtual-machine-disks","text":"","title":"Managing Virtual Machine Disks"},{"location":"web_console/#creating-a-disk-for-a-virtual-machine","text":"Create and attach additional storage disks to a virtual machine from the web console. Click Workloads > Virtual Machines from the side menu. Select a virtual machine. Click Disks to display the disks already attached to the virtual machine. Click Create Disk to create a new slot in the list. Fill in the NAME , SIZE , and optional STORAGE CLASS details for the new disk. Click the button to save and attach the disk to the virtual machine.","title":"Creating a Disk for a Virtual Machine"},{"location":"web_console/#deleting-a-disk-from-a-virtual-machine","text":"Deleting a disk from a virtual machine detaches and permanently deletes the disk. Click Workloads > Virtual Machines from the side menu. Select a virtual machine. Click Disks to display the disks already attached to the virtual machine. Click the button of the disk you wish to delete and select Delete . Click Confirm to detach and delete the disk.","title":"Deleting a Disk from a Virtual Machine"},{"location":"web_console/#virtual-machine-templates-in-the-web-console","text":"","title":"Virtual Machine Templates in the Web Console"},{"location":"web_console/#creating-a-virtual-machine-template-with-the-interactive-wizard","text":"Virtual machine templates are an easy way to create multiple virtual machines with similar configuration. After a template is created, reference the template when creating virtual machines . The web console features an interactive wizard that guides you through Basic Settings , Networking , and Storage screens to simplify the process of creating virtual machine templates. All required fields are marked with a * . The wizard prevents you from moving to the next screen until the required fields have been completed. NICs and storage disks can be created and attached to virtual machines after they have been created. *Bootable Disk *. If either URL or Container are selected as the Provision Source in the Basic Settings screen, a rootdisk disk is created and attached to virtual machines as the Bootable Disk . You can modify the rootdisk but you cannot remove it. A Bootable Disk is not required for virtual machines provisioned from a PXE source if there are no disks attached to the virtual machine. If one or more disks are attached to the virtual machine, you must select one as the Bootable Disk . Click Workloads > Virtual Machine Templates from the side menu. Click Create Template and select Create with Wizard . Fill in all required Basic Settings . Click Next to progress to the Networking screen. An nic0 NIC is attached by default. (Optional) Click Create NIC to create additional NICs. (Optional) You can remove any or all NICs by clicking the button and selecting Remove NIC . Virtual machines created from a template do not need a NIC attached. NICs can be created after a virtual machine has been created. Click Next to progress to the Storage screen. (Optional) Click Create Disk to create additional disks. These disks can be removed by clicking the button and selecting Remove Disk . (Optional) Click on a disk to modify available fields. Click the button to save the update. (Optional) Click Attach Disk to choose an available disk from the Select Storage drop-down list. Click Create Virtual Machine Template > . The Results screen displays the JSON configuration file for the virtual machine template. The template should now be listed in Workloads > Virtual Machine Templates .","title":"Creating a Virtual Machine Template with the Interactive Wizard"},{"location":"web_console/#editing-the-yaml-of-a-virtual-machine-template","text":"You can edit the YAML configuration of a virtual machine template directly within the web console. Not all parameters can be updated. If you edit values that cannot be changed and click Save , an error message shows, indicating the parameter that was not able to be updated. Note: Navigating away from the YAML screen while editing cancels any changes to the configuration you have made. Click Workloads > Virtual Machine Template from the side menu. Select a template. Click the YAML tab to display the editable configuration. (Optional) You can click Download to download the YAML file to your local machine in its current state. Edit the file and click Save . A confirmation message shows the modification has been successful, including the updated version number for the object.","title":"Editing the YAML of a Virtual Machine Template"},{"location":"web_console/#deleting-a-virtual-machine-template","text":"Deleting a virtual machine template permanently removes it from the cluster. Delete a virtual machine template using the button found on each template in the Workloads > Virtual Machines Templates list, or using the button of the Virtual Machine Templates Details screen. Click Workloads > Virtual Machine Templates from the side menu. Click the button of the template you wish to delete and select Delete Template . Alternatively, click the template name to open the Virtual Machine Template Details screen and click > Delete Template . In the confirmation pop-up window, click Delete to permanently delete the template.","title":"Deleting a Virtual Machine Template"},{"location":"web_console/#reference","text":"","title":"Reference"},{"location":"web_console/#virtual-machine-wizard-fields","text":"Name Parameter Description Name The name of the virtual machine * Alphanumeric characters only (63 characters max) Description Optional description field Template Template from which to create the virtual machine * Selecting a template will automatically fill other fields Provision Source PXE URL Provision virtual machine from an image available from an HTTP or S3 endpoint Container Provision virtual machine from a bootable operating system container located in a registry accessible from the cluster * Example: _kubevirt/cirros-registry-disk-demo_ Operating System A list of operating systems available in the cluster * This is the primary operating system for the virtual machine Flavor Presets that determine the amount of CPU and memory allocated to the virtual machine Workload Profile A general configuration that balances performance and compatibility for a broad range of workloads highperformance The virtual machine has a more efficient configuration optimized for high performance loads Start virtual machine on creation Select this checkbox to automatically start the virtual machine upon creation cloud-init Select this checkbox to enable the cloud-init fields","title":"Virtual Machine Wizard Fields"},{"location":"web_console/#virtual-machine-template-wizard-fields","text":"Name Parameter Description Name The name of the virtual machine * Alphanumeric characters only (63 characters max) Description Optional description field Provision Source PXE Template Template from which to create the virtual machine * Selecting a template will automatically fill other fields. Provision Source Provision virtual machine from PXE menu * Requires a PXE-capable NIC in the cluster URL Provision virtual machines from an image available from a HTTP or S3 endpoint. Container Provision virtual machines from a bootable operating system container located in a registry accessible from the cluster * Example: _kubevirt/cirros-registry-disk-demo_ Operating System A list of operating systems available in the cluster. This is the primary operating system for the virtual machine. Flavor small, medium, large, tiny, Custom Presets that determine the amount of CPU and memory allocated to the virtual machine Workload Profile generic A general configuration that balances performance and compatibility for a broad range of workloads highperformance Virtual machines have a more efficient configuration optimized for high performance loads cloud-init Select this checkbox to enable the cloud-init fields","title":"Virtual Machine Template Wizard Fields"},{"location":"web_console/#cloud-init-fields","text":"Name Description Hostname Sets a specific hostname for the virtual machine Authenticated SSH Keys The user's public key * This will be copied to ~/.ssh/authorized_keys on the virtual machine Use custom script Replaces other options with a textbox into which you can paste a custom cloud-init script","title":"Cloud-init Fields"},{"location":"web_console/#networking-fields","text":"Name Description Create NIC Create a new NIC for the virtual machine NIC NAME Name for the NIC MAC ADDRESS MAC address for the network interface * If a MAC address is not specified an ephemeral address is generated for the session NETWORK CONFIGURATION List of available NetworkAttachmentDefinition objects BINDING METHOD List of available binding methods For pod networking, the possible values are masquerade, bridge, and sriov For other networks, the possible values are bridge and sriov PXE NIC List of PXE-capable networks * Only visible if PXE has been selected as the Provision Source","title":"Networking Fields"},{"location":"web_console/#storage-fields","text":"Name Description Create Disk Create a new disk for the virtual machine Attach Disk Select an existing disk from a list of available PVCs to attach to the virtual machine DISK NAME Name of the disk SIZE (GiB) Size in GiB of the disk STORAGE CLASS Name of the underlying StorageClass Bootable Disk List of available disks from which the virtual machine will boot * This is locked to rootdisk if the Provision Source of the virtual machine is URL or Container","title":"Storage Fields"},{"location":"web_console/#virtual-machine-actions","text":"Action Available in state Description Start Virtual Machine Off Start the virtual machine Stop Virtual Machine Running or Other Stop the virtual machine Restart Virtual Machine Running or Other Restart the running virtual machine Delete Virtual Machine All Permanently delete the virtual machine from the cluster Migrate Virtual Machine Running Live migrates the virtual machine to an another node Cancel Virtual Machine Migration Migrating Cancels the migration of the virtual machine","title":"Virtual Machine Actions"},{"location":"web_console/#pxe-booting-with-a-specified-mac-address","text":"Network booting allows a computer to boot and load an operating system or other program without requiring a locally attached storage device. For example, you can use it to choose your desired OS image from a PXE server when deploying a new host. Configure a PXE network on the cluster: Create NetworkAttachmentDefinition of PXE network pxe-net-conf : apiVersion: \"k8s.cni.cncf.io/v1\" kind: NetworkAttachmentDefinition metadata: name: pxe-net-conf spec: config: '{ \"cniVersion\": \"0.3.1\", \"type\": \"ovs\", \"bridge\": \"br1\" }' Note: In this example, the virtual machine instance (VMI) will be attached through a trunk port to the Open vSwitch bridge <br1> . Create Open vSwitch bridge <br1> and connect it to interface <eth1> , which is connected to a network that allows for PXE booting: $ ovs-vsctl add-br br1 $ ovs-vsctl add-port br1 eth1 $ ovs-vsctl show 8d004495-ea9a-44e1-b00c-3b65648dae5f Bridge br1 Port br1 Interface br1 type: internal Port \"eth1\" Interface \"eth1\" ovs_version: \"2.8.1\" Note: This bridge must be configured on all nodes. If it is only available on a subset of nodes, make sure that VMIs have nodeSelector constraints in place. Edit the virtual machine instance configuration file to include the details of the interface and network. Specify the network and MAC address if required by the PXE server. A value is assigned automatically if the MAC address is not specified; however, note that at this time, MAC addresses that are assigned automatically are not persistent. Ensure that bootOrder is set to 1 so that the interface boots first. In this example, the interface is connected to a network called <pxe-net> : interfaces: - masquerade: {} name: default - bridge: {} name: pxe-net macAddress: de:00:00:00:00:de bootOrder: 1 Note: Boot order is global for interfaces and disks. Assign a boot device number to the disk to ensure proper booting after OS provisioning. Set the disk bootOrder value to 2 : devices: disks: - disk: bus: virtio name: containerdisk bootOrder: 2 Specify that the network is connected to the previously created NetworkAttachmentDefinition . In this scenario, <pxe-net> is connected to the NetworkAttachmentDefinition called <pxe-net-conf> : networks: - name: default pod: {} - name: pxe-net multus: networkName: pxe-net-conf Create the virtual machine instance: $ oc create -f vmi-pxe-boot.yaml virtualmachineinstance.kubevirt.io \"vmi-pxe-boot\" created Wait for the virtual machine instance to run: $ oc get vmi vmi-pxe-boot -o yaml | grep -i phase phase: Running View the virtual machine instance using VNC: $ virtctl vnc vmi-pxe-boot Watch the boot screen to verify that the PXE boot is successful. Log in to the VMI: $ virtctl console vmi-pxe-boot Verify the interfaces and MAC address on the VM, and that the interface connected to the bridge has the specified MAC address. In this case, we used eth1 for the PXE boot, without an IP address. The other $ ip addr ... 3. eth1: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether de:00:00:00:00:de brd ff:ff:ff:ff:ff:ff","title":"PXE Booting with a Specified MAC Address"},{"location":"appendix/contributing/","text":"Contributing \u00b6 Prerequisites \u00b6 Reviewing the following will prepare you for contributing For code contributors: You need to be familiar with writing code in golang. See the golang tour to familiarize yourself. Read the Developer contribution page and the getting started page . For all contributors: You need to be comfortable with git, GitHub workflow of fork, branch, commit, open pull request, reviewing changes, and merge to work effectively in the KubeVirt community. If you're new to git git-scm.com very nice tutorials here . Familiarize yourself with the various repositories of the KubeVirt GitHub organization. Try the Deploy KubeVirt on minikube or kind quickstart lab. Try the Using KubeVirt lab. See the other ways to contribute section Your first contribution \u00b6 The following will help you decide where to start Check a repository issues list and label good-first-issue for issues that make good entry points. Open a pull request using GitHub to documentation. Review a pull request from other community members for accuracy and language. Other ways to contribute \u00b6 Visit the KubeVirt community page , participate on Twitter or Stack Overflow, learn about local meetups and events. Visit the KubeVirt website repository and submit a blog post, case study or lab.","title":"Contributing"},{"location":"appendix/contributing/#contributing","text":"","title":"Contributing"},{"location":"appendix/contributing/#prerequisites","text":"Reviewing the following will prepare you for contributing For code contributors: You need to be familiar with writing code in golang. See the golang tour to familiarize yourself. Read the Developer contribution page and the getting started page . For all contributors: You need to be comfortable with git, GitHub workflow of fork, branch, commit, open pull request, reviewing changes, and merge to work effectively in the KubeVirt community. If you're new to git git-scm.com very nice tutorials here . Familiarize yourself with the various repositories of the KubeVirt GitHub organization. Try the Deploy KubeVirt on minikube or kind quickstart lab. Try the Using KubeVirt lab. See the other ways to contribute section","title":"Prerequisites"},{"location":"appendix/contributing/#your-first-contribution","text":"The following will help you decide where to start Check a repository issues list and label good-first-issue for issues that make good entry points. Open a pull request using GitHub to documentation. Review a pull request from other community members for accuracy and language.","title":"Your first contribution"},{"location":"appendix/contributing/#other-ways-to-contribute","text":"Visit the KubeVirt community page , participate on Twitter or Stack Overflow, learn about local meetups and events. Visit the KubeVirt website repository and submit a blog post, case study or lab.","title":"Other ways to contribute"},{"location":"operations/activating_feature_gates/","text":"Activating feature gates \u00b6 KubeVirt has a set of features that are not mature enough to be enabled by default. As such, they are protected by a Kubernetes concept called feature gates . How to activate a feature gate \u00b6 You can activate a specific feature gate directly in KubeVirt's CR, by provisioning the following yaml, which uses the LiveMigration feature gate as an example: cat << END > enable-feature-gate.yaml --- apiVersion: kubevirt.io/v1 kind: KubeVirt metadata: name: kubevirt namespace: kubevirt spec: configuration: developerConfiguration: featureGates: - LiveMigration END kubectl apply -f enable-feature-gate.yaml Alternatively, the existing kubevirt CR can be altered: kubectl edit kubevirt kubevirt -n kubevirt ... spec: configuration: developerConfiguration: featureGates: - DataVolumes - LiveMigration Note: the name of the feature gates is case sensitive. The snippet above assumes KubeVirt is installed in the kubevirt namespace. Change the namespace to suite your installation. List of feature gates \u00b6 The list of feature gates (which evolve in time) can be checked directly from the source code .","title":"Activating feature gates"},{"location":"operations/activating_feature_gates/#activating-feature-gates","text":"KubeVirt has a set of features that are not mature enough to be enabled by default. As such, they are protected by a Kubernetes concept called feature gates .","title":"Activating feature gates"},{"location":"operations/activating_feature_gates/#how-to-activate-a-feature-gate","text":"You can activate a specific feature gate directly in KubeVirt's CR, by provisioning the following yaml, which uses the LiveMigration feature gate as an example: cat << END > enable-feature-gate.yaml --- apiVersion: kubevirt.io/v1 kind: KubeVirt metadata: name: kubevirt namespace: kubevirt spec: configuration: developerConfiguration: featureGates: - LiveMigration END kubectl apply -f enable-feature-gate.yaml Alternatively, the existing kubevirt CR can be altered: kubectl edit kubevirt kubevirt -n kubevirt ... spec: configuration: developerConfiguration: featureGates: - DataVolumes - LiveMigration Note: the name of the feature gates is case sensitive. The snippet above assumes KubeVirt is installed in the kubevirt namespace. Change the namespace to suite your installation.","title":"How to activate a feature gate"},{"location":"operations/activating_feature_gates/#list-of-feature-gates","text":"The list of feature gates (which evolve in time) can be checked directly from the source code .","title":"List of feature gates"},{"location":"operations/annotations_and_labels/","text":"Annotations and labels \u00b6 KubeVirt builds on and exposes a number of labels and annotations that either are used for internal implementation needs or expose useful information to API users. This page documents the labels and annotations that may be useful for regular API consumers. This page intentionally does not list labels and annotations that are merely part of internal implementation. Note: Annotations and labels that are not specific to KubeVirt are also documented here . kubevirt.io \u00b6 Example: kubevirt.io=virt-launcher Used on: Pod This label marks resources that belong to KubeVirt. An optional value may indicate which specific KubeVirt component a resource belongs to. This label may be used to list all resources that belong to KubeVirt, for example, to uninstall it from a cluster. kubevirt.io/schedulable \u00b6 Example: kubevirt.io/schedulable=true Used on: Node This label declares whether a particular node is available for scheduling virtual machine instances on it. kubevirt.io/heartbeat \u00b6 Example: kubevirt.io/heartbeat=2018-07-03T20:07:25Z Used on: Node This annotation is regularly updated by virt-handler to help determine if a particular node is alive and hence should be available for new virtual machine instance scheduling.","title":"Annotations and labels"},{"location":"operations/annotations_and_labels/#annotations-and-labels","text":"KubeVirt builds on and exposes a number of labels and annotations that either are used for internal implementation needs or expose useful information to API users. This page documents the labels and annotations that may be useful for regular API consumers. This page intentionally does not list labels and annotations that are merely part of internal implementation. Note: Annotations and labels that are not specific to KubeVirt are also documented here .","title":"Annotations and labels"},{"location":"operations/annotations_and_labels/#kubevirtio","text":"Example: kubevirt.io=virt-launcher Used on: Pod This label marks resources that belong to KubeVirt. An optional value may indicate which specific KubeVirt component a resource belongs to. This label may be used to list all resources that belong to KubeVirt, for example, to uninstall it from a cluster.","title":"kubevirt.io"},{"location":"operations/annotations_and_labels/#kubevirtioschedulable","text":"Example: kubevirt.io/schedulable=true Used on: Node This label declares whether a particular node is available for scheduling virtual machine instances on it.","title":"kubevirt.io/schedulable"},{"location":"operations/annotations_and_labels/#kubevirtioheartbeat","text":"Example: kubevirt.io/heartbeat=2018-07-03T20:07:25Z Used on: Node This annotation is regularly updated by virt-handler to help determine if a particular node is alive and hence should be available for new virtual machine instance scheduling.","title":"kubevirt.io/heartbeat"},{"location":"operations/api_validation/","text":"API Validation \u00b6 The KubeVirt VirtualMachineInstance API is implemented using a Kubernetes Custom Resource Definition (CRD). Because of this, KubeVirt is able to leverage a couple of features Kubernetes provides in order to perform validation checks on our API as objects created and updated on the cluster. How API Validation Works \u00b6 CRD OpenAPIv3 Schema \u00b6 The KubeVirt API is registered with Kubernetes at install time through a series of CRD definitions. KubeVirt includes an OpenAPIv3 schema in these definitions which indicates to the Kubernetes Apiserver some very basic information about our API, such as what fields are required and what type of data is expected for each value. This OpenAPIv3 schema validation is installed automatically and requires no thought on the users part to enable. Admission Control Webhooks \u00b6 The OpenAPIv3 schema validation is limited. It only validates the general structure of a KubeVirt object looks correct. It does not however verify that the contents of that object make sense. With OpenAPIv3 validation alone, users can easily make simple mistakes (like not referencing a volume's name correctly with a disk) and the cluster will still accept the object. However, the VirtualMachineInstance will of course not start if these errors in the API exist. Ideally we'd like to catch configuration issues as early as possible and not allow an object to even be posted to the cluster if we can detect there's a problem with the object's Spec. In order to perform this advanced validation, KubeVirt implements its own admission controller which is registered with kubernetes as an admission controller webhook. This webhook is registered with Kubernetes at install time. As KubeVirt objects are posted to the cluster, the Kubernetes API server forwards Creation requests to our webhook for validation before persisting the object into storage. Note however that the KubeVirt admission controller requires features to be enabled on the cluster in order to be enabled. Enabling KubeVirt Admission Controller on Kubernetes \u00b6 When provisioning a new Kubernetes cluster, ensure that both the MutatingAdmissionWebhook and ValidatingAdmissionWebhook values are present in the Apiserver's --admission-control cli argument. Below is an example of the --admission-control values we use during development --admission-control='Initializers,NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota' Enabling KubeVirt Admission Controller on OKD \u00b6 OKD also requires the admission control webhooks to be enabled at install time. The process is slightly different though. With OKD, we enable webhooks using an admission plugin. These admission control plugins can be configured in openshift-ansible by setting the following value in ansible inventory file. openshift_master_admission_plugin_config={\"ValidatingAdmissionWebhook\":{\"configuration\":{\"kind\": \"DefaultAdmissionConfig\",\"apiVersion\": \"v1\",\"disable\": false}},\"MutatingAdmissionWebhook\":{\"configuration\":{\"kind\": \"DefaultAdmissionConfig\",\"apiVersion\": \"v1\",\"disable\": false}}}","title":"API Validation"},{"location":"operations/api_validation/#api-validation","text":"The KubeVirt VirtualMachineInstance API is implemented using a Kubernetes Custom Resource Definition (CRD). Because of this, KubeVirt is able to leverage a couple of features Kubernetes provides in order to perform validation checks on our API as objects created and updated on the cluster.","title":"API Validation"},{"location":"operations/api_validation/#how-api-validation-works","text":"","title":"How API Validation Works"},{"location":"operations/api_validation/#crd-openapiv3-schema","text":"The KubeVirt API is registered with Kubernetes at install time through a series of CRD definitions. KubeVirt includes an OpenAPIv3 schema in these definitions which indicates to the Kubernetes Apiserver some very basic information about our API, such as what fields are required and what type of data is expected for each value. This OpenAPIv3 schema validation is installed automatically and requires no thought on the users part to enable.","title":"CRD OpenAPIv3 Schema"},{"location":"operations/api_validation/#admission-control-webhooks","text":"The OpenAPIv3 schema validation is limited. It only validates the general structure of a KubeVirt object looks correct. It does not however verify that the contents of that object make sense. With OpenAPIv3 validation alone, users can easily make simple mistakes (like not referencing a volume's name correctly with a disk) and the cluster will still accept the object. However, the VirtualMachineInstance will of course not start if these errors in the API exist. Ideally we'd like to catch configuration issues as early as possible and not allow an object to even be posted to the cluster if we can detect there's a problem with the object's Spec. In order to perform this advanced validation, KubeVirt implements its own admission controller which is registered with kubernetes as an admission controller webhook. This webhook is registered with Kubernetes at install time. As KubeVirt objects are posted to the cluster, the Kubernetes API server forwards Creation requests to our webhook for validation before persisting the object into storage. Note however that the KubeVirt admission controller requires features to be enabled on the cluster in order to be enabled.","title":"Admission Control Webhooks"},{"location":"operations/api_validation/#enabling-kubevirt-admission-controller-on-kubernetes","text":"When provisioning a new Kubernetes cluster, ensure that both the MutatingAdmissionWebhook and ValidatingAdmissionWebhook values are present in the Apiserver's --admission-control cli argument. Below is an example of the --admission-control values we use during development --admission-control='Initializers,NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota'","title":"Enabling KubeVirt Admission Controller on Kubernetes"},{"location":"operations/api_validation/#enabling-kubevirt-admission-controller-on-okd","text":"OKD also requires the admission control webhooks to be enabled at install time. The process is slightly different though. With OKD, we enable webhooks using an admission plugin. These admission control plugins can be configured in openshift-ansible by setting the following value in ansible inventory file. openshift_master_admission_plugin_config={\"ValidatingAdmissionWebhook\":{\"configuration\":{\"kind\": \"DefaultAdmissionConfig\",\"apiVersion\": \"v1\",\"disable\": false}},\"MutatingAdmissionWebhook\":{\"configuration\":{\"kind\": \"DefaultAdmissionConfig\",\"apiVersion\": \"v1\",\"disable\": false}}}","title":"Enabling KubeVirt Admission Controller on OKD"},{"location":"operations/authorization/","text":"Authorization \u00b6 KubeVirt authorization is performed using Kubernetes's Resource Based Authorization Control system (RBAC). RBAC allows cluster admins to grant access to cluster resources by binding RBAC roles to users. For example, an admin creates an RBAC role that represents the permissions required to create a VirtualMachineInstance. The admin can then bind that role to users in order to grant them the permissions required to launch a VirtualMachineInstance. With RBAC roles, admins can grant users targeted access to various KubeVirt features. KubeVirt Default RBAC ClusterRoles \u00b6 KubeVirt comes with a set of predefined RBAC ClusterRoles that can be used to grant users permissions to access KubeVirt Resources. Default View Role \u00b6 The kubevirt.io:view ClusterRole gives users permissions to view all KubeVirt resources in the cluster. The permissions to create, delete, modify or access any KubeVirt resources beyond viewing the resource's spec are not included in this role. This means a user with this role could see that a VirtualMachineInstance is running, but neither shutdown nor gain access to that VirtualMachineInstance via console/VNC. Default Edit Role \u00b6 The kubevirt.io:edit ClusterRole gives users permissions to modify all KubeVirt resources in the cluster. For example, a user with this role can create new VirtualMachineInstances, delete VirtualMachineInstances, and gain access to both console and VNC. Default Admin Role \u00b6 The kubevirt.io:admin ClusterRole grants users full permissions to all KubeVirt resources, including the ability to delete collections of resources. The admin role also grants users access to view and modify the KubeVirt runtime config. This config exists within the Kubevirt Custom Resource under the configuration key in the namespace the KubeVirt operator is running. NOTE Users are only guaranteed the ability to modify the kubevirt runtime configuration if a ClusterRoleBinding is used. A RoleBinding will work to provide kubevirt CR access only if the RoleBinding targets the same namespace that the kubevirt CR exists in. Binding Default ClusterRoles to Users \u00b6 The KubeVirt default ClusterRoles are granted to users by creating either a ClusterRoleBinding or RoleBinding object. Binding within All Namespaces \u00b6 With a ClusterRoleBinding, users receive the permissions granted by the role across all namespaces. Binding within Single Namespace \u00b6 With a RoleBinding, users receive the permissions granted by the role only within a targeted namespace. Extending Kubernetes Default Roles with KubeVirt permissions \u00b6 The aggregated ClusterRole Kubernetes feature facilitates combining multiple ClusterRoles into a single aggregated ClusterRole. This feature is commonly used to extend the default Kubernetes roles with permissions to access custom resources that do not exist in the Kubernetes core. In order to extend the default Kubernetes roles to provide permission to access KubeVirt resources, we need to add the following labels to the KubeVirt ClusterRoles. kubectl label clusterrole kubevirt.io:admin rbac.authorization.k8s.io/aggregate-to-admin=true kubectl label clusterrole kubevirt.io:edit rbac.authorization.k8s.io/aggregate-to-edit=true kubectl label clusterrole kubevirt.io:view rbac.authorization.k8s.io/aggregate-to-view=true By adding these labels, any user with a RoleBinding or ClusterRoleBinding involving one of the default Kubernetes roles will automatically gain access to the equivalent KubeVirt roles as well. More information about aggregated cluster roles can be found here Creating Custom RBAC Roles \u00b6 If the default KubeVirt ClusterRoles are not expressive enough, admins can create their own custom RBAC roles to grant user access to KubeVirt resources. The creation of a RBAC role is inclusive only, meaning there's no way to deny access. Instead access is only granted. Below is an example of what KubeVirt's default admin ClusterRole looks like. A custom RBAC role can be created by reducing the permissions in this example role. apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRole metadata: name: my-custom-rbac-role labels: kubevirt.io: \"\" rules: - apiGroups: - subresources.kubevirt.io resources: - virtualmachineinstances/console - virtualmachineinstances/vnc verbs: - get - apiGroups: - kubevirt.io resources: - virtualmachineinstances - virtualmachines - virtualmachineinstancepresets - virtualmachineinstancereplicasets verbs: - get - delete - create - update - patch - list - watch - deletecollection","title":"Authorization"},{"location":"operations/authorization/#authorization","text":"KubeVirt authorization is performed using Kubernetes's Resource Based Authorization Control system (RBAC). RBAC allows cluster admins to grant access to cluster resources by binding RBAC roles to users. For example, an admin creates an RBAC role that represents the permissions required to create a VirtualMachineInstance. The admin can then bind that role to users in order to grant them the permissions required to launch a VirtualMachineInstance. With RBAC roles, admins can grant users targeted access to various KubeVirt features.","title":"Authorization"},{"location":"operations/authorization/#kubevirt-default-rbac-clusterroles","text":"KubeVirt comes with a set of predefined RBAC ClusterRoles that can be used to grant users permissions to access KubeVirt Resources.","title":"KubeVirt Default RBAC ClusterRoles"},{"location":"operations/authorization/#default-view-role","text":"The kubevirt.io:view ClusterRole gives users permissions to view all KubeVirt resources in the cluster. The permissions to create, delete, modify or access any KubeVirt resources beyond viewing the resource's spec are not included in this role. This means a user with this role could see that a VirtualMachineInstance is running, but neither shutdown nor gain access to that VirtualMachineInstance via console/VNC.","title":"Default View Role"},{"location":"operations/authorization/#default-edit-role","text":"The kubevirt.io:edit ClusterRole gives users permissions to modify all KubeVirt resources in the cluster. For example, a user with this role can create new VirtualMachineInstances, delete VirtualMachineInstances, and gain access to both console and VNC.","title":"Default Edit Role"},{"location":"operations/authorization/#default-admin-role","text":"The kubevirt.io:admin ClusterRole grants users full permissions to all KubeVirt resources, including the ability to delete collections of resources. The admin role also grants users access to view and modify the KubeVirt runtime config. This config exists within the Kubevirt Custom Resource under the configuration key in the namespace the KubeVirt operator is running. NOTE Users are only guaranteed the ability to modify the kubevirt runtime configuration if a ClusterRoleBinding is used. A RoleBinding will work to provide kubevirt CR access only if the RoleBinding targets the same namespace that the kubevirt CR exists in.","title":"Default Admin Role"},{"location":"operations/authorization/#binding-default-clusterroles-to-users","text":"The KubeVirt default ClusterRoles are granted to users by creating either a ClusterRoleBinding or RoleBinding object.","title":"Binding Default ClusterRoles to Users"},{"location":"operations/authorization/#binding-within-all-namespaces","text":"With a ClusterRoleBinding, users receive the permissions granted by the role across all namespaces.","title":"Binding within All Namespaces"},{"location":"operations/authorization/#binding-within-single-namespace","text":"With a RoleBinding, users receive the permissions granted by the role only within a targeted namespace.","title":"Binding within Single Namespace"},{"location":"operations/authorization/#extending-kubernetes-default-roles-with-kubevirt-permissions","text":"The aggregated ClusterRole Kubernetes feature facilitates combining multiple ClusterRoles into a single aggregated ClusterRole. This feature is commonly used to extend the default Kubernetes roles with permissions to access custom resources that do not exist in the Kubernetes core. In order to extend the default Kubernetes roles to provide permission to access KubeVirt resources, we need to add the following labels to the KubeVirt ClusterRoles. kubectl label clusterrole kubevirt.io:admin rbac.authorization.k8s.io/aggregate-to-admin=true kubectl label clusterrole kubevirt.io:edit rbac.authorization.k8s.io/aggregate-to-edit=true kubectl label clusterrole kubevirt.io:view rbac.authorization.k8s.io/aggregate-to-view=true By adding these labels, any user with a RoleBinding or ClusterRoleBinding involving one of the default Kubernetes roles will automatically gain access to the equivalent KubeVirt roles as well. More information about aggregated cluster roles can be found here","title":"Extending Kubernetes Default Roles with KubeVirt permissions"},{"location":"operations/authorization/#creating-custom-rbac-roles","text":"If the default KubeVirt ClusterRoles are not expressive enough, admins can create their own custom RBAC roles to grant user access to KubeVirt resources. The creation of a RBAC role is inclusive only, meaning there's no way to deny access. Instead access is only granted. Below is an example of what KubeVirt's default admin ClusterRole looks like. A custom RBAC role can be created by reducing the permissions in this example role. apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRole metadata: name: my-custom-rbac-role labels: kubevirt.io: \"\" rules: - apiGroups: - subresources.kubevirt.io resources: - virtualmachineinstances/console - virtualmachineinstances/vnc verbs: - get - apiGroups: - kubevirt.io resources: - virtualmachineinstances - virtualmachines - virtualmachineinstancepresets - virtualmachineinstancereplicasets verbs: - get - delete - create - update - patch - list - watch - deletecollection","title":"Creating Custom RBAC Roles"},{"location":"operations/basic_use/","text":"Basic use \u00b6 Using KubeVirt should be fairly natural if you are used to working with Kubernetes. The primary way of using KubeVirt is by working with the KubeVirt kinds in the Kubernetes API: $ kubectl create -f vmi.yaml $ kubectl wait --for=condition=Ready vmis/my-vmi $ kubectl get vmis $ kubectl delete vmis testvmi The following pages describe how to use and discover the API, manage, and access virtual machines. User Interface \u00b6 KubeVirt does not come with a UI, it is only extending the Kubernetes API with virtualization functionality.","title":"Basic use"},{"location":"operations/basic_use/#basic-use","text":"Using KubeVirt should be fairly natural if you are used to working with Kubernetes. The primary way of using KubeVirt is by working with the KubeVirt kinds in the Kubernetes API: $ kubectl create -f vmi.yaml $ kubectl wait --for=condition=Ready vmis/my-vmi $ kubectl get vmis $ kubectl delete vmis testvmi The following pages describe how to use and discover the API, manage, and access virtual machines.","title":"Basic use"},{"location":"operations/basic_use/#user-interface","text":"KubeVirt does not come with a UI, it is only extending the Kubernetes API with virtualization functionality.","title":"User Interface"},{"location":"operations/component_monitoring/","text":"Component monitoring \u00b6 All KubeVirt system-components expose Prometheus metrics at their /metrics REST endpoint. Custom Service Discovery \u00b6 Prometheus supports service discovery based on Pods and Endpoints out of the box. Both can be used to discover KubeVirt services. All Pods which expose metrics are labeled with prometheus.kubevirt.io and contain a port-definition which is called metrics . In the KubeVirt release-manifests, the default metrics port is 8443 . The above labels and port informations are collected by a Service called kubevirt-prometheus-metrics . Kubernetes automatically creates a corresponding Endpoint with an equal name: $ kubectl get endpoints -n kubevirt kubevirt-prometheus-metrics -o yaml apiVersion: v1 kind: Endpoints metadata: labels: kubevirt.io: \"\" prometheus.kubevirt.io: \"\" name: kubevirt-prometheus-metrics namespace: kubevirt subsets: - addresses: - ip: 10.244.0.5 nodeName: node01 targetRef: kind: Pod name: virt-handler-cjzg6 namespace: kubevirt resourceVersion: \"4891\" uid: c67331f9-bfcf-11e8-bc54-525500d15501 - ip: 10.244.0.6 [...] ports: - name: metrics port: 8443 protocol: TCP By watching this endpoint for added and removed IPs to subsets.addresses and appending the metrics port from subsets.ports , it is possible to always get a complete list of ready-to-be-scraped Prometheus targets. Integrating with the prometheus-operator \u00b6 The prometheus-operator can make use of the kubevirt-prometheus-metrics service to automatically create the appropriate Prometheus config. KubeVirt's virt-operator checks if the ServiceMonitor custom resource exists when creating an install strategy for deployment. KubeVirt will automatically create a ServiceMonitor resource in the monitorNamespace , as well as an appropriate role and rolebinding in KubeVirt's namespace. Two settings are exposed in the KubeVirt custom resource to direct KubeVirt to create these resources correctly: monitorNamespace : The namespace that prometheus-operator runs in. Defaults to openshift-monitoring . monitorAccount : The serviceAccount that prometheus-operator runs with. Defaults to prometheus-k8s . If the prometheus-operator for a given deployment uses these defaults, then these values can be omitted. An example of the KubeVirt resource depicting these default values: apiVersion: kubevirt.io/v1alpha3 kind: KubeVirt metadata: name: kubevirt spec: monitorNamespace: openshift-monitoring monitorAccount: prometheus-k8s Integrating with the OKD cluster-monitoring-operator \u00b6 After the cluster-monitoring-operator is up and running, KubeVirt will detect the existence of the ServiceMonitor resource. Because the definition contains the openshift.io/cluster-monitoring label, it will automatically be picked up by the cluster monitor. Metrics about Virtual Machines \u00b6 The endpoints report metrics related to the runtime behaviour of the Virtual Machines. All the relevant metrics are prefixed with kubevirt_vmi . The metrics have labels that allow to connect to the VMI objects they refer to. At minimum, the labels will expose node , name and namespace of the related VMI object. For example, reported metrics could look like kubevirt_vmi_memory_resident_bytes{domain=\"default_vm-test-01\",name=\"vm-test-01\",namespace=\"default\",node=\"node01\"} 2.5595904e+07 kubevirt_vmi_network_traffic_bytes_total{domain=\"default_vm-test-01\",interface=\"vnet0\",name=\"vm-test-01\",namespace=\"default\",node=\"node01\",type=\"rx\"} 8431 kubevirt_vmi_network_traffic_bytes_total{domain=\"default_vm-test-01\",interface=\"vnet0\",name=\"vm-test-01\",namespace=\"default\",node=\"node01\",type=\"tx\"} 1835 kubevirt_vmi_vcpu_seconds{domain=\"default_vm-test-01\",id=\"0\",name=\"vm-test-01\",namespace=\"default\",node=\"node01\",state=\"1\"} 19 Please note the domain label in the above example. This label is deprecated and it will be removed in a future release. You should identify the VMI using the node , namespace , name labels instead. Important Queries \u00b6 Detecting connection issues for the REST client \u00b6 Use the following query to get a counter for all REST call which indicate connection issues: rest_client_requests_total{code=\"<error>\"} If this counter is continuously increasing, it is an indicator that the corresponding KubeVirt component has general issues to connect to the apiserver","title":"Component monitoring"},{"location":"operations/component_monitoring/#component-monitoring","text":"All KubeVirt system-components expose Prometheus metrics at their /metrics REST endpoint.","title":"Component monitoring"},{"location":"operations/component_monitoring/#custom-service-discovery","text":"Prometheus supports service discovery based on Pods and Endpoints out of the box. Both can be used to discover KubeVirt services. All Pods which expose metrics are labeled with prometheus.kubevirt.io and contain a port-definition which is called metrics . In the KubeVirt release-manifests, the default metrics port is 8443 . The above labels and port informations are collected by a Service called kubevirt-prometheus-metrics . Kubernetes automatically creates a corresponding Endpoint with an equal name: $ kubectl get endpoints -n kubevirt kubevirt-prometheus-metrics -o yaml apiVersion: v1 kind: Endpoints metadata: labels: kubevirt.io: \"\" prometheus.kubevirt.io: \"\" name: kubevirt-prometheus-metrics namespace: kubevirt subsets: - addresses: - ip: 10.244.0.5 nodeName: node01 targetRef: kind: Pod name: virt-handler-cjzg6 namespace: kubevirt resourceVersion: \"4891\" uid: c67331f9-bfcf-11e8-bc54-525500d15501 - ip: 10.244.0.6 [...] ports: - name: metrics port: 8443 protocol: TCP By watching this endpoint for added and removed IPs to subsets.addresses and appending the metrics port from subsets.ports , it is possible to always get a complete list of ready-to-be-scraped Prometheus targets.","title":"Custom Service Discovery"},{"location":"operations/component_monitoring/#integrating-with-the-prometheus-operator","text":"The prometheus-operator can make use of the kubevirt-prometheus-metrics service to automatically create the appropriate Prometheus config. KubeVirt's virt-operator checks if the ServiceMonitor custom resource exists when creating an install strategy for deployment. KubeVirt will automatically create a ServiceMonitor resource in the monitorNamespace , as well as an appropriate role and rolebinding in KubeVirt's namespace. Two settings are exposed in the KubeVirt custom resource to direct KubeVirt to create these resources correctly: monitorNamespace : The namespace that prometheus-operator runs in. Defaults to openshift-monitoring . monitorAccount : The serviceAccount that prometheus-operator runs with. Defaults to prometheus-k8s . If the prometheus-operator for a given deployment uses these defaults, then these values can be omitted. An example of the KubeVirt resource depicting these default values: apiVersion: kubevirt.io/v1alpha3 kind: KubeVirt metadata: name: kubevirt spec: monitorNamespace: openshift-monitoring monitorAccount: prometheus-k8s","title":"Integrating with the prometheus-operator"},{"location":"operations/component_monitoring/#integrating-with-the-okd-cluster-monitoring-operator","text":"After the cluster-monitoring-operator is up and running, KubeVirt will detect the existence of the ServiceMonitor resource. Because the definition contains the openshift.io/cluster-monitoring label, it will automatically be picked up by the cluster monitor.","title":"Integrating with the OKD cluster-monitoring-operator"},{"location":"operations/component_monitoring/#metrics-about-virtual-machines","text":"The endpoints report metrics related to the runtime behaviour of the Virtual Machines. All the relevant metrics are prefixed with kubevirt_vmi . The metrics have labels that allow to connect to the VMI objects they refer to. At minimum, the labels will expose node , name and namespace of the related VMI object. For example, reported metrics could look like kubevirt_vmi_memory_resident_bytes{domain=\"default_vm-test-01\",name=\"vm-test-01\",namespace=\"default\",node=\"node01\"} 2.5595904e+07 kubevirt_vmi_network_traffic_bytes_total{domain=\"default_vm-test-01\",interface=\"vnet0\",name=\"vm-test-01\",namespace=\"default\",node=\"node01\",type=\"rx\"} 8431 kubevirt_vmi_network_traffic_bytes_total{domain=\"default_vm-test-01\",interface=\"vnet0\",name=\"vm-test-01\",namespace=\"default\",node=\"node01\",type=\"tx\"} 1835 kubevirt_vmi_vcpu_seconds{domain=\"default_vm-test-01\",id=\"0\",name=\"vm-test-01\",namespace=\"default\",node=\"node01\",state=\"1\"} 19 Please note the domain label in the above example. This label is deprecated and it will be removed in a future release. You should identify the VMI using the node , namespace , name labels instead.","title":"Metrics about Virtual Machines"},{"location":"operations/component_monitoring/#important-queries","text":"","title":"Important Queries"},{"location":"operations/component_monitoring/#detecting-connection-issues-for-the-rest-client","text":"Use the following query to get a counter for all REST call which indicate connection issues: rest_client_requests_total{code=\"<error>\"} If this counter is continuously increasing, it is an indicator that the corresponding KubeVirt component has general issues to connect to the apiserver","title":"Detecting connection issues for the REST client"},{"location":"operations/containerized_data_importer/","text":"Containerized Data Importer \u00b6 The Containerized Data Importer (CDI) project provides facilities for enabling Persistent Volume Claims (PVCs) to be used as disks for KubeVirt VMs by way of DataVolumes . The three main CDI use cases are: Import a disk image from a web server or container registry to a DataVolume Clone an existing PVC to a DataVolume Upload a local disk image to a DataVolume This document deals with the third use case. So you should have CDI installed in your cluster, a VM disk that you'd like to upload, and virtctl in your path. Install CDI \u00b6 Install the latest CDI release here VERSION=$(curl -s https://github.com/kubevirt/containerized-data-importer/releases/latest | grep -o \"v[0-9]\\.[0-9]*\\.[0-9]*\") kubectl create -f https://github.com/kubevirt/containerized-data-importer/releases/download/$VERSION/cdi-operator.yaml kubectl create -f https://github.com/kubevirt/containerized-data-importer/releases/download/$VERSION/cdi-cr.yaml Expose cdi-uploadproxy service \u00b6 The cdi-uploadproxy service must be accessible from outside the cluster. Here are some ways to do that: NodePort Service Ingress Route kubectl port-forward (not recommended for production clusters) Look here for example manifests. Supported image formats \u00b6 CDI supports the raw and qcow2 image formats which are supported by qemu. See the qemu documentation for more details. Bootable ISO images can also be used and are treated like raw images. Images may be compressed with either the gz or xz format. The example in this document uses this CirrOS image virtctl image-upload \u00b6 virtctl has an image-upload command with the following options: virtctl image-upload --help Upload a VM image to a DataVolume/PersistentVolumeClaim. Usage: virtctl image-upload [flags] Examples: # Upload a local disk image to a newly created DataVolume: virtctl image-upload dv dv-name --size=10Gi --image-path=/images/fedora30.qcow2 # Upload a local disk image to an existing DataVolume virtctl image-upload dv dv-name --no-create --image-path=/images/fedora30.qcow2 # Upload a local disk image to an existing PersistentVolumeClaim virtctl image-upload pvc pvc-name --image-path=/images/fedora30.qcow2 # Upload to a DataVolume with explicit URL to CDI Upload Proxy virtctl image-upload dv dv-name --uploadproxy-url=https://cdi-uploadproxy.mycluster.com --image-path=/images/fedora30.qcow2 Flags: --access-mode string The access mode for the PVC. (default \"ReadWriteOnce\") --block-volume Create a PVC with VolumeMode=Block (default Filesystem). -h, --help help for image-upload --image-path string Path to the local VM image. --insecure Allow insecure server connections when using HTTPS. --no-create Don't attempt to create a new DataVolume/PVC. --pvc-name string DEPRECATED - The destination DataVolume/PVC name. --pvc-size string DEPRECATED - The size of the PVC to create (ex. 10Gi, 500Mi). --size string The size of the DataVolume to create (ex. 10Gi, 500Mi). --storage-class string The storage class for the PVC. --uploadproxy-url string The URL of the cdi-upload proxy service. --wait-secs uint Seconds to wait for upload pod to start. (default 60) Use \"virtctl options\" for a list of global command-line options (applies to all commands). virtctl image-upload works by creating a DataVolume of the requested size, sending an UploadTokenRequest to the cdi-apiserver , and uploading the file to the cdi-uploadproxy . virtctl image-upload dv cirros-vm-disk --size=500Mi --image-path=/home/mhenriks/images/cirros-0.4.0-x86_64-disk.img --uploadproxy-url=<url to upload proxy service> Addressing Certificate Issues when Uploading Images \u00b6 Issues with the certificates can be circumvented by using the --insecure flag to prevent the virtctl command from verifying the remote host. It is better to resolve certificate issues that prevent uploading images using the virtctl image-upload command and not use the --insecure flag. The following are some common issues with certificates and some easy ways to fix them. Does not contain any IP SANs \u00b6 This issue happens when trying to upload images using an IP address instead of a resolvable name. For example, trying to upload to the IP address 192.168.39.32 at port 31001 would produce the following error. virtctl image-upload dv f33 \\ --size 5Gi \\ --image-path Fedora-Cloud-Base-33-1.2.x86_64.raw.xz \\ --uploadproxy-url https://192.168.39.32:31001 PVC default/f33 not found DataVolume default/f33 created Waiting for PVC f33 upload pod to be ready... Pod now ready Uploading data to https://192.168.39.32:31001 0 B / 193.89 MiB [-------------------------------------------------------] 0.00% 0s Post https://192.168.39.32:31001/v1alpha1/upload: x509: cannot validate certificate for 192.168.39.32 because it doesn't contain any IP SANs It is easily fixed by adding an entry it your local name resolution service. This could be a DNS server or the local hosts file. The URL used to upload the proxy should be changed to reflect the resolvable name. The Subject and the Subject Alternative Name in the certificate contain valid names that can be used for resolution. Only one of these names needs to be resolvable. Use the openssl command to view the names of the cdi-uploadproxy service. echo | openssl s_client -showcerts -connect 192.168.39.32:31001 2>/dev/null \\ | openssl x509 -inform pem -noout -text \\ | sed -n -e '/Subject.*CN/p' -e '/Subject Alternative/{N;p}' Subject: CN = cdi-uploadproxy X509v3 Subject Alternative Name: DNS:cdi-uploadproxy, DNS:cdi-uploadproxy.cdi, DNS:cdi-uploadproxy.cdi.svc Adding the following entry to the /etc/hosts file, if it provides name resolution, should fix this issue. Any service that provides name resolution for the system could be used. echo \"192.168.39.32 cdi-uploadproxy\" >> /etc/hosts The upload should now work. virtctl image-upload dv f33 \\ --size 5Gi \\ --image-path Fedora-Cloud-Base-33-1.2.x86_64.raw.xz \\ --uploadproxy-url https://cdi-uploadproxy:31001 PVC default/f33 not found DataVolume default/f33 created Waiting for PVC f33 upload pod to be ready... Pod now ready Uploading data to https://cdi-uploadproxy:31001 193.89 MiB / 193.89 MiB [=============================================] 100.00% 1m38s Uploading data completed successfully, waiting for processing to complete, you can hit ctrl-c without interrupting the progress Processing completed successfully Uploading Fedora-Cloud-Base-33-1.2.x86_64.raw.xz completed successfully Certificate Signed by Unknown Authority \u00b6 This happens because the cdi-uploadproxy certificate is self signed and the system does not trust the cdi-uploadproxy as a Certificate Authority. virtctl image-upload dv f33 \\ --size 5Gi \\ --image-path Fedora-Cloud-Base-33-1.2.x86_64.raw.xz \\ --uploadproxy-url https://cdi-uploadproxy:31001 PVC default/f33 not found DataVolume default/f33 created Waiting for PVC f33 upload pod to be ready... Pod now ready Uploading data to https://cdi-uploadproxy:31001 0 B / 193.89 MiB [-------------------------------------------------------] 0.00% 0s Post https://cdi-uploadproxy:31001/v1alpha1/upload: x509: certificate signed by unknown authority This can be fixed by adding the certificate to the systems trust store. Download the cdi-uploadproxy-server-cert. kubectl get secret -n cdi cdi-uploadproxy-server-cert \\ -o jsonpath=\"{.data['tls\\.crt']}\" \\ | base64 -d > cdi-uploadproxy-server-cert.crt Add this certificate to the systems trust store. On Fedora, this can be done as follows. sudo cp cdi-uploadproxy-server-cert.crt /etc/pki/ca-trust/source/anchors sudo update-ca-trust The upload should now work. virtctl image-upload dv f33 \\ --size 5Gi \\ --image-path Fedora-Cloud-Base-33-1.2.x86_64.raw.xz \\ --uploadproxy-url https://cdi-uploadproxy:31001 PVC default/f33 not found DataVolume default/f33 created Waiting for PVC f33 upload pod to be ready... Pod now ready Uploading data to https://cdi-uploadproxy:31001 193.89 MiB / 193.89 MiB [=============================================] 100.00% 1m36s Uploading data completed successfully, waiting for processing to complete, you can hit ctrl-c without interrupting the progress Processing completed successfully Uploading Fedora-Cloud-Base-33-1.2.x86_64.raw.xz completed successfully Setting the URL of the cdi-upload Proxy Service \u00b6 Setting the URL for the cdi-upload proxy service allows the virtctl image-upload command to upload the images without specifying the --uploadproxy-url flag. Permanently setting the URL is done by patching the CDI configuration. The following will set the default upload proxy to use port 31001 of cdi-uploadproxy. An IP address could also be used instead of the dns name. See the section Addressing Certificate Issues when Uploading for why cdi-uploadproxy was chosen and issues that can be encountered when using an IP address. kubectl patch cdi cdi \\ --type merge \\ --patch '{\"spec\":{\"config\":{\"uploadProxyURLOverride\":\"https://cdi-uploadproxy:31001\"}}}' Create a VirtualMachineInstance \u00b6 To create a VirtualMachineInstance from a DataVolume, you can execute the following: cat <<EOF | kubectl apply -f - apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance metadata: name: cirros-vm spec: domain: devices: disks: - disk: bus: virtio name: dvdisk machine: type: \"\" resources: requests: memory: 64M terminationGracePeriodSeconds: 0 volumes: - name: dvdisk dataVolume: name: cirros-vm-disk status: {} EOF Connect to VirtualMachineInstance console \u00b6 Use virtctl to connect to the newly create VirtualMachineInstance . virtctl console cirros-vm","title":"Containerized Data Importer"},{"location":"operations/containerized_data_importer/#containerized-data-importer","text":"The Containerized Data Importer (CDI) project provides facilities for enabling Persistent Volume Claims (PVCs) to be used as disks for KubeVirt VMs by way of DataVolumes . The three main CDI use cases are: Import a disk image from a web server or container registry to a DataVolume Clone an existing PVC to a DataVolume Upload a local disk image to a DataVolume This document deals with the third use case. So you should have CDI installed in your cluster, a VM disk that you'd like to upload, and virtctl in your path.","title":"Containerized Data Importer"},{"location":"operations/containerized_data_importer/#install-cdi","text":"Install the latest CDI release here VERSION=$(curl -s https://github.com/kubevirt/containerized-data-importer/releases/latest | grep -o \"v[0-9]\\.[0-9]*\\.[0-9]*\") kubectl create -f https://github.com/kubevirt/containerized-data-importer/releases/download/$VERSION/cdi-operator.yaml kubectl create -f https://github.com/kubevirt/containerized-data-importer/releases/download/$VERSION/cdi-cr.yaml","title":"Install CDI"},{"location":"operations/containerized_data_importer/#expose-cdi-uploadproxy-service","text":"The cdi-uploadproxy service must be accessible from outside the cluster. Here are some ways to do that: NodePort Service Ingress Route kubectl port-forward (not recommended for production clusters) Look here for example manifests.","title":"Expose cdi-uploadproxy service"},{"location":"operations/containerized_data_importer/#supported-image-formats","text":"CDI supports the raw and qcow2 image formats which are supported by qemu. See the qemu documentation for more details. Bootable ISO images can also be used and are treated like raw images. Images may be compressed with either the gz or xz format. The example in this document uses this CirrOS image","title":"Supported image formats"},{"location":"operations/containerized_data_importer/#virtctl-image-upload","text":"virtctl has an image-upload command with the following options: virtctl image-upload --help Upload a VM image to a DataVolume/PersistentVolumeClaim. Usage: virtctl image-upload [flags] Examples: # Upload a local disk image to a newly created DataVolume: virtctl image-upload dv dv-name --size=10Gi --image-path=/images/fedora30.qcow2 # Upload a local disk image to an existing DataVolume virtctl image-upload dv dv-name --no-create --image-path=/images/fedora30.qcow2 # Upload a local disk image to an existing PersistentVolumeClaim virtctl image-upload pvc pvc-name --image-path=/images/fedora30.qcow2 # Upload to a DataVolume with explicit URL to CDI Upload Proxy virtctl image-upload dv dv-name --uploadproxy-url=https://cdi-uploadproxy.mycluster.com --image-path=/images/fedora30.qcow2 Flags: --access-mode string The access mode for the PVC. (default \"ReadWriteOnce\") --block-volume Create a PVC with VolumeMode=Block (default Filesystem). -h, --help help for image-upload --image-path string Path to the local VM image. --insecure Allow insecure server connections when using HTTPS. --no-create Don't attempt to create a new DataVolume/PVC. --pvc-name string DEPRECATED - The destination DataVolume/PVC name. --pvc-size string DEPRECATED - The size of the PVC to create (ex. 10Gi, 500Mi). --size string The size of the DataVolume to create (ex. 10Gi, 500Mi). --storage-class string The storage class for the PVC. --uploadproxy-url string The URL of the cdi-upload proxy service. --wait-secs uint Seconds to wait for upload pod to start. (default 60) Use \"virtctl options\" for a list of global command-line options (applies to all commands). virtctl image-upload works by creating a DataVolume of the requested size, sending an UploadTokenRequest to the cdi-apiserver , and uploading the file to the cdi-uploadproxy . virtctl image-upload dv cirros-vm-disk --size=500Mi --image-path=/home/mhenriks/images/cirros-0.4.0-x86_64-disk.img --uploadproxy-url=<url to upload proxy service>","title":"virtctl image-upload"},{"location":"operations/containerized_data_importer/#addressing-certificate-issues-when-uploading-images","text":"Issues with the certificates can be circumvented by using the --insecure flag to prevent the virtctl command from verifying the remote host. It is better to resolve certificate issues that prevent uploading images using the virtctl image-upload command and not use the --insecure flag. The following are some common issues with certificates and some easy ways to fix them.","title":"Addressing Certificate Issues when Uploading Images"},{"location":"operations/containerized_data_importer/#does-not-contain-any-ip-sans","text":"This issue happens when trying to upload images using an IP address instead of a resolvable name. For example, trying to upload to the IP address 192.168.39.32 at port 31001 would produce the following error. virtctl image-upload dv f33 \\ --size 5Gi \\ --image-path Fedora-Cloud-Base-33-1.2.x86_64.raw.xz \\ --uploadproxy-url https://192.168.39.32:31001 PVC default/f33 not found DataVolume default/f33 created Waiting for PVC f33 upload pod to be ready... Pod now ready Uploading data to https://192.168.39.32:31001 0 B / 193.89 MiB [-------------------------------------------------------] 0.00% 0s Post https://192.168.39.32:31001/v1alpha1/upload: x509: cannot validate certificate for 192.168.39.32 because it doesn't contain any IP SANs It is easily fixed by adding an entry it your local name resolution service. This could be a DNS server or the local hosts file. The URL used to upload the proxy should be changed to reflect the resolvable name. The Subject and the Subject Alternative Name in the certificate contain valid names that can be used for resolution. Only one of these names needs to be resolvable. Use the openssl command to view the names of the cdi-uploadproxy service. echo | openssl s_client -showcerts -connect 192.168.39.32:31001 2>/dev/null \\ | openssl x509 -inform pem -noout -text \\ | sed -n -e '/Subject.*CN/p' -e '/Subject Alternative/{N;p}' Subject: CN = cdi-uploadproxy X509v3 Subject Alternative Name: DNS:cdi-uploadproxy, DNS:cdi-uploadproxy.cdi, DNS:cdi-uploadproxy.cdi.svc Adding the following entry to the /etc/hosts file, if it provides name resolution, should fix this issue. Any service that provides name resolution for the system could be used. echo \"192.168.39.32 cdi-uploadproxy\" >> /etc/hosts The upload should now work. virtctl image-upload dv f33 \\ --size 5Gi \\ --image-path Fedora-Cloud-Base-33-1.2.x86_64.raw.xz \\ --uploadproxy-url https://cdi-uploadproxy:31001 PVC default/f33 not found DataVolume default/f33 created Waiting for PVC f33 upload pod to be ready... Pod now ready Uploading data to https://cdi-uploadproxy:31001 193.89 MiB / 193.89 MiB [=============================================] 100.00% 1m38s Uploading data completed successfully, waiting for processing to complete, you can hit ctrl-c without interrupting the progress Processing completed successfully Uploading Fedora-Cloud-Base-33-1.2.x86_64.raw.xz completed successfully","title":"Does not contain any IP SANs"},{"location":"operations/containerized_data_importer/#certificate-signed-by-unknown-authority","text":"This happens because the cdi-uploadproxy certificate is self signed and the system does not trust the cdi-uploadproxy as a Certificate Authority. virtctl image-upload dv f33 \\ --size 5Gi \\ --image-path Fedora-Cloud-Base-33-1.2.x86_64.raw.xz \\ --uploadproxy-url https://cdi-uploadproxy:31001 PVC default/f33 not found DataVolume default/f33 created Waiting for PVC f33 upload pod to be ready... Pod now ready Uploading data to https://cdi-uploadproxy:31001 0 B / 193.89 MiB [-------------------------------------------------------] 0.00% 0s Post https://cdi-uploadproxy:31001/v1alpha1/upload: x509: certificate signed by unknown authority This can be fixed by adding the certificate to the systems trust store. Download the cdi-uploadproxy-server-cert. kubectl get secret -n cdi cdi-uploadproxy-server-cert \\ -o jsonpath=\"{.data['tls\\.crt']}\" \\ | base64 -d > cdi-uploadproxy-server-cert.crt Add this certificate to the systems trust store. On Fedora, this can be done as follows. sudo cp cdi-uploadproxy-server-cert.crt /etc/pki/ca-trust/source/anchors sudo update-ca-trust The upload should now work. virtctl image-upload dv f33 \\ --size 5Gi \\ --image-path Fedora-Cloud-Base-33-1.2.x86_64.raw.xz \\ --uploadproxy-url https://cdi-uploadproxy:31001 PVC default/f33 not found DataVolume default/f33 created Waiting for PVC f33 upload pod to be ready... Pod now ready Uploading data to https://cdi-uploadproxy:31001 193.89 MiB / 193.89 MiB [=============================================] 100.00% 1m36s Uploading data completed successfully, waiting for processing to complete, you can hit ctrl-c without interrupting the progress Processing completed successfully Uploading Fedora-Cloud-Base-33-1.2.x86_64.raw.xz completed successfully","title":"Certificate Signed by Unknown Authority"},{"location":"operations/containerized_data_importer/#setting-the-url-of-the-cdi-upload-proxy-service","text":"Setting the URL for the cdi-upload proxy service allows the virtctl image-upload command to upload the images without specifying the --uploadproxy-url flag. Permanently setting the URL is done by patching the CDI configuration. The following will set the default upload proxy to use port 31001 of cdi-uploadproxy. An IP address could also be used instead of the dns name. See the section Addressing Certificate Issues when Uploading for why cdi-uploadproxy was chosen and issues that can be encountered when using an IP address. kubectl patch cdi cdi \\ --type merge \\ --patch '{\"spec\":{\"config\":{\"uploadProxyURLOverride\":\"https://cdi-uploadproxy:31001\"}}}'","title":"Setting the URL of the cdi-upload Proxy Service"},{"location":"operations/containerized_data_importer/#create-a-virtualmachineinstance","text":"To create a VirtualMachineInstance from a DataVolume, you can execute the following: cat <<EOF | kubectl apply -f - apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance metadata: name: cirros-vm spec: domain: devices: disks: - disk: bus: virtio name: dvdisk machine: type: \"\" resources: requests: memory: 64M terminationGracePeriodSeconds: 0 volumes: - name: dvdisk dataVolume: name: cirros-vm-disk status: {} EOF","title":"Create a VirtualMachineInstance"},{"location":"operations/containerized_data_importer/#connect-to-virtualmachineinstance-console","text":"Use virtctl to connect to the newly create VirtualMachineInstance . virtctl console cirros-vm","title":"Connect to VirtualMachineInstance console"},{"location":"operations/customize_components/","text":"Customize KubeVirt Components \u00b6 Customize components using patches \u00b6 :warning: If the patch created is invalid KubeVirt will not be able to update or deploy the system. This is intended for special use cases and should not be used unless you know what you are doing. Valid resource types are: Deployment, DaemonSet, Service, ValidatingWebhookConfiguraton, MutatingWebhookConfiguration, APIService, and CertificateSecret. More information can be found in the API spec . Example customization patch: --- apiVersion: kubevirt.io/v1alpha3 kind: KubeVirt metadata: name: kubevirt namespace: kubevirt spec: certificateRotateStrategy: {} configuration: {} customizeComponents: patches: - resourceType: Deployment resourceName: virt-controller patch: '[{\"op\": \"remove\", \"path\": \"/spec/template/spec/containers/0/livenessProbe\"}]' type: json - resourceType: Deployment resourecName: virt-controller patch: '{\"metadata\":{\"annotations\":{\"patch\": \"true\"}}}' type: strategic The above example will update the virt-controller deployment to have an annotation in it's metadata that says patch: true and will remove the livenessProbe from the container definition. Customize Flags \u00b6 :warning: If the flags are invalid or become invalid on update the component will not be able to run By using the customize flag option, whichever component the flags are to be applied to, all default flags will be removed and only the flags specified will be used. The available resources to change the flags on are api , controller and handler . You can find our more details about the API in the API spec . apiVersion: kubevirt.io/v1alpha3 kind: KubeVirt metadata: name: kubevirt namespace: kubevirt spec: certificateRotateStrategy: {} configuration: {} customizeComponents: flags: api: v: \"5\" port: \"8443\" console-server-port: \"8186\" subresources-only: \"true\" The above example would produce a virt-api pod with the following command ... spec: .... container: - name: virt-api command: - virt-api - --v - \"5\" - --console-server-port - \"8186\" - --port - \"8443\" - --subresources-only - \"true\" ...","title":"Customize components"},{"location":"operations/customize_components/#customize-kubevirt-components","text":"","title":"Customize KubeVirt Components"},{"location":"operations/customize_components/#customize-components-using-patches","text":":warning: If the patch created is invalid KubeVirt will not be able to update or deploy the system. This is intended for special use cases and should not be used unless you know what you are doing. Valid resource types are: Deployment, DaemonSet, Service, ValidatingWebhookConfiguraton, MutatingWebhookConfiguration, APIService, and CertificateSecret. More information can be found in the API spec . Example customization patch: --- apiVersion: kubevirt.io/v1alpha3 kind: KubeVirt metadata: name: kubevirt namespace: kubevirt spec: certificateRotateStrategy: {} configuration: {} customizeComponents: patches: - resourceType: Deployment resourceName: virt-controller patch: '[{\"op\": \"remove\", \"path\": \"/spec/template/spec/containers/0/livenessProbe\"}]' type: json - resourceType: Deployment resourecName: virt-controller patch: '{\"metadata\":{\"annotations\":{\"patch\": \"true\"}}}' type: strategic The above example will update the virt-controller deployment to have an annotation in it's metadata that says patch: true and will remove the livenessProbe from the container definition.","title":"Customize components using patches"},{"location":"operations/customize_components/#customize-flags","text":":warning: If the flags are invalid or become invalid on update the component will not be able to run By using the customize flag option, whichever component the flags are to be applied to, all default flags will be removed and only the flags specified will be used. The available resources to change the flags on are api , controller and handler . You can find our more details about the API in the API spec . apiVersion: kubevirt.io/v1alpha3 kind: KubeVirt metadata: name: kubevirt namespace: kubevirt spec: certificateRotateStrategy: {} configuration: {} customizeComponents: flags: api: v: \"5\" port: \"8443\" console-server-port: \"8186\" subresources-only: \"true\" The above example would produce a virt-api pod with the following command ... spec: .... container: - name: virt-api command: - virt-api - --v - \"5\" - --console-server-port - \"8186\" - --port - \"8443\" - --subresources-only - \"true\" ...","title":"Customize Flags"},{"location":"operations/hotplug_volumes/","text":"Hotplug Volumes \u00b6 KubeVirt now supports hotplugging volumes into a running Virtual Machine Instance (VMI). The volume must be either a block volume or contain a disk image. When a VM that has hotplugged volumes is rebooted, the hotplugged volumes will be attached to the restarted VM. If the volumes are persisted they will become part of the VM spec, and will not be considered hotplugged. If they are not persisted, the volumes will be reattached as hotplugged volumes Enabling hotplug volume support \u00b6 Hotplug volume support must be enabled in the feature gates to be supported. The feature gates field in the KubeVirt CR must be expanded by adding the HotplugVolumes to it. Virtctl support \u00b6 In order to hotplug a volume, you must first prepare a volume. This can be done by using a DataVolume (DV). In the example we will use a blank DV in order to add some extra storage to a running VMI apiVersion: cdi.kubevirt.io/v1beta1 kind: DataVolume metadata: name: example-volume-hotplug spec: source: blank: {} pvc: accessModes: - ReadWriteOnce resources: requests: storage: 5Gi In this example we are using ReadWriteOnce accessMode, and the default FileSystem volume mode. Volume hotplugging supports all combinations of block volume mode and ReadWriteMany / ReadWriteOnce / ReadOnlyMany accessModes, if your storage supports the combination. Addvolume \u00b6 Now lets assume we have started a VMI like the Fedora VMI in examples and the name of the VMI is 'vmi-fedora'. We can add the above blank volume to this running VMI by using the 'addvolume' command available with virtctl $ virtctl addvolume vmi-fedora --volume-name=example-volume-hotplug This will hotplug the volume into the running VMI, and set the serial of the disk to the volume name. In this example it is set to example-hotplug-volume. Serial \u00b6 You can change the serial of the disk by specifying the --serial parameter, for example: $ virtctl addvolume vmi-fedora --volume-name=example-volume-hotplug --serial=1234567890 The serial will be used in the guest so you can identify the disk inside the guest by the serial. For instance in Fedora the disk by id will contain the serial. $ virtctl console vmi-fedora Fedora 32 (Cloud Edition) Kernel 5.6.6-300.fc32.x86_64 on an x86_64 (ttyS0) SSH host key: SHA256:c8ik1A9F4E7AxVrd6eE3vMNOcMcp6qBxsf8K30oC/C8 (ECDSA) SSH host key: SHA256:fOAKptNAH2NWGo2XhkaEtFHvOMfypv2t6KIPANev090 (ED25519) eth0: 10.244.196.144 fe80::d8b7:51ff:fec4:7099 vmi-fedora login:fedora Password:fedora [fedora@vmi-fedora ~]$ ls /dev/disk/by-id scsi-0QEMU_QEMU_HARDDISK_1234567890 [fedora@vmi-fedora ~]$ As you can see the serial is part of the disk name, so you can uniquely identify it. The format and length of serials are specified according to the libvirt documentation: If present, this specify serial number of virtual hard drive. For example, it may look like <serial>WD-WMAP9A966149</serial>. Not supported for scsi-block devices, that is those using disk type 'block' using device 'lun' on bus 'scsi'. Since 0.7.1 Note that depending on hypervisor and device type the serial number may be truncated silently. IDE/SATA devices are commonly limited to 20 characters. SCSI devices depending on hypervisor version are limited to 20, 36 or 247 characters. Hypervisors may also start rejecting overly long serials instead of truncating them in the future so it's advised to avoid the implicit truncation by testing the desired serial length range with the desired device and hypervisor combination. Persist \u00b6 In some cases you want a hotplugged volume to become part of the standard disks after a restart of the VM. For instance if you added some permanent storage to the VM. We also assume that the running VMI has a matching VM that defines it specification. You can call the addvolume command with the --persist flag. This will update the VM domain disks section in addition to updating the VMI domain disks. This means that when you restart the VM, the disk is already defined in the VM, and thus in the new VMI. $ virtctl addvolume vm-fedora --volume-name=example-volume-hotplug --persist In the VM spec this will now show as a new disk spec: domain: devices: disks: - disk: bus: virtio name: containerdisk - disk: bus: virtio name: cloudinitdisk - disk: bus: scsi name: example-volume-hotplug machine: type: \"\" Removevolume \u00b6 In addition to hotplug plugging the volume, you can also unplug it by using the 'removevolume' command available with virtctl $ virtctl removevolume vmi-fedora --volume-name=example-volume-hotplug NOTE You can only unplug volumes that were dynamically added with addvolume, or using the API. VolumeStatus \u00b6 VMI objects have a new status.VolumeStatus field. This is an array containing each disk, hotplugged or not. For example, after hotplugging the volume in the addvolume example, the VMI status will contain this: volumeStatus: - name: cloudinitdisk target: vdb - name: containerdisk target: vda - hotplugVolume: attachPodName: hp-volume-7fmz4 attachPodUID: 62a7f6bf-474c-4e25-8db5-1db9725f0ed2 message: Successfully attach hotplugged volume volume-hotplug to VM name: example-volume-hotplug phase: Ready reason: VolumeReady target: sda Vda is the container disk that contains the Fedora OS, vdb is the cloudinit disk. As you can see those just contain the name and target used when assigning them to the VM. The target is the value passed to QEMU when specifying the disks. The value is unique for the VM and does NOT represent the naming inside the guest. For instance for a Windows Guest OS the target has no meaning. The same will be true for hotplugged volumes. The target is just a unique identifier meant for QEMU, inside the guest the disk can be assigned a different name. The hotplugVolume has some extra information that regular volume statuses do not have. The attachPodName is the name of the pod that was used to attach the volume to the node the VMI is running on. If this pod is deleted it will also stop the VMI as we cannot guarantee the volume will remain attached to the node. The other fields are similar to conditions and indicate the status of the hot plug process. Once a Volume is ready it can be used by the VM. Live Migration \u00b6 Currently Live Migration is enabled for any VMI that has volumes hotplugged into it. NOTE However there is a known issue that the migration may fail for VMIs with hotplugged block volumes if the target node uses CPU manager with static policy and runc prior to version v1.0.0 .","title":"Hotplug Volumes"},{"location":"operations/hotplug_volumes/#hotplug-volumes","text":"KubeVirt now supports hotplugging volumes into a running Virtual Machine Instance (VMI). The volume must be either a block volume or contain a disk image. When a VM that has hotplugged volumes is rebooted, the hotplugged volumes will be attached to the restarted VM. If the volumes are persisted they will become part of the VM spec, and will not be considered hotplugged. If they are not persisted, the volumes will be reattached as hotplugged volumes","title":"Hotplug Volumes"},{"location":"operations/hotplug_volumes/#enabling-hotplug-volume-support","text":"Hotplug volume support must be enabled in the feature gates to be supported. The feature gates field in the KubeVirt CR must be expanded by adding the HotplugVolumes to it.","title":"Enabling hotplug volume support"},{"location":"operations/hotplug_volumes/#virtctl-support","text":"In order to hotplug a volume, you must first prepare a volume. This can be done by using a DataVolume (DV). In the example we will use a blank DV in order to add some extra storage to a running VMI apiVersion: cdi.kubevirt.io/v1beta1 kind: DataVolume metadata: name: example-volume-hotplug spec: source: blank: {} pvc: accessModes: - ReadWriteOnce resources: requests: storage: 5Gi In this example we are using ReadWriteOnce accessMode, and the default FileSystem volume mode. Volume hotplugging supports all combinations of block volume mode and ReadWriteMany / ReadWriteOnce / ReadOnlyMany accessModes, if your storage supports the combination.","title":"Virtctl support"},{"location":"operations/hotplug_volumes/#addvolume","text":"Now lets assume we have started a VMI like the Fedora VMI in examples and the name of the VMI is 'vmi-fedora'. We can add the above blank volume to this running VMI by using the 'addvolume' command available with virtctl $ virtctl addvolume vmi-fedora --volume-name=example-volume-hotplug This will hotplug the volume into the running VMI, and set the serial of the disk to the volume name. In this example it is set to example-hotplug-volume.","title":"Addvolume"},{"location":"operations/hotplug_volumes/#serial","text":"You can change the serial of the disk by specifying the --serial parameter, for example: $ virtctl addvolume vmi-fedora --volume-name=example-volume-hotplug --serial=1234567890 The serial will be used in the guest so you can identify the disk inside the guest by the serial. For instance in Fedora the disk by id will contain the serial. $ virtctl console vmi-fedora Fedora 32 (Cloud Edition) Kernel 5.6.6-300.fc32.x86_64 on an x86_64 (ttyS0) SSH host key: SHA256:c8ik1A9F4E7AxVrd6eE3vMNOcMcp6qBxsf8K30oC/C8 (ECDSA) SSH host key: SHA256:fOAKptNAH2NWGo2XhkaEtFHvOMfypv2t6KIPANev090 (ED25519) eth0: 10.244.196.144 fe80::d8b7:51ff:fec4:7099 vmi-fedora login:fedora Password:fedora [fedora@vmi-fedora ~]$ ls /dev/disk/by-id scsi-0QEMU_QEMU_HARDDISK_1234567890 [fedora@vmi-fedora ~]$ As you can see the serial is part of the disk name, so you can uniquely identify it. The format and length of serials are specified according to the libvirt documentation: If present, this specify serial number of virtual hard drive. For example, it may look like <serial>WD-WMAP9A966149</serial>. Not supported for scsi-block devices, that is those using disk type 'block' using device 'lun' on bus 'scsi'. Since 0.7.1 Note that depending on hypervisor and device type the serial number may be truncated silently. IDE/SATA devices are commonly limited to 20 characters. SCSI devices depending on hypervisor version are limited to 20, 36 or 247 characters. Hypervisors may also start rejecting overly long serials instead of truncating them in the future so it's advised to avoid the implicit truncation by testing the desired serial length range with the desired device and hypervisor combination.","title":"Serial"},{"location":"operations/hotplug_volumes/#persist","text":"In some cases you want a hotplugged volume to become part of the standard disks after a restart of the VM. For instance if you added some permanent storage to the VM. We also assume that the running VMI has a matching VM that defines it specification. You can call the addvolume command with the --persist flag. This will update the VM domain disks section in addition to updating the VMI domain disks. This means that when you restart the VM, the disk is already defined in the VM, and thus in the new VMI. $ virtctl addvolume vm-fedora --volume-name=example-volume-hotplug --persist In the VM spec this will now show as a new disk spec: domain: devices: disks: - disk: bus: virtio name: containerdisk - disk: bus: virtio name: cloudinitdisk - disk: bus: scsi name: example-volume-hotplug machine: type: \"\"","title":"Persist"},{"location":"operations/hotplug_volumes/#removevolume","text":"In addition to hotplug plugging the volume, you can also unplug it by using the 'removevolume' command available with virtctl $ virtctl removevolume vmi-fedora --volume-name=example-volume-hotplug NOTE You can only unplug volumes that were dynamically added with addvolume, or using the API.","title":"Removevolume"},{"location":"operations/hotplug_volumes/#volumestatus","text":"VMI objects have a new status.VolumeStatus field. This is an array containing each disk, hotplugged or not. For example, after hotplugging the volume in the addvolume example, the VMI status will contain this: volumeStatus: - name: cloudinitdisk target: vdb - name: containerdisk target: vda - hotplugVolume: attachPodName: hp-volume-7fmz4 attachPodUID: 62a7f6bf-474c-4e25-8db5-1db9725f0ed2 message: Successfully attach hotplugged volume volume-hotplug to VM name: example-volume-hotplug phase: Ready reason: VolumeReady target: sda Vda is the container disk that contains the Fedora OS, vdb is the cloudinit disk. As you can see those just contain the name and target used when assigning them to the VM. The target is the value passed to QEMU when specifying the disks. The value is unique for the VM and does NOT represent the naming inside the guest. For instance for a Windows Guest OS the target has no meaning. The same will be true for hotplugged volumes. The target is just a unique identifier meant for QEMU, inside the guest the disk can be assigned a different name. The hotplugVolume has some extra information that regular volume statuses do not have. The attachPodName is the name of the pod that was used to attach the volume to the node the VMI is running on. If this pod is deleted it will also stop the VMI as we cannot guarantee the volume will remain attached to the node. The other fields are similar to conditions and indicate the status of the hot plug process. Once a Volume is ready it can be used by the VM.","title":"VolumeStatus"},{"location":"operations/hotplug_volumes/#live-migration","text":"Currently Live Migration is enabled for any VMI that has volumes hotplugged into it. NOTE However there is a known issue that the migration may fail for VMIs with hotplugged block volumes if the target node uses CPU manager with static policy and runc prior to version v1.0.0 .","title":"Live Migration"},{"location":"operations/hugepages/","text":"Hugepages support \u00b6 For hugepages support you need at least Kubernetes version 1.9 . Enable feature-gate \u00b6 To enable hugepages on Kubernetes, check the official documentation . To enable hugepages on OKD, check the official documentation . Pre-allocate hugepages on a node \u00b6 To pre-allocate hugepages on boot time, you will need to specify hugepages under kernel boot parameters hugepagesz=2M hugepages=64 and restart your machine. You can find more about hugepages under official documentation .","title":"Hugepages support"},{"location":"operations/hugepages/#hugepages-support","text":"For hugepages support you need at least Kubernetes version 1.9 .","title":"Hugepages support"},{"location":"operations/hugepages/#enable-feature-gate","text":"To enable hugepages on Kubernetes, check the official documentation . To enable hugepages on OKD, check the official documentation .","title":"Enable feature-gate"},{"location":"operations/hugepages/#pre-allocate-hugepages-on-a-node","text":"To pre-allocate hugepages on boot time, you will need to specify hugepages under kernel boot parameters hugepagesz=2M hugepages=64 and restart your machine. You can find more about hugepages under official documentation .","title":"Pre-allocate hugepages on a node"},{"location":"operations/installation/","text":"Installation \u00b6 KubeVirt is a virtualization add-on to Kubernetes and this guide assumes that a Kubernetes cluster is already installed. If installed on OKD, the web console is extended for management of virtual machines. Requirements \u00b6 A few requirements need to be met before you can begin: Kubernetes cluster or derivative (such as OpenShift , Tectonic) based on Kubernetes 1.10 or greater Kubernetes apiserver must have --allow-privileged=true in order to run KubeVirt's privileged DaemonSet. kubectl client utility Container Runtime Support \u00b6 KubeVirt is currently supported on the following container runtimes: docker crio (with runv) Other container runtimes, which do not use virtualization features, should work too. However, they are not tested. Validate Hardware Virtualization Support \u00b6 Hardware with virtualization support is recommended. You can use virt-host-validate to ensure that your hosts are capable of running virtualization workloads: $ virt-host-validate qemu QEMU: Checking for hardware virtualization : PASS QEMU: Checking if device /dev/kvm exists : PASS QEMU: Checking if device /dev/kvm is accessible : PASS QEMU: Checking if device /dev/vhost-net exists : PASS QEMU: Checking if device /dev/net/tun exists : PASS ... Installing KubeVirt on Kubernetes \u00b6 KubeVirt can be installed using the KubeVirt operator, which manages the lifecycle of all the KubeVirt core components. Below is an example of how to install KubeVirt using an official release. # Pick an upstream version of KubeVirt to install $ export RELEASE=v0.35.0 # Deploy the KubeVirt operator $ kubectl apply -f https://github.com/kubevirt/kubevirt/releases/download/${RELEASE}/kubevirt-operator.yaml # Create the KubeVirt CR (instance deployment request) which triggers the actual installation $ kubectl apply -f https://github.com/kubevirt/kubevirt/releases/download/${RELEASE}/kubevirt-cr.yaml # wait until all KubeVirt components are up $ kubectl -n kubevirt wait kv kubevirt --for condition=Available If hardware virtualization is not available, then a software emulation fallback can be enabled using by setting in the KubeVirt CR spec.configuration.developerConfiguration.useEmulation to true as follows: $ kubectl edit -n kubevirt kubevirt kubevirt Add the following to the kubevirt.yaml file spec: ... configuration: developerConfiguration: useEmulation: true Note: Prior to release v0.20.0 the condition for the kubectl wait command was named \"Ready\" instead of \"Available\" Note: Prior to KubeVirt 0.34.2 a ConfigMap called kubevirt-config in the install-namespace was used to configure KubeVirt. Since 0.34.2 this method is deprecated. The configmap still has precedence over configuration on the CR exists, but it will not receive future updates and you should migrate any custom configurations to spec.configuration on the KubeVirt CR. All new components will be deployed under the kubevirt namespace: kubectl get pods -n kubevirt NAME READY STATUS RESTARTS AGE virt-api-6d4fc3cf8a-b2ere 1/1 Running 0 1m virt-controller-5d9fc8cf8b-n5trt 1/1 Running 0 1m virt-handler-vwdjx 1/1 Running 0 1m ... Installing KubeVirt on OKD \u00b6 The following SCC needs to be added prior KubeVirt deployment: $ oc adm policy add-scc-to-user privileged -n kubevirt -z kubevirt-operator Once privileges are granted, the KubeVirt can be deployed as described above. Web user interface on OKD \u00b6 No additional steps are required to extend OKD's web console for KubeVirt. The virtualization extension is automatically enabled when KubeVirt deployment is detected. From Service Catalog as an APB \u00b6 You can find KubeVirt in the OKD Service Catalog and install it from there. In order to do that please follow the documentation in the KubeVirt APB repository . Installing KubeVirt on k3OS \u00b6 The following configuration needs to be added to all nodes prior KubeVirt deployment: k3os: modules: - kvm - vhost_net Once nodes are restarted with this configuration, the KubeVirt can be deployed as described above. Installing the Daily Developer Builds \u00b6 KubeVirt releases daily a developer build from current master. One can see when the last release happened by looking at our nightly-build-jobs . To install the latest developer build, run the following commands: $ LATEST=$(curl -L https://storage.googleapis.com/kubevirt-prow/devel/nightly/release/kubevirt/kubevirt/latest) $ kubectl apply -f https://storage.googleapis.com/kubevirt-prow/devel/nightly/release/kubevirt/kubevirt/${LATEST}/kubevirt-operator.yaml $ kubectl apply -f https://storage.googleapis.com/kubevirt-prow/devel/nightly/release/kubevirt/kubevirt/${LATEST}/kubevirt-cr.yaml To find out which commit this build is based on, run: $ LATEST=$(curl -L https://storage.googleapis.com/kubevirt-prow/devel/nightly/release/kubevirt/kubevirt/latest) $ curl https://storage.googleapis.com/kubevirt-prow/devel/nightly/release/kubevirt/kubevirt/${LATEST}/commit d358cf085b5a86cc4fa516215f8b757a4e61def2 Experimental ARM64 developer builds \u00b6 Experimental ARM64 developer builds can be installed like this: $ LATEST=$(curl -L https://storage.googleapis.com/kubevirt-prow/devel/nightly/release/kubevirt/kubevirt/latest-arm64) $ kubectl apply -f https://storage.googleapis.com/kubevirt-prow/devel/nightly/release/kubevirt/kubevirt/${LATEST}/kubevirt-operator-arm64.yaml $ kubectl apply -f https://storage.googleapis.com/kubevirt-prow/devel/nightly/release/kubevirt/kubevirt/${LATEST}/kubevirt-cr-arm64.yaml Deploying from Source \u00b6 See the Developer Getting Started Guide to understand how to build and deploy KubeVirt from source. Installing network plugins (optional) \u00b6 KubeVirt alone does not bring any additional network plugins, it just allows user to utilize them. If you want to attach your VMs to multiple networks (Multus CNI) or have full control over L2 (OVS CNI), you need to deploy respective network plugins. For more information, refer to OVS CNI installation guide . Note: KubeVirt Ansible network playbook installs these plugins by default. Restricting virt-handler DaemonSet \u00b6 You can patch the virt-handler DaemonSet post-deployment to restrict it to a specific subset of nodes with a nodeSelector. For example, to restrict the DaemonSet to only nodes with the \"region=primary\" label: kubectl patch ds/virt-handler -n kubevirt -p '{\"spec\": {\"template\": {\"spec\": {\"nodeSelector\": {\"region\": \"primary\"}}}}}'","title":"Installation"},{"location":"operations/installation/#installation","text":"KubeVirt is a virtualization add-on to Kubernetes and this guide assumes that a Kubernetes cluster is already installed. If installed on OKD, the web console is extended for management of virtual machines.","title":"Installation"},{"location":"operations/installation/#requirements","text":"A few requirements need to be met before you can begin: Kubernetes cluster or derivative (such as OpenShift , Tectonic) based on Kubernetes 1.10 or greater Kubernetes apiserver must have --allow-privileged=true in order to run KubeVirt's privileged DaemonSet. kubectl client utility","title":"Requirements"},{"location":"operations/installation/#container-runtime-support","text":"KubeVirt is currently supported on the following container runtimes: docker crio (with runv) Other container runtimes, which do not use virtualization features, should work too. However, they are not tested.","title":"Container Runtime Support"},{"location":"operations/installation/#validate-hardware-virtualization-support","text":"Hardware with virtualization support is recommended. You can use virt-host-validate to ensure that your hosts are capable of running virtualization workloads: $ virt-host-validate qemu QEMU: Checking for hardware virtualization : PASS QEMU: Checking if device /dev/kvm exists : PASS QEMU: Checking if device /dev/kvm is accessible : PASS QEMU: Checking if device /dev/vhost-net exists : PASS QEMU: Checking if device /dev/net/tun exists : PASS ...","title":"Validate Hardware Virtualization Support"},{"location":"operations/installation/#installing-kubevirt-on-kubernetes","text":"KubeVirt can be installed using the KubeVirt operator, which manages the lifecycle of all the KubeVirt core components. Below is an example of how to install KubeVirt using an official release. # Pick an upstream version of KubeVirt to install $ export RELEASE=v0.35.0 # Deploy the KubeVirt operator $ kubectl apply -f https://github.com/kubevirt/kubevirt/releases/download/${RELEASE}/kubevirt-operator.yaml # Create the KubeVirt CR (instance deployment request) which triggers the actual installation $ kubectl apply -f https://github.com/kubevirt/kubevirt/releases/download/${RELEASE}/kubevirt-cr.yaml # wait until all KubeVirt components are up $ kubectl -n kubevirt wait kv kubevirt --for condition=Available If hardware virtualization is not available, then a software emulation fallback can be enabled using by setting in the KubeVirt CR spec.configuration.developerConfiguration.useEmulation to true as follows: $ kubectl edit -n kubevirt kubevirt kubevirt Add the following to the kubevirt.yaml file spec: ... configuration: developerConfiguration: useEmulation: true Note: Prior to release v0.20.0 the condition for the kubectl wait command was named \"Ready\" instead of \"Available\" Note: Prior to KubeVirt 0.34.2 a ConfigMap called kubevirt-config in the install-namespace was used to configure KubeVirt. Since 0.34.2 this method is deprecated. The configmap still has precedence over configuration on the CR exists, but it will not receive future updates and you should migrate any custom configurations to spec.configuration on the KubeVirt CR. All new components will be deployed under the kubevirt namespace: kubectl get pods -n kubevirt NAME READY STATUS RESTARTS AGE virt-api-6d4fc3cf8a-b2ere 1/1 Running 0 1m virt-controller-5d9fc8cf8b-n5trt 1/1 Running 0 1m virt-handler-vwdjx 1/1 Running 0 1m ...","title":"Installing KubeVirt on Kubernetes"},{"location":"operations/installation/#installing-kubevirt-on-okd","text":"The following SCC needs to be added prior KubeVirt deployment: $ oc adm policy add-scc-to-user privileged -n kubevirt -z kubevirt-operator Once privileges are granted, the KubeVirt can be deployed as described above.","title":"Installing KubeVirt on OKD"},{"location":"operations/installation/#web-user-interface-on-okd","text":"No additional steps are required to extend OKD's web console for KubeVirt. The virtualization extension is automatically enabled when KubeVirt deployment is detected.","title":"Web user interface on OKD"},{"location":"operations/installation/#from-service-catalog-as-an-apb","text":"You can find KubeVirt in the OKD Service Catalog and install it from there. In order to do that please follow the documentation in the KubeVirt APB repository .","title":"From Service Catalog as an APB"},{"location":"operations/installation/#installing-kubevirt-on-k3os","text":"The following configuration needs to be added to all nodes prior KubeVirt deployment: k3os: modules: - kvm - vhost_net Once nodes are restarted with this configuration, the KubeVirt can be deployed as described above.","title":"Installing KubeVirt on k3OS"},{"location":"operations/installation/#installing-the-daily-developer-builds","text":"KubeVirt releases daily a developer build from current master. One can see when the last release happened by looking at our nightly-build-jobs . To install the latest developer build, run the following commands: $ LATEST=$(curl -L https://storage.googleapis.com/kubevirt-prow/devel/nightly/release/kubevirt/kubevirt/latest) $ kubectl apply -f https://storage.googleapis.com/kubevirt-prow/devel/nightly/release/kubevirt/kubevirt/${LATEST}/kubevirt-operator.yaml $ kubectl apply -f https://storage.googleapis.com/kubevirt-prow/devel/nightly/release/kubevirt/kubevirt/${LATEST}/kubevirt-cr.yaml To find out which commit this build is based on, run: $ LATEST=$(curl -L https://storage.googleapis.com/kubevirt-prow/devel/nightly/release/kubevirt/kubevirt/latest) $ curl https://storage.googleapis.com/kubevirt-prow/devel/nightly/release/kubevirt/kubevirt/${LATEST}/commit d358cf085b5a86cc4fa516215f8b757a4e61def2","title":"Installing the Daily Developer Builds"},{"location":"operations/installation/#experimental-arm64-developer-builds","text":"Experimental ARM64 developer builds can be installed like this: $ LATEST=$(curl -L https://storage.googleapis.com/kubevirt-prow/devel/nightly/release/kubevirt/kubevirt/latest-arm64) $ kubectl apply -f https://storage.googleapis.com/kubevirt-prow/devel/nightly/release/kubevirt/kubevirt/${LATEST}/kubevirt-operator-arm64.yaml $ kubectl apply -f https://storage.googleapis.com/kubevirt-prow/devel/nightly/release/kubevirt/kubevirt/${LATEST}/kubevirt-cr-arm64.yaml","title":"Experimental ARM64 developer builds"},{"location":"operations/installation/#deploying-from-source","text":"See the Developer Getting Started Guide to understand how to build and deploy KubeVirt from source.","title":"Deploying from Source"},{"location":"operations/installation/#installing-network-plugins-optional","text":"KubeVirt alone does not bring any additional network plugins, it just allows user to utilize them. If you want to attach your VMs to multiple networks (Multus CNI) or have full control over L2 (OVS CNI), you need to deploy respective network plugins. For more information, refer to OVS CNI installation guide . Note: KubeVirt Ansible network playbook installs these plugins by default.","title":"Installing network plugins (optional)"},{"location":"operations/installation/#restricting-virt-handler-daemonset","text":"You can patch the virt-handler DaemonSet post-deployment to restrict it to a specific subset of nodes with a nodeSelector. For example, to restrict the DaemonSet to only nodes with the \"region=primary\" label: kubectl patch ds/virt-handler -n kubevirt -p '{\"spec\": {\"template\": {\"spec\": {\"nodeSelector\": {\"region\": \"primary\"}}}}}'","title":"Restricting virt-handler DaemonSet"},{"location":"operations/live_migration/","text":"Live Migration \u00b6 Live migration is a process during which a running Virtual Machine Instance moves to another compute node while the guest workload continues to run and remain accessible. Enabling the live-migration support \u00b6 Live migration must be enabled in the feature gates to be supported. The feature gates field in the KubeVirt CR must be expanded by adding the LiveMigration to it. Limitations \u00b6 Virtual machines using a PersistentVolumeClaim (PVC) must have a shared ReadWriteMany (RWX) access mode to be live migrated. Live migration is not allowed with a pod network binding of bridge interface type ( ) Live migration requires ports 49152, 49153 to be available in the virt-launcher pod. If these ports are explicitly specified in masquarade interface , live migration will not function. Initiate live migration \u00b6 Live migration is initiated by posting a VirtualMachineInstanceMigration (VMIM) object to the cluster. The example below starts a migration process for a virtual machine instance vmi-fedora apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstanceMigration metadata: name: migration-job spec: vmiName: vmi-fedora Migration Status Reporting \u00b6 Condition and migration method \u00b6 When starting a virtual machine instance, it has also been calculated whether the machine is live migratable. The result is being stored in the VMI VMI.status.conditions . The calculation can be based on multiple parameters of the VMI, however, at the moment, the calculation is largely based on the Access Mode of the VMI volumes. Live migration is only permitted when the volume access mode is set to ReadWriteMany . Requests to migrate a non-LiveMigratable VMI will be rejected. The reported Migration Method is also being calculated during VMI start. BlockMigration indicates that some of the VMI disks require copying from the source to the destination. LiveMigration means that only the instance memory will be copied. Status: Conditions: Status: True Type: LiveMigratable Migration Method: BlockMigration Migration Status \u00b6 The migration progress status is being reported in the VMI VMI.status . Most importantly, it indicates whether the migration has been Completed or if it Failed . Below is an example of a successful migration. Migration State: Completed: true End Timestamp: 2019-03-29T03:37:52Z Migration Config: Completion Timeout Per GiB: 800 Progress Timeout: 150 Migration UID: c64d4898-51d3-11e9-b370-525500d15501 Source Node: node02 Start Timestamp: 2019-03-29T04:02:47Z Target Direct Migration Node Ports: 35001: 0 41068: 49152 38284: 49153 Target Node: node01 Target Node Address: 10.128.0.46 Target Node Domain Detected: true Target Pod: virt-launcher-testvmimcbjgw6zrzcmp8wpddvztvzm7x2k6cjbdgktwv8tkq Cancel live migration \u00b6 Live migration can also be canceled by simply deleting the migration object. A successfully aborted migration will indicate that the abort has been requested Abort Requested , and that it succeeded: Abort Status: Succeeded . The migration in this case will be Completed and Failed . Migration State: Abort Requested: true Abort Status: Succeeded Completed: true End Timestamp: 2019-03-29T04:02:49Z Failed: true Migration Config: Completion Timeout Per GiB: 800 Progress Timeout: 150 Migration UID: 57a693d6-51d7-11e9-b370-525500d15501 Source Node: node02 Start Timestamp: 2019-03-29T04:02:47Z Target Direct Migration Node Ports: 39445: 0 43345: 49152 44222: 49153 Target Node: node01 Target Node Address: 10.128.0.46 Target Node Domain Detected: true Target Pod: virt-launcher-testvmimcbjgw6zrzcmp8wpddvztvzm7x2k6cjbdgktwv8tkq Changing Cluster Wide Migration Limits \u00b6 KubeVirt puts some limits in place, so that migrations don't overwhelm the cluster. By default, it is configured to only run 5 migrations in parallel with an additional limit of a maximum of 2 outbound migrations per node. Finally, every migration is limited to a bandwidth of 64MiB/s . These values can be change in the kubevirt CR: apiVersion: kubevirt.io/v1alpha3 kind: Kubevirt metadata: name: kubevirt namespace: kubevirt spec: configuration: developerConfiguration: featureGates: - \"LiveMigration\" migrationConfiguration: parallelMigrationsPerCluster: 5 parallelOutboundMigrationsPerNode: 2 bandwidthPerMigration: 64Mi completionTimeoutPerGiB: 800 progressTimeout: 150 disableTLS: false Migration timeouts \u00b6 Depending on the type, the live migration process will copy virtual machine memory pages and disk blocks to the destination. During this process non-locked pages and blocks are being copied and become free for the instance to use again. To achieve a successful migration, it is assumed that the instance will write to the free pages and blocks (pollute the pages) at a lower rate than these are being copied. Completion time \u00b6 In some cases the virtual machine can write to different memory pages / disk blocks at a higher rate than these can be copied, which will prevent the migration process from completing in a reasonable amount of time. In this case, live migration will be aborted if it is running for a long period of time. The timeout is calculated base on the size of the VMI, it's memory and the ephemeral disks that are needed to be copied. The configurable parameter completionTimeoutPerGiB , which defaults to 800s is the time for GiB of data to wait for the migration to be completed before aborting it. A VMI with 8Gib of memory will time out after 6400 seconds. Progress timeout \u00b6 Live migration will also be aborted when it will be noticed that copying memory doesn't make any progress. The time to wait for live migration to make progress in transferring data is configurable by progressTimeout parameter, which defaults to 150s Disabling secure migrations \u00b6 FEATURE STATE: KubeVirt v0.43 Sometimes it may be desirable to disable TLS encryption of migrations to improve performance. Use disableTLS to do that: apiVersion: kubevirt.io/v1 kind: Kubevirt metadata: name: kubevirt namespace: kubevirt spec: configuration: developerConfiguration: featureGates: - \"LiveMigration\" migrationConfiguration: disableTLS: true Note: While this increases perfomance it may allow MITM attacks. Be careful.","title":"Live Migration"},{"location":"operations/live_migration/#live-migration","text":"Live migration is a process during which a running Virtual Machine Instance moves to another compute node while the guest workload continues to run and remain accessible.","title":"Live Migration"},{"location":"operations/live_migration/#enabling-the-live-migration-support","text":"Live migration must be enabled in the feature gates to be supported. The feature gates field in the KubeVirt CR must be expanded by adding the LiveMigration to it.","title":"Enabling the live-migration support"},{"location":"operations/live_migration/#limitations","text":"Virtual machines using a PersistentVolumeClaim (PVC) must have a shared ReadWriteMany (RWX) access mode to be live migrated. Live migration is not allowed with a pod network binding of bridge interface type ( ) Live migration requires ports 49152, 49153 to be available in the virt-launcher pod. If these ports are explicitly specified in masquarade interface , live migration will not function.","title":"Limitations"},{"location":"operations/live_migration/#initiate-live-migration","text":"Live migration is initiated by posting a VirtualMachineInstanceMigration (VMIM) object to the cluster. The example below starts a migration process for a virtual machine instance vmi-fedora apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstanceMigration metadata: name: migration-job spec: vmiName: vmi-fedora","title":"Initiate live migration"},{"location":"operations/live_migration/#migration-status-reporting","text":"","title":"Migration Status Reporting"},{"location":"operations/live_migration/#condition-and-migration-method","text":"When starting a virtual machine instance, it has also been calculated whether the machine is live migratable. The result is being stored in the VMI VMI.status.conditions . The calculation can be based on multiple parameters of the VMI, however, at the moment, the calculation is largely based on the Access Mode of the VMI volumes. Live migration is only permitted when the volume access mode is set to ReadWriteMany . Requests to migrate a non-LiveMigratable VMI will be rejected. The reported Migration Method is also being calculated during VMI start. BlockMigration indicates that some of the VMI disks require copying from the source to the destination. LiveMigration means that only the instance memory will be copied. Status: Conditions: Status: True Type: LiveMigratable Migration Method: BlockMigration","title":"Condition and migration method"},{"location":"operations/live_migration/#migration-status","text":"The migration progress status is being reported in the VMI VMI.status . Most importantly, it indicates whether the migration has been Completed or if it Failed . Below is an example of a successful migration. Migration State: Completed: true End Timestamp: 2019-03-29T03:37:52Z Migration Config: Completion Timeout Per GiB: 800 Progress Timeout: 150 Migration UID: c64d4898-51d3-11e9-b370-525500d15501 Source Node: node02 Start Timestamp: 2019-03-29T04:02:47Z Target Direct Migration Node Ports: 35001: 0 41068: 49152 38284: 49153 Target Node: node01 Target Node Address: 10.128.0.46 Target Node Domain Detected: true Target Pod: virt-launcher-testvmimcbjgw6zrzcmp8wpddvztvzm7x2k6cjbdgktwv8tkq","title":"Migration Status"},{"location":"operations/live_migration/#cancel-live-migration","text":"Live migration can also be canceled by simply deleting the migration object. A successfully aborted migration will indicate that the abort has been requested Abort Requested , and that it succeeded: Abort Status: Succeeded . The migration in this case will be Completed and Failed . Migration State: Abort Requested: true Abort Status: Succeeded Completed: true End Timestamp: 2019-03-29T04:02:49Z Failed: true Migration Config: Completion Timeout Per GiB: 800 Progress Timeout: 150 Migration UID: 57a693d6-51d7-11e9-b370-525500d15501 Source Node: node02 Start Timestamp: 2019-03-29T04:02:47Z Target Direct Migration Node Ports: 39445: 0 43345: 49152 44222: 49153 Target Node: node01 Target Node Address: 10.128.0.46 Target Node Domain Detected: true Target Pod: virt-launcher-testvmimcbjgw6zrzcmp8wpddvztvzm7x2k6cjbdgktwv8tkq","title":"Cancel live migration"},{"location":"operations/live_migration/#changing-cluster-wide-migration-limits","text":"KubeVirt puts some limits in place, so that migrations don't overwhelm the cluster. By default, it is configured to only run 5 migrations in parallel with an additional limit of a maximum of 2 outbound migrations per node. Finally, every migration is limited to a bandwidth of 64MiB/s . These values can be change in the kubevirt CR: apiVersion: kubevirt.io/v1alpha3 kind: Kubevirt metadata: name: kubevirt namespace: kubevirt spec: configuration: developerConfiguration: featureGates: - \"LiveMigration\" migrationConfiguration: parallelMigrationsPerCluster: 5 parallelOutboundMigrationsPerNode: 2 bandwidthPerMigration: 64Mi completionTimeoutPerGiB: 800 progressTimeout: 150 disableTLS: false","title":"Changing Cluster Wide Migration Limits"},{"location":"operations/live_migration/#migration-timeouts","text":"Depending on the type, the live migration process will copy virtual machine memory pages and disk blocks to the destination. During this process non-locked pages and blocks are being copied and become free for the instance to use again. To achieve a successful migration, it is assumed that the instance will write to the free pages and blocks (pollute the pages) at a lower rate than these are being copied.","title":"Migration timeouts"},{"location":"operations/live_migration/#completion-time","text":"In some cases the virtual machine can write to different memory pages / disk blocks at a higher rate than these can be copied, which will prevent the migration process from completing in a reasonable amount of time. In this case, live migration will be aborted if it is running for a long period of time. The timeout is calculated base on the size of the VMI, it's memory and the ephemeral disks that are needed to be copied. The configurable parameter completionTimeoutPerGiB , which defaults to 800s is the time for GiB of data to wait for the migration to be completed before aborting it. A VMI with 8Gib of memory will time out after 6400 seconds.","title":"Completion time"},{"location":"operations/live_migration/#progress-timeout","text":"Live migration will also be aborted when it will be noticed that copying memory doesn't make any progress. The time to wait for live migration to make progress in transferring data is configurable by progressTimeout parameter, which defaults to 150s","title":"Progress timeout"},{"location":"operations/live_migration/#disabling-secure-migrations","text":"FEATURE STATE: KubeVirt v0.43 Sometimes it may be desirable to disable TLS encryption of migrations to improve performance. Use disableTLS to do that: apiVersion: kubevirt.io/v1 kind: Kubevirt metadata: name: kubevirt namespace: kubevirt spec: configuration: developerConfiguration: featureGates: - \"LiveMigration\" migrationConfiguration: disableTLS: true Note: While this increases perfomance it may allow MITM attacks. Be careful.","title":"Disabling secure migrations"},{"location":"operations/node_assignment/","text":"Node assignment \u00b6 You can constrain the VM to only run on specific nodes or to prefer running on specific nodes: nodeSelector Affinity and anti-affinity Taints and Tolerations nodeSelector \u00b6 Setting spec.nodeSelector requirements, constrains the scheduler to only schedule VMs on nodes, which contain the specified labels. In the following example the vmi contains the labels cpu: slow and storage: fast : metadata: name: testvmi-ephemeral apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance spec: nodeSelector: cpu: slow storage: fast domain: resources: requests: memory: 64M devices: disks: - name: mypvcdisk lun: {} volumes: - name: mypvcdisk persistentVolumeClaim: claimName: mypvc Thus the scheduler will only schedule the vmi to nodes which contain these labels in their metadata. It works exactly like the Pods nodeSelector . See the Pod nodeSelector Documentation for more examples. Affinity and anti-affinity \u00b6 The spec.affinity field allows specifying hard- and soft-affinity for VMs. It is possible to write matching rules against workloads (VMs and Pods) and Nodes. Since VMs are a workload type based on Pods, Pod-affinity affects VMs as well. An example for podAffinity and podAntiAffinity may look like this: metadata: name: testvmi-ephemeral apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance spec: nodeSelector: cpu: slow storage: fast domain: resources: requests: memory: 64M devices: disks: - name: mypvcdisk lun: {} affinity: podAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: security operator: In values: - S1 topologyKey: failure-domain.beta.kubernetes.io/zone podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: security operator: In values: - S2 topologyKey: kubernetes.io/hostname volumes: - name: mypvcdisk persistentVolumeClaim: claimName: mypvc Affinity and anti-affinity works exactly like the Pods affinity . This includes podAffinity , podAntiAffinity , nodeAffinity and nodeAntiAffinity . See the Pod affinity and anti-affinity Documentation for more examples and details. Taints and Tolerations \u00b6 Affinity as described above, is a property of VMs that attracts them to a set of nodes (either as a preference or a hard requirement). Taints are the opposite - they allow a node to repel a set of VMs. Taints and tolerations work together to ensure that VMs are not scheduled onto inappropriate nodes. One or more taints are applied to a node; this marks that the node should not accept any VMs that do not tolerate the taints. Tolerations are applied to VMs, and allow (but do not require) the VMs to schedule onto nodes with matching taints. You add a taint to a node using kubectl taint. For example, kubectl taint nodes node1 key=value:NoSchedule An example for tolerations may look like this: metadata: name: testvmi-ephemeral apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance spec: nodeSelector: cpu: slow storage: fast domain: resources: requests: memory: 64M devices: disks: - name: mypvcdisk lun: {} tolerations: - key: \"key\" operator: \"Equal\" value: \"value\" effect: \"NoSchedule\" Node balancing with Descheduler \u00b6 In some cases we might need to rebalance the cluster on current scheduling policy and load conditions. Descheduler can find pods, which violates e.g. scheduling decisions and evict them based on descheduler policies. Kubevirt VMs are handled as pods with local storage, so by default, descheduler will not evict them. But it can be easily overridden by adding special annotation to the VMI template in the VM: spec: template: metadata: annotations: descheduler.alpha.kubernetes.io/evict: true This annotation will cause, that the descheduler will be able to evict the VM's pod which can then be scheduled by scheduler on different nodes. A VirtualMachine will never restart or re-create a VirtualMachineInstance until the current instance of the VirtualMachineInstance is deleted from the cluster.","title":"Node assignment"},{"location":"operations/node_assignment/#node-assignment","text":"You can constrain the VM to only run on specific nodes or to prefer running on specific nodes: nodeSelector Affinity and anti-affinity Taints and Tolerations","title":"Node assignment"},{"location":"operations/node_assignment/#nodeselector","text":"Setting spec.nodeSelector requirements, constrains the scheduler to only schedule VMs on nodes, which contain the specified labels. In the following example the vmi contains the labels cpu: slow and storage: fast : metadata: name: testvmi-ephemeral apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance spec: nodeSelector: cpu: slow storage: fast domain: resources: requests: memory: 64M devices: disks: - name: mypvcdisk lun: {} volumes: - name: mypvcdisk persistentVolumeClaim: claimName: mypvc Thus the scheduler will only schedule the vmi to nodes which contain these labels in their metadata. It works exactly like the Pods nodeSelector . See the Pod nodeSelector Documentation for more examples.","title":"nodeSelector"},{"location":"operations/node_assignment/#affinity-and-anti-affinity","text":"The spec.affinity field allows specifying hard- and soft-affinity for VMs. It is possible to write matching rules against workloads (VMs and Pods) and Nodes. Since VMs are a workload type based on Pods, Pod-affinity affects VMs as well. An example for podAffinity and podAntiAffinity may look like this: metadata: name: testvmi-ephemeral apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance spec: nodeSelector: cpu: slow storage: fast domain: resources: requests: memory: 64M devices: disks: - name: mypvcdisk lun: {} affinity: podAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: security operator: In values: - S1 topologyKey: failure-domain.beta.kubernetes.io/zone podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: security operator: In values: - S2 topologyKey: kubernetes.io/hostname volumes: - name: mypvcdisk persistentVolumeClaim: claimName: mypvc Affinity and anti-affinity works exactly like the Pods affinity . This includes podAffinity , podAntiAffinity , nodeAffinity and nodeAntiAffinity . See the Pod affinity and anti-affinity Documentation for more examples and details.","title":"Affinity and anti-affinity"},{"location":"operations/node_assignment/#taints-and-tolerations","text":"Affinity as described above, is a property of VMs that attracts them to a set of nodes (either as a preference or a hard requirement). Taints are the opposite - they allow a node to repel a set of VMs. Taints and tolerations work together to ensure that VMs are not scheduled onto inappropriate nodes. One or more taints are applied to a node; this marks that the node should not accept any VMs that do not tolerate the taints. Tolerations are applied to VMs, and allow (but do not require) the VMs to schedule onto nodes with matching taints. You add a taint to a node using kubectl taint. For example, kubectl taint nodes node1 key=value:NoSchedule An example for tolerations may look like this: metadata: name: testvmi-ephemeral apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance spec: nodeSelector: cpu: slow storage: fast domain: resources: requests: memory: 64M devices: disks: - name: mypvcdisk lun: {} tolerations: - key: \"key\" operator: \"Equal\" value: \"value\" effect: \"NoSchedule\"","title":"Taints and Tolerations"},{"location":"operations/node_assignment/#node-balancing-with-descheduler","text":"In some cases we might need to rebalance the cluster on current scheduling policy and load conditions. Descheduler can find pods, which violates e.g. scheduling decisions and evict them based on descheduler policies. Kubevirt VMs are handled as pods with local storage, so by default, descheduler will not evict them. But it can be easily overridden by adding special annotation to the VMI template in the VM: spec: template: metadata: annotations: descheduler.alpha.kubernetes.io/evict: true This annotation will cause, that the descheduler will be able to evict the VM's pod which can then be scheduled by scheduler on different nodes. A VirtualMachine will never restart or re-create a VirtualMachineInstance until the current instance of the VirtualMachineInstance is deleted from the cluster.","title":"Node balancing with Descheduler"},{"location":"operations/node_maintenance/","text":"Node maintenance \u00b6 Before removing a kubernetes node from the cluster, users will want to ensure that VirtualMachineInstances have been gracefully terminated before powering down the node. Since all VirtualMachineInstances are backed by a Pod, the recommended method of evicting VirtualMachineInstances is to use the kubectl drain command, or in the case of OKD the oc adm drain command. Evict all VMs from a Node \u00b6 Select the node you'd like to evict VirtualMachineInstances from by identifying the node from the list of cluster nodes. kubectl get nodes The following command will gracefully terminate all VMs on a specific node. Replace <node-name> with the name of the node where the eviction should occur. kubectl drain <node-name> --delete-local-data --ignore-daemonsets=true --force --pod-selector=kubevirt.io=virt-launcher Below is a break down of why each argument passed to the drain command is required. kubectl drain <node-name> is selecting a specific node as a target for the eviction --delete-local-data is a required flag that is necessary for removing any pod that utilizes an emptyDir volume. The VirtualMachineInstance Pod does use emptyDir volumes, however the data in those volumes are ephemeral which means it is safe to delete after termination. --ignore-daemonsets=true is a required flag because every node running a VirtualMachineInstance will also be running our helper DaemonSet called virt-handler. DaemonSets are not allowed to be evicted using kubectl drain . By default, if this command encounters a DaemonSet on the target node, the command will fail. This flag tells the command it is safe to proceed with the eviction and to just ignore DaemonSets. --force is a required flag because VirtualMachineInstance pods are not owned by a ReplicaSet or DaemonSet controller. This means kubectl can't guarantee that the pods being terminated on the target node will get re-scheduled replacements placed else where in the cluster after the pods are evicted. KubeVirt has its own controllers which manage the underlying VirtualMachineInstance pods. Each controller behaves differently to a VirtualMachineInstance being evicted. That behavior is outlined further down in this document. --pod-selector=kubevirt.io=virt-launcher means only VirtualMachineInstance pods managed by KubeVirt will be removed from the node. Evict all VMs and Pods from a Node \u00b6 By removing the -pod-selector argument from the previous command, we can issue the eviction of all Pods on a node. This command ensures Pods associated with VMs as well as all other Pods are evicted from the target node. kubectl drain <node name> --delete-local-data --ignore-daemonsets=true --force Evacuate VMIs via Live Migration from a Node \u00b6 If the LiveMigration feature gate is enabled, it is possible to specify an evictionStrategy on VMIs which will react with live-migrations on specific taints on nodes. The following snippet on a VMI ensures that the VMI is migrated if the kubevirt.io/drain:NoSchedule taint is added to a nodes: spec: evictionStrategy: LiveMigrate Here a full VMI: apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance metadata: name: testvmi-nocloud spec: terminationGracePeriodSeconds: 30 evictionStrategy: LiveMigrate domain: resources: requests: memory: 1024M devices: disks: - name: containerdisk disk: bus: virtio - disk: bus: virtio name: cloudinitdisk volumes: - name: containerdisk containerDisk: image: kubevirt/fedora-cloud-container-disk-demo:latest - name: cloudinitdisk cloudInitNoCloud: userData: |- #cloud-config password: fedora chpasswd: { expire: False } Once the VMI is created, taint the node with kubectl taint nodes foo kubevirt.io/drain=draining:NoSchedule which will trigger a migration. Behind the scenes a PodDisruptionBudget is created for each VMI which has an evictionStrategy defined. This ensures that evictions are be blocked on these VMIs and that we can guarantee that a VMI will be migrated instead of shut off. Note: While the evictionStrategy blocks the shutdown of VMIs during evictions, the live migration process is detached from the drain process itself. Therefore it is necessary to add specified taints as part of the drain process explicitly, until we have a better integrated solution. By default KubeVirt will trigger live migrations if the taint kubevirt.io/drain:NoSchedule is added to the node. It is possible to configure a different key in the kubevirt CR, by setting the migration option nodeDrainTaintKey : apiVersion: kubevirt.io/v1alpha3 kind: Kubevirt metadata: name: kubevirt namespace: kubevirt spec: configuration: developerConfiguration: featureGates: - \"LiveMigration\" migrationConfiguration: nodeDrainTaintKey: mytaint/drain The default value is kubevirt.io/drain . With the change above migrations can be triggered with kubectl taint nodes foo mytaint/drain=draining:NoSchedule Here a full drain flow for nodes which includes VMI live migrations with the default setting: kubectl taint nodes foo kubevirt.io/drain=draining:NoSchedule kubectl drain foo --delete-local-data --ignore-daemonsets=true --force To make the node schedulable again, run kubectl taint nodes foo kubevirt.io/drain- kubectl uncordon foo Re-enabling a Node after Eviction \u00b6 The kubectl drain will result in the target node being marked as unschedulable. This means the node will not be eligible for running new VirtualMachineInstances or Pods. If it is decided that the target node should become schedulable again, the following command must be run. kubectl uncordon <node name> or in the case of OKD. oc adm uncordon <node name> Shutting down a Node after Eviction \u00b6 From KubeVirt's perspective, a node is safe to shutdown once all VirtualMachineInstances have been evicted from the node. In a multi-use cluster where VirtualMachineInstances are being scheduled alongside other containerized workloads, it is up to the cluster admin to ensure all other pods have been safely evicted before powering down the node. VirtualMachine Evictions \u00b6 The eviction of any VirtualMachineInstance that is owned by a VirtualMachine set to running=true will result in the VirtualMachineInstance being re-scheduled to another node. The VirtualMachineInstance in this case will be forced to power down and restart on another node. In the future once KubeVirt introduces live migration support, the VM will be able to seamlessly migrate to another node during eviction. VirtualMachineInstanceReplicaSet Eviction Behavior \u00b6 The eviction of VirtualMachineInstances owned by a VirtualMachineInstanceReplicaSet will result in the VirtualMachineInstanceReplicaSet scheduling replacements for the evicted VirtualMachineInstances on other nodes in the cluster. VirtualMachineInstance Eviction Behavior \u00b6 VirtualMachineInstances not backed by either a VirtualMachineInstanceReplicaSet or an VirtualMachine object will not be re-scheduled after eviction.","title":"Node maintenance"},{"location":"operations/node_maintenance/#node-maintenance","text":"Before removing a kubernetes node from the cluster, users will want to ensure that VirtualMachineInstances have been gracefully terminated before powering down the node. Since all VirtualMachineInstances are backed by a Pod, the recommended method of evicting VirtualMachineInstances is to use the kubectl drain command, or in the case of OKD the oc adm drain command.","title":"Node maintenance"},{"location":"operations/node_maintenance/#evict-all-vms-from-a-node","text":"Select the node you'd like to evict VirtualMachineInstances from by identifying the node from the list of cluster nodes. kubectl get nodes The following command will gracefully terminate all VMs on a specific node. Replace <node-name> with the name of the node where the eviction should occur. kubectl drain <node-name> --delete-local-data --ignore-daemonsets=true --force --pod-selector=kubevirt.io=virt-launcher Below is a break down of why each argument passed to the drain command is required. kubectl drain <node-name> is selecting a specific node as a target for the eviction --delete-local-data is a required flag that is necessary for removing any pod that utilizes an emptyDir volume. The VirtualMachineInstance Pod does use emptyDir volumes, however the data in those volumes are ephemeral which means it is safe to delete after termination. --ignore-daemonsets=true is a required flag because every node running a VirtualMachineInstance will also be running our helper DaemonSet called virt-handler. DaemonSets are not allowed to be evicted using kubectl drain . By default, if this command encounters a DaemonSet on the target node, the command will fail. This flag tells the command it is safe to proceed with the eviction and to just ignore DaemonSets. --force is a required flag because VirtualMachineInstance pods are not owned by a ReplicaSet or DaemonSet controller. This means kubectl can't guarantee that the pods being terminated on the target node will get re-scheduled replacements placed else where in the cluster after the pods are evicted. KubeVirt has its own controllers which manage the underlying VirtualMachineInstance pods. Each controller behaves differently to a VirtualMachineInstance being evicted. That behavior is outlined further down in this document. --pod-selector=kubevirt.io=virt-launcher means only VirtualMachineInstance pods managed by KubeVirt will be removed from the node.","title":"Evict all VMs from a Node"},{"location":"operations/node_maintenance/#evict-all-vms-and-pods-from-a-node","text":"By removing the -pod-selector argument from the previous command, we can issue the eviction of all Pods on a node. This command ensures Pods associated with VMs as well as all other Pods are evicted from the target node. kubectl drain <node name> --delete-local-data --ignore-daemonsets=true --force","title":"Evict all VMs and Pods from a Node"},{"location":"operations/node_maintenance/#evacuate-vmis-via-live-migration-from-a-node","text":"If the LiveMigration feature gate is enabled, it is possible to specify an evictionStrategy on VMIs which will react with live-migrations on specific taints on nodes. The following snippet on a VMI ensures that the VMI is migrated if the kubevirt.io/drain:NoSchedule taint is added to a nodes: spec: evictionStrategy: LiveMigrate Here a full VMI: apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance metadata: name: testvmi-nocloud spec: terminationGracePeriodSeconds: 30 evictionStrategy: LiveMigrate domain: resources: requests: memory: 1024M devices: disks: - name: containerdisk disk: bus: virtio - disk: bus: virtio name: cloudinitdisk volumes: - name: containerdisk containerDisk: image: kubevirt/fedora-cloud-container-disk-demo:latest - name: cloudinitdisk cloudInitNoCloud: userData: |- #cloud-config password: fedora chpasswd: { expire: False } Once the VMI is created, taint the node with kubectl taint nodes foo kubevirt.io/drain=draining:NoSchedule which will trigger a migration. Behind the scenes a PodDisruptionBudget is created for each VMI which has an evictionStrategy defined. This ensures that evictions are be blocked on these VMIs and that we can guarantee that a VMI will be migrated instead of shut off. Note: While the evictionStrategy blocks the shutdown of VMIs during evictions, the live migration process is detached from the drain process itself. Therefore it is necessary to add specified taints as part of the drain process explicitly, until we have a better integrated solution. By default KubeVirt will trigger live migrations if the taint kubevirt.io/drain:NoSchedule is added to the node. It is possible to configure a different key in the kubevirt CR, by setting the migration option nodeDrainTaintKey : apiVersion: kubevirt.io/v1alpha3 kind: Kubevirt metadata: name: kubevirt namespace: kubevirt spec: configuration: developerConfiguration: featureGates: - \"LiveMigration\" migrationConfiguration: nodeDrainTaintKey: mytaint/drain The default value is kubevirt.io/drain . With the change above migrations can be triggered with kubectl taint nodes foo mytaint/drain=draining:NoSchedule Here a full drain flow for nodes which includes VMI live migrations with the default setting: kubectl taint nodes foo kubevirt.io/drain=draining:NoSchedule kubectl drain foo --delete-local-data --ignore-daemonsets=true --force To make the node schedulable again, run kubectl taint nodes foo kubevirt.io/drain- kubectl uncordon foo","title":"Evacuate VMIs via Live Migration from a Node"},{"location":"operations/node_maintenance/#re-enabling-a-node-after-eviction","text":"The kubectl drain will result in the target node being marked as unschedulable. This means the node will not be eligible for running new VirtualMachineInstances or Pods. If it is decided that the target node should become schedulable again, the following command must be run. kubectl uncordon <node name> or in the case of OKD. oc adm uncordon <node name>","title":"Re-enabling a Node after Eviction"},{"location":"operations/node_maintenance/#shutting-down-a-node-after-eviction","text":"From KubeVirt's perspective, a node is safe to shutdown once all VirtualMachineInstances have been evicted from the node. In a multi-use cluster where VirtualMachineInstances are being scheduled alongside other containerized workloads, it is up to the cluster admin to ensure all other pods have been safely evicted before powering down the node.","title":"Shutting down a Node after Eviction"},{"location":"operations/node_maintenance/#virtualmachine-evictions","text":"The eviction of any VirtualMachineInstance that is owned by a VirtualMachine set to running=true will result in the VirtualMachineInstance being re-scheduled to another node. The VirtualMachineInstance in this case will be forced to power down and restart on another node. In the future once KubeVirt introduces live migration support, the VM will be able to seamlessly migrate to another node during eviction.","title":"VirtualMachine Evictions"},{"location":"operations/node_maintenance/#virtualmachineinstancereplicaset-eviction-behavior","text":"The eviction of VirtualMachineInstances owned by a VirtualMachineInstanceReplicaSet will result in the VirtualMachineInstanceReplicaSet scheduling replacements for the evicted VirtualMachineInstances on other nodes in the cluster.","title":"VirtualMachineInstanceReplicaSet Eviction Behavior"},{"location":"operations/node_maintenance/#virtualmachineinstance-eviction-behavior","text":"VirtualMachineInstances not backed by either a VirtualMachineInstanceReplicaSet or an VirtualMachine object will not be re-scheduled after eviction.","title":"VirtualMachineInstance Eviction Behavior"},{"location":"operations/node_overcommit/","text":"Node overcommit \u00b6 KubeVirt does not yet support classical Memory Overcommit Management or Memory Ballooning. In other words VirtualMachineInstances can't give back memory they have allocated. However, a few other things can be tweaked to reduce the memory footprint and overcommit the per-VMI memory overhead. Remove the Graphical Devices \u00b6 First the safest option to reduce the memory footprint, is removing the graphical device from the VMI by setting spec.domain.devices.autottachGraphicsDevice to false . See the video and graphics device documentation for further details and examples. This will save a constant amount of 16MB per VirtualMachineInstance but also disable VNC access. Overcommit the Guest Overhead \u00b6 Before you continue, make sure you make yourself comfortable with the Out of Resource Management of Kubernetes. Every VirtualMachineInstance requests slightly more memory from Kubernetes than what was requested by the user for the Operating System. The additional memory is used for the per-VMI overhead consisting of our infrastructure which is wrapping the actual VirtualMachineInstance process. In order to increase the VMI density on the node, it is possible to not request the additional overhead by setting spec.domain.resources.overcommitGuestOverhead to true : apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance metadata: name: testvmi-nocloud spec: terminationGracePeriodSeconds: 30 domain: resources: overcommitGuestOverhead: true requests: memory: 1024M [...] This will work fine for as long as most of the VirtualMachineInstances will not request the whole memory. That is especially the case if you have short-lived VMIs. But if you have long-lived VirtualMachineInstances or do extremely memory intensive tasks inside the VirtualMachineInstance, your VMIs will use all memory they are granted sooner or later. Overcommit Guest Memory \u00b6 The third option is real memory overcommit on the VMI. In this scenario the VMI is explicitly told that it has more memory available than what is requested from the cluster by setting spec.domain.memory.guest to a value higher than spec.domain.resources.requests.memory . The following definition requests 1024MB from the cluster but tells the VMI that it has 2048MB of memory available: apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance metadata: name: testvmi-nocloud spec: terminationGracePeriodSeconds: 30 domain: resources: overcommitGuestOverhead: true requests: memory: 1024M memory: guest: 2048M [...] For as long as there is enough free memory available on the node, the VMI can happily consume up to 2048MB . This VMI will get the Burstable resource class assigned by Kubernetes (See QoS classes in Kubernetes for more details). The same eviction rules like for Pods apply to the VMI in case the node gets under memory pressure. Implicit memory overcommit is disabled by default. This means that when memory request is not specified, it is set to match spec.domain.memory.guest . However, it can be enabled using spec.configuration.developerConfiguration.memoryOvercommit in the kubevirt CR. For example, by setting memoryOvercommit: \"150\" we define that when memory request is not explicitly set, it will be implicitly set to achieve memory overcommit of 150%. For instance, when spec.domain.memory.guest: 3072M , memory request is set to 2048M, if omitted. Note that the actual memory request depends on additional configuration options like OvercommitGuestOverhead. Configuring the memory pressure behavior of nodes \u00b6 If the node gets under memory pressure, depending on the kubelet configuration the virtual machines may get killed by the OOM handler or by the kubelet itself. It is possible to tweak that behaviour based on the requirements of your VirtualMachineInstances by: Configuring Soft Eviction Thresholds Configuring Hard Eviction Thresholds Requesting the right QoS class for VirtualMachineInstances Setting --system-reserved and --kubelet-reserved Enabling KSM Enabling swap Configuring Soft Eviction Thresholds \u00b6 Note: Soft Eviction will effectively shutdown VirtualMachineInstances. They are not paused, hibernated or migrated. Further, Soft Eviction is disabled by default. If configured, VirtualMachineInstances get evicted once the available memory falls below the threshold specified via --eviction-soft and the VirtualmachineInstance is given the chance to perform a shutdown of the VMI within a timespan specified via --eviction-max-pod-grace-period . The flag --eviction-soft-grace-period specifies for how long a soft eviction condition must be held before soft evictions are triggered. If set properly according to the demands of the VMIs, overcommitting should only lead to soft evictions in rare cases for some VMIs. They may even get re-scheduled to the same node with less initial memory demand. For some workload types, this can be perfectly fine and lead to better overall memory-utilization. Configuring Hard Eviction Thresholds \u00b6 Note: If unspecified, the kubelet will do hard evictions for Pods once memory.available falls below 100Mi . Limits set via --eviction-hard will lead to immediate eviction of VirtualMachineInstances or Pods. This stops VMIs without a grace period and is comparable with power-loss on a real computer. If the hard limit is hit, VMIs may from time to time simply be killed. They may be re-scheduled to the same node immediately again, since they start with less memory consumption again. This can be a simple option, if the memory threshold is only very seldom hit and the work performed by the VMIs is reproducible or it can be resumed from some checkpoints. Requesting the right QoS Class for VirtualMachineInstances \u00b6 Different QoS classes get assigned to Pods and VirtualMachineInstances based on the requests.memory and limits.memory . KubeVirt right now supports the QoS classes Burstable and Guaranteed . Burstable VMIs are evicted before Guaranteed VMIs. This allows creating two classes of VMIs: One type can have equal requests.memory and limits.memory set and therefore gets the Guaranteed class assigned. This one will not get evicted and should never run into memory issues, but is more demanding. One type can have no limits.memory or a limits.memory which is greater than requests.memory and therefore gets the Burstable class assigned. These VMIs will be evicted first. Setting --system-reserved and --kubelet-reserved \u00b6 It may be important to reserve some memory for other daemons (not DaemonSets) which are running on the same node (ssh, dhcp servers, etc). The reservation can be done with the --system reserved switch. Further for the Kubelet and Docker a special flag called --kubelet-reserved exists. Enabling KSM \u00b6 The KSM (Kernel same-page merging) daemon can be started on the node. Depending on its tuning parameters it can more or less aggressively try to merge identical pages between applications and VirtualMachineInstances. The more aggressive it is configured the more CPU it will use itself, so the memory overcommit advantages comes with a slight CPU performance hit. Config file tuning allows changes to scanning frequency (how often will KSM activate) and aggressiveness (how many pages per second will it scan). Enabling Swap \u00b6 Note: This will definitely make sure that your VirtualMachines can't crash or get evicted from the node but it comes with the cost of pretty unpredictable performance once the node runs out of memory and the kubelet may not detect that it should evict Pods to increase the performance again. Enabling swap is in general not recommended on Kubernetes right now. However, it can be useful in combination with KSM, since KSM merges identical pages over time. Swap allows the VMIs to successfully allocate memory which will then effectively never be used because of the later de-duplication done by KSM. Node CPU allocation ratio \u00b6 KubeVirt runs Virtual Machines in a Kubernetes Pod. This pod requests a certain amount of CPU time from the host. On the other hand, the Virtual Machine is being created with a certain amount of vCPUs. The number of vCPUs may not necessarily correlate to the number of requested CPUs by the POD. Depending on the QOS of the POD, vCPUs can be scheduled on a variable amount of physical CPUs; this depends on the available CPU resources on a node. When there are fewer available CPUs on the node as the requested vCPU, vCPU will be over committed. By default, each pod requests 100mil of CPU time. The CPU requested on the pod sets the cgroups cpu.shares which serves as a priority for the scheduler to provide CPU time for vCPUs in this POD. As the number of vCPUs increases, this will reduce the amount of CPU time each vCPU may get when competing with other processes on the node or other Virtual Machine Instances with a lower amount of vCPUs. The cpuAllocationRatio comes to normalize the amount of CPU time the POD will request based on the number of vCPUs. For example, POD CPU request = number of vCPUs * 1/cpuAllocationRatio When cpuAllocationRatio is set to 1, a full amount of vCPUs will be requested for the POD. Note: In Kubernetes, one full core is 1000 of CPU time More Information Administrators can change this ratio by updating the KubeVirt CR ... spec: configuration: developerConfiguration: cpuAllocationRatio: 10","title":"Node overcommit"},{"location":"operations/node_overcommit/#node-overcommit","text":"KubeVirt does not yet support classical Memory Overcommit Management or Memory Ballooning. In other words VirtualMachineInstances can't give back memory they have allocated. However, a few other things can be tweaked to reduce the memory footprint and overcommit the per-VMI memory overhead.","title":"Node overcommit"},{"location":"operations/node_overcommit/#remove-the-graphical-devices","text":"First the safest option to reduce the memory footprint, is removing the graphical device from the VMI by setting spec.domain.devices.autottachGraphicsDevice to false . See the video and graphics device documentation for further details and examples. This will save a constant amount of 16MB per VirtualMachineInstance but also disable VNC access.","title":"Remove the Graphical Devices"},{"location":"operations/node_overcommit/#overcommit-the-guest-overhead","text":"Before you continue, make sure you make yourself comfortable with the Out of Resource Management of Kubernetes. Every VirtualMachineInstance requests slightly more memory from Kubernetes than what was requested by the user for the Operating System. The additional memory is used for the per-VMI overhead consisting of our infrastructure which is wrapping the actual VirtualMachineInstance process. In order to increase the VMI density on the node, it is possible to not request the additional overhead by setting spec.domain.resources.overcommitGuestOverhead to true : apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance metadata: name: testvmi-nocloud spec: terminationGracePeriodSeconds: 30 domain: resources: overcommitGuestOverhead: true requests: memory: 1024M [...] This will work fine for as long as most of the VirtualMachineInstances will not request the whole memory. That is especially the case if you have short-lived VMIs. But if you have long-lived VirtualMachineInstances or do extremely memory intensive tasks inside the VirtualMachineInstance, your VMIs will use all memory they are granted sooner or later.","title":"Overcommit the Guest Overhead"},{"location":"operations/node_overcommit/#overcommit-guest-memory","text":"The third option is real memory overcommit on the VMI. In this scenario the VMI is explicitly told that it has more memory available than what is requested from the cluster by setting spec.domain.memory.guest to a value higher than spec.domain.resources.requests.memory . The following definition requests 1024MB from the cluster but tells the VMI that it has 2048MB of memory available: apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance metadata: name: testvmi-nocloud spec: terminationGracePeriodSeconds: 30 domain: resources: overcommitGuestOverhead: true requests: memory: 1024M memory: guest: 2048M [...] For as long as there is enough free memory available on the node, the VMI can happily consume up to 2048MB . This VMI will get the Burstable resource class assigned by Kubernetes (See QoS classes in Kubernetes for more details). The same eviction rules like for Pods apply to the VMI in case the node gets under memory pressure. Implicit memory overcommit is disabled by default. This means that when memory request is not specified, it is set to match spec.domain.memory.guest . However, it can be enabled using spec.configuration.developerConfiguration.memoryOvercommit in the kubevirt CR. For example, by setting memoryOvercommit: \"150\" we define that when memory request is not explicitly set, it will be implicitly set to achieve memory overcommit of 150%. For instance, when spec.domain.memory.guest: 3072M , memory request is set to 2048M, if omitted. Note that the actual memory request depends on additional configuration options like OvercommitGuestOverhead.","title":"Overcommit Guest Memory"},{"location":"operations/node_overcommit/#configuring-the-memory-pressure-behavior-of-nodes","text":"If the node gets under memory pressure, depending on the kubelet configuration the virtual machines may get killed by the OOM handler or by the kubelet itself. It is possible to tweak that behaviour based on the requirements of your VirtualMachineInstances by: Configuring Soft Eviction Thresholds Configuring Hard Eviction Thresholds Requesting the right QoS class for VirtualMachineInstances Setting --system-reserved and --kubelet-reserved Enabling KSM Enabling swap","title":"Configuring the memory pressure behavior of nodes"},{"location":"operations/node_overcommit/#configuring-soft-eviction-thresholds","text":"Note: Soft Eviction will effectively shutdown VirtualMachineInstances. They are not paused, hibernated or migrated. Further, Soft Eviction is disabled by default. If configured, VirtualMachineInstances get evicted once the available memory falls below the threshold specified via --eviction-soft and the VirtualmachineInstance is given the chance to perform a shutdown of the VMI within a timespan specified via --eviction-max-pod-grace-period . The flag --eviction-soft-grace-period specifies for how long a soft eviction condition must be held before soft evictions are triggered. If set properly according to the demands of the VMIs, overcommitting should only lead to soft evictions in rare cases for some VMIs. They may even get re-scheduled to the same node with less initial memory demand. For some workload types, this can be perfectly fine and lead to better overall memory-utilization.","title":"Configuring Soft Eviction Thresholds"},{"location":"operations/node_overcommit/#configuring-hard-eviction-thresholds","text":"Note: If unspecified, the kubelet will do hard evictions for Pods once memory.available falls below 100Mi . Limits set via --eviction-hard will lead to immediate eviction of VirtualMachineInstances or Pods. This stops VMIs without a grace period and is comparable with power-loss on a real computer. If the hard limit is hit, VMIs may from time to time simply be killed. They may be re-scheduled to the same node immediately again, since they start with less memory consumption again. This can be a simple option, if the memory threshold is only very seldom hit and the work performed by the VMIs is reproducible or it can be resumed from some checkpoints.","title":"Configuring Hard Eviction Thresholds"},{"location":"operations/node_overcommit/#requesting-the-right-qos-class-for-virtualmachineinstances","text":"Different QoS classes get assigned to Pods and VirtualMachineInstances based on the requests.memory and limits.memory . KubeVirt right now supports the QoS classes Burstable and Guaranteed . Burstable VMIs are evicted before Guaranteed VMIs. This allows creating two classes of VMIs: One type can have equal requests.memory and limits.memory set and therefore gets the Guaranteed class assigned. This one will not get evicted and should never run into memory issues, but is more demanding. One type can have no limits.memory or a limits.memory which is greater than requests.memory and therefore gets the Burstable class assigned. These VMIs will be evicted first.","title":"Requesting the right QoS Class for VirtualMachineInstances"},{"location":"operations/node_overcommit/#setting-system-reserved-and-kubelet-reserved","text":"It may be important to reserve some memory for other daemons (not DaemonSets) which are running on the same node (ssh, dhcp servers, etc). The reservation can be done with the --system reserved switch. Further for the Kubelet and Docker a special flag called --kubelet-reserved exists.","title":"Setting --system-reserved and --kubelet-reserved"},{"location":"operations/node_overcommit/#enabling-ksm","text":"The KSM (Kernel same-page merging) daemon can be started on the node. Depending on its tuning parameters it can more or less aggressively try to merge identical pages between applications and VirtualMachineInstances. The more aggressive it is configured the more CPU it will use itself, so the memory overcommit advantages comes with a slight CPU performance hit. Config file tuning allows changes to scanning frequency (how often will KSM activate) and aggressiveness (how many pages per second will it scan).","title":"Enabling KSM"},{"location":"operations/node_overcommit/#enabling-swap","text":"Note: This will definitely make sure that your VirtualMachines can't crash or get evicted from the node but it comes with the cost of pretty unpredictable performance once the node runs out of memory and the kubelet may not detect that it should evict Pods to increase the performance again. Enabling swap is in general not recommended on Kubernetes right now. However, it can be useful in combination with KSM, since KSM merges identical pages over time. Swap allows the VMIs to successfully allocate memory which will then effectively never be used because of the later de-duplication done by KSM.","title":"Enabling Swap"},{"location":"operations/node_overcommit/#node-cpu-allocation-ratio","text":"KubeVirt runs Virtual Machines in a Kubernetes Pod. This pod requests a certain amount of CPU time from the host. On the other hand, the Virtual Machine is being created with a certain amount of vCPUs. The number of vCPUs may not necessarily correlate to the number of requested CPUs by the POD. Depending on the QOS of the POD, vCPUs can be scheduled on a variable amount of physical CPUs; this depends on the available CPU resources on a node. When there are fewer available CPUs on the node as the requested vCPU, vCPU will be over committed. By default, each pod requests 100mil of CPU time. The CPU requested on the pod sets the cgroups cpu.shares which serves as a priority for the scheduler to provide CPU time for vCPUs in this POD. As the number of vCPUs increases, this will reduce the amount of CPU time each vCPU may get when competing with other processes on the node or other Virtual Machine Instances with a lower amount of vCPUs. The cpuAllocationRatio comes to normalize the amount of CPU time the POD will request based on the number of vCPUs. For example, POD CPU request = number of vCPUs * 1/cpuAllocationRatio When cpuAllocationRatio is set to 1, a full amount of vCPUs will be requested for the POD. Note: In Kubernetes, one full core is 1000 of CPU time More Information Administrators can change this ratio by updating the KubeVirt CR ... spec: configuration: developerConfiguration: cpuAllocationRatio: 10","title":"Node CPU allocation ratio"},{"location":"operations/snapshot_restore_api/","text":"Snapshot Restore API \u00b6 The snapshot.kubevirt.io API Group defines resources for snapshotting and restoring KubeVirt VirtualMachines Prerequesites \u00b6 VolumeSnapshotClass \u00b6 KubeVirt leverages the VolumeSnapshot functionality of Kubernetes CSI drivers for capturing persistent VirtualMachine state. So, you should make sure that your VirtualMachine uses DataVolumes or PersistentVolumeClaims backed by a StorageClass that supports VolumeSnapshots and a VolumeSnapshotClass is properly configured for that StorageClass . To list VolumeSnapshotClasses : kubectl get volumesnapshotclass Make sure the provisioner property of your StorageClass matches the driver property of the VolumeSnapshotClass Even if you have no VolumeSnapshotClasses in your cluster, VirtualMachineSnapshots are not totally useless. They will still backup your VirtualMachine configuration. Snapshot Feature Gate \u00b6 Snapshot/Restore support must be enabled in the feature gates to be supported. The feature gates field in the KubeVirt CR must be expanded by adding the Snapshot to it. Snapshot a VirtualMachine \u00b6 Snapshotting a virtualMachine is supported for online and offline vms. When snapshotting a running vm the controller will check for qemu guest agent in the vm. If the agent exists it will freeze the vm filesystems before taking the snapshot and unfreeze after the snapshot. It is recommended to take online snapshots with the guest agent for a better snapshot, if not present a best effort snapshot will be taken. Note To check if your vm has a qemu-guest-agent check for 'AgentConnected' in the vm status. There will be an indication in the vmSnapshot status if the snapshot was taken online and with or without guest agent participation. Note Currently online vm snapshot is not supported with hotplugged disks, in this case the vm has to be turned off in order to take the snapshot (or all hotplugged disks unplugged). To snapshot a VirtualMachine named larry , apply the following yaml. apiVersion: snapshot.kubevirt.io/v1alpha1 kind: VirtualMachineSnapshot metadata: name: snap-larry spec: source: apiGroup: kubevirt.io kind: VirtualMachine name: larry To wait for a snapshot to complete, execute: kubectl wait vmsnapshot snap-larry --for condition=Ready You can check the vmSnapshot phase in the vmSnapshot status. It can be one of the following: * InProgress * Succeeded * Failed. The vmSnapshot has a default deadline of 5 minutes. If the vmSnapshot has not succeessfully completed before the deadline, it will be marked as Failed. The VM will be unfrozen and the created snapshot content will be cleaned up if necessary. The vmSnapshot object will remain in Failed state until deleted by the user. To change the default deadline add 'FailureDeadline' to the VirtualMachineSnapshot spec with a new value. The allowed format is a duration string which is a possibly signed sequence of decimal numbers, each with optional fraction and a unit suffix, such as \"300ms\", \"-1.5h\" or \"2h45m\" apiVersion: snapshot.kubevirt.io/v1alpha1 kind: VirtualMachineSnapshot metadata: name: snap-larry spec: source: apiGroup: kubevirt.io kind: VirtualMachine name: larry failureDeadline: 1m In order to set an infinite deadline you can set it to 0 (not recommended). Restoring a VirtualMachine \u00b6 To restore the VirtualMachine larry from VirtualMachineSnapshot snap-larry , apply the following yaml. apiVersion: snapshot.kubevirt.io/v1alpha1 kind: VirtualMachineRestore metadata: name: restore-larry spec: target: apiGroup: kubevirt.io kind: VirtualMachine name: larry virtualMachineSnapshotName: snap-larry To wait for a restore to complete, execute: kubectl wait vmrestore restore-larry --for condition=Ready Cleanup \u00b6 Keep VirtualMachineSnapshots (and their corresponding VirtualMachineSnapshotContents ) around as long as you may want to restore from them again. Feel free to delete larry-restore as it is not needed once the restore is complete.","title":"Snapshot Restore API"},{"location":"operations/snapshot_restore_api/#snapshot-restore-api","text":"The snapshot.kubevirt.io API Group defines resources for snapshotting and restoring KubeVirt VirtualMachines","title":"Snapshot Restore API"},{"location":"operations/snapshot_restore_api/#prerequesites","text":"","title":"Prerequesites"},{"location":"operations/snapshot_restore_api/#volumesnapshotclass","text":"KubeVirt leverages the VolumeSnapshot functionality of Kubernetes CSI drivers for capturing persistent VirtualMachine state. So, you should make sure that your VirtualMachine uses DataVolumes or PersistentVolumeClaims backed by a StorageClass that supports VolumeSnapshots and a VolumeSnapshotClass is properly configured for that StorageClass . To list VolumeSnapshotClasses : kubectl get volumesnapshotclass Make sure the provisioner property of your StorageClass matches the driver property of the VolumeSnapshotClass Even if you have no VolumeSnapshotClasses in your cluster, VirtualMachineSnapshots are not totally useless. They will still backup your VirtualMachine configuration.","title":"VolumeSnapshotClass"},{"location":"operations/snapshot_restore_api/#snapshot-feature-gate","text":"Snapshot/Restore support must be enabled in the feature gates to be supported. The feature gates field in the KubeVirt CR must be expanded by adding the Snapshot to it.","title":"Snapshot Feature Gate"},{"location":"operations/snapshot_restore_api/#snapshot-a-virtualmachine","text":"Snapshotting a virtualMachine is supported for online and offline vms. When snapshotting a running vm the controller will check for qemu guest agent in the vm. If the agent exists it will freeze the vm filesystems before taking the snapshot and unfreeze after the snapshot. It is recommended to take online snapshots with the guest agent for a better snapshot, if not present a best effort snapshot will be taken. Note To check if your vm has a qemu-guest-agent check for 'AgentConnected' in the vm status. There will be an indication in the vmSnapshot status if the snapshot was taken online and with or without guest agent participation. Note Currently online vm snapshot is not supported with hotplugged disks, in this case the vm has to be turned off in order to take the snapshot (or all hotplugged disks unplugged). To snapshot a VirtualMachine named larry , apply the following yaml. apiVersion: snapshot.kubevirt.io/v1alpha1 kind: VirtualMachineSnapshot metadata: name: snap-larry spec: source: apiGroup: kubevirt.io kind: VirtualMachine name: larry To wait for a snapshot to complete, execute: kubectl wait vmsnapshot snap-larry --for condition=Ready You can check the vmSnapshot phase in the vmSnapshot status. It can be one of the following: * InProgress * Succeeded * Failed. The vmSnapshot has a default deadline of 5 minutes. If the vmSnapshot has not succeessfully completed before the deadline, it will be marked as Failed. The VM will be unfrozen and the created snapshot content will be cleaned up if necessary. The vmSnapshot object will remain in Failed state until deleted by the user. To change the default deadline add 'FailureDeadline' to the VirtualMachineSnapshot spec with a new value. The allowed format is a duration string which is a possibly signed sequence of decimal numbers, each with optional fraction and a unit suffix, such as \"300ms\", \"-1.5h\" or \"2h45m\" apiVersion: snapshot.kubevirt.io/v1alpha1 kind: VirtualMachineSnapshot metadata: name: snap-larry spec: source: apiGroup: kubevirt.io kind: VirtualMachine name: larry failureDeadline: 1m In order to set an infinite deadline you can set it to 0 (not recommended).","title":"Snapshot a VirtualMachine"},{"location":"operations/snapshot_restore_api/#restoring-a-virtualmachine","text":"To restore the VirtualMachine larry from VirtualMachineSnapshot snap-larry , apply the following yaml. apiVersion: snapshot.kubevirt.io/v1alpha1 kind: VirtualMachineRestore metadata: name: restore-larry spec: target: apiGroup: kubevirt.io kind: VirtualMachine name: larry virtualMachineSnapshotName: snap-larry To wait for a restore to complete, execute: kubectl wait vmrestore restore-larry --for condition=Ready","title":"Restoring a VirtualMachine"},{"location":"operations/snapshot_restore_api/#cleanup","text":"Keep VirtualMachineSnapshots (and their corresponding VirtualMachineSnapshotContents ) around as long as you may want to restore from them again. Feel free to delete larry-restore as it is not needed once the restore is complete.","title":"Cleanup"},{"location":"operations/unresponsive_nodes/","text":"Unresponsive nodes \u00b6 KubeVirt has its own node daemon, called virt-handler. In addition to the usual k8s methods of detecting issues on nodes, the virt-handler daemon has its own heartbeat mechanism. This allows for fine-tuned error handling of VirtualMachineInstances. virt-handler heartbeat \u00b6 virt-handler periodically tries to update the kubevirt.io/schedulable label and the kubevirt.io/heartbeat annotation on the node it is running on: $ kubectl get nodes -o yaml apiVersion: v1 items: - apiVersion: v1 kind: Node metadata: annotations: kubevirt.io/heartbeat: 2018-11-05T09:42:25Z creationTimestamp: 2018-11-05T08:55:53Z labels: beta.kubernetes.io/arch: amd64 beta.kubernetes.io/os: linux cpumanager: \"false\" kubernetes.io/hostname: node01 kubevirt.io/schedulable: \"true\" node-role.kubernetes.io/master: \"\" If a VirtualMachineInstance gets scheduled, the scheduler is only considering nodes where kubevirt.io/schedulable is true . This can be seen when looking on the corresponding pod of a VirtualMachineInstance : $ kubectl get pods virt-launcher-vmi-nocloud-ct6mr -o yaml apiVersion: v1 kind: Pod metadata: [...] spec: [...] nodeName: node01 nodeSelector: kubevirt.io/schedulable: \"true\" [...] In case there is a communication issue or the host goes down, virt-handler can't update its labels and annotations any-more. Once the last kubevirt.io/heartbeat timestamp is older than five minutes, the KubeVirt node-controller kicks in and sets the kubevirt.io/schedulable label to false . As a consequence no more VMIs will be schedule to this node until virt-handler is connected again. Deleting stuck VMIs when virt-handler is unresponsive \u00b6 In cases where virt-handler has some issues but the node is in general fine, a VirtualMachineInstance can be deleted as usual via kubectl delete vmi <myvm> . Pods of a VirtualMachineInstance will be told by the cluster-controllers they should shut down. As soon as the Pod is gone, the VirtualMachineInstance will be moved to Failed state, if virt-handler did not manage to update it's heartbeat in the meantime. If virt-handler could recover in the meantime, virt-handler will move the VirtualMachineInstance to failed state instead of the cluster-controllers. Deleting stuck VMIs when the whole node is unresponsive \u00b6 If the whole node is unresponsive, deleting a VirtualMachineInstance via kubectl delete vmi <myvmi> alone will never remove the VirtualMachineInstance . In this case all pods on the unresponsive node need to be force-deleted: First make sure that the node is really dead. Then delete all pods on the node via a force-delete: kubectl delete pod --force --grace-period=0 <mypod> . As soon as the pod disappears and the heartbeat from virt-handler timed out, the VMIs will be moved to Failed state. If they were already marked for deletion they will simply disappear. If not, they can be deleted and will disappear almost immediately. Timing considerations \u00b6 It takes up to five minutes until the KubeVirt cluster components can detect that virt-handler is unhealthy. During that time-frame it is possible that new VMIs are scheduled to the affected node. If virt-handler is not capable of connecting to these pods on the node, the pods will sooner or later go to failed state. As soon as the cluster finally detects the issue, the VMIs will be set to failed by the cluster.","title":"Unresponsive nodes"},{"location":"operations/unresponsive_nodes/#unresponsive-nodes","text":"KubeVirt has its own node daemon, called virt-handler. In addition to the usual k8s methods of detecting issues on nodes, the virt-handler daemon has its own heartbeat mechanism. This allows for fine-tuned error handling of VirtualMachineInstances.","title":"Unresponsive nodes"},{"location":"operations/unresponsive_nodes/#virt-handler-heartbeat","text":"virt-handler periodically tries to update the kubevirt.io/schedulable label and the kubevirt.io/heartbeat annotation on the node it is running on: $ kubectl get nodes -o yaml apiVersion: v1 items: - apiVersion: v1 kind: Node metadata: annotations: kubevirt.io/heartbeat: 2018-11-05T09:42:25Z creationTimestamp: 2018-11-05T08:55:53Z labels: beta.kubernetes.io/arch: amd64 beta.kubernetes.io/os: linux cpumanager: \"false\" kubernetes.io/hostname: node01 kubevirt.io/schedulable: \"true\" node-role.kubernetes.io/master: \"\" If a VirtualMachineInstance gets scheduled, the scheduler is only considering nodes where kubevirt.io/schedulable is true . This can be seen when looking on the corresponding pod of a VirtualMachineInstance : $ kubectl get pods virt-launcher-vmi-nocloud-ct6mr -o yaml apiVersion: v1 kind: Pod metadata: [...] spec: [...] nodeName: node01 nodeSelector: kubevirt.io/schedulable: \"true\" [...] In case there is a communication issue or the host goes down, virt-handler can't update its labels and annotations any-more. Once the last kubevirt.io/heartbeat timestamp is older than five minutes, the KubeVirt node-controller kicks in and sets the kubevirt.io/schedulable label to false . As a consequence no more VMIs will be schedule to this node until virt-handler is connected again.","title":"virt-handler heartbeat"},{"location":"operations/unresponsive_nodes/#deleting-stuck-vmis-when-virt-handler-is-unresponsive","text":"In cases where virt-handler has some issues but the node is in general fine, a VirtualMachineInstance can be deleted as usual via kubectl delete vmi <myvm> . Pods of a VirtualMachineInstance will be told by the cluster-controllers they should shut down. As soon as the Pod is gone, the VirtualMachineInstance will be moved to Failed state, if virt-handler did not manage to update it's heartbeat in the meantime. If virt-handler could recover in the meantime, virt-handler will move the VirtualMachineInstance to failed state instead of the cluster-controllers.","title":"Deleting stuck VMIs when virt-handler is unresponsive"},{"location":"operations/unresponsive_nodes/#deleting-stuck-vmis-when-the-whole-node-is-unresponsive","text":"If the whole node is unresponsive, deleting a VirtualMachineInstance via kubectl delete vmi <myvmi> alone will never remove the VirtualMachineInstance . In this case all pods on the unresponsive node need to be force-deleted: First make sure that the node is really dead. Then delete all pods on the node via a force-delete: kubectl delete pod --force --grace-period=0 <mypod> . As soon as the pod disappears and the heartbeat from virt-handler timed out, the VMIs will be moved to Failed state. If they were already marked for deletion they will simply disappear. If not, they can be deleted and will disappear almost immediately.","title":"Deleting stuck VMIs when the whole node is unresponsive"},{"location":"operations/unresponsive_nodes/#timing-considerations","text":"It takes up to five minutes until the KubeVirt cluster components can detect that virt-handler is unhealthy. During that time-frame it is possible that new VMIs are scheduled to the affected node. If virt-handler is not capable of connecting to these pods on the node, the pods will sooner or later go to failed state. As soon as the cluster finally detects the issue, the VMIs will be set to failed by the cluster.","title":"Timing considerations"},{"location":"operations/updating_and_deletion/","text":"Updating and deletion \u00b6 Updating KubeVirt Control Plane \u00b6 Zero downtime rolling updates are supported starting with release v0.17.0 onward. Updating from any release prior to the KubeVirt v0.17.0 release is not supported. Note: Updating is only supported from N-1 to N release. Updates are triggered one of two ways. By changing the imageTag value in the KubeVirt CR's spec. For example, updating from v0.17.0-alpha.1 to v0.17.0 is as simple as patching the KubeVirt CR with the imageTag: v0.17.0 value. From there the KubeVirt operator will begin the process of rolling out the new version of KubeVirt. Existing VM/VMIs will remain uninterrupted both during and after the update succeeds. $ kubectl patch kv kubevirt -n kubevirt --type=json -p '[{ \"op\": \"add\", \"path\": \"/spec/imageTag\", \"value\": \"v0.17.0\" }]' Or, by updating the kubevirt operator if no imageTag value is set. When no imageTag value is set in the kubevirt CR, the system assumes that the version of KubeVirt is locked to the version of the operator. This means that updating the operator will result in the underlying KubeVirt installation being updated as well. $ export RELEASE=v0.26.0 $ kubectl apply -f https://github.com/kubevirt/kubevirt/releases/download/${RELEASE}/kubevirt-operator.yaml The first way provides a fine granular approach where you have full control over what version of KubeVirt is installed independently of what version of the KubeVirt operator you might be running. The second approach allows you to lock both the operator and operand to the same version. Newer KubeVirt may require additional or extended RBAC rules. In this case, the #1 update method may fail, because the virt-operator present in the cluster doesn't have these RBAC rules itself. In this case, you need to update the virt-operator first, and then proceed to update kubevirt. See this issue for more details . Updating KubeVirt Workloads \u00b6 Workload updates are supported as an opt in feature starting with v0.39.0 By default, when KubeVirt is updated this only involves the control plane components. Any existing VirtualMachineInstance (VMI) workloads that are running before an update occurs remain 100% untouched. The workloads continue to run and are not interrupted as part of the default update process. It's important to note that these VMI workloads do involve components such as libvirt, qemu, and virt-launcher, which can optionally be updated during the KubeVirt update process as well. However that requires opting in to having virt-operator perform automated actions on workloads. Opting in to VMI updates involves configuring the workloadUpdateStrategy field on the KubeVirt CR. This field controls the methods virt-operator will use to when updating the VMI workload pods. There are two methods supported. LiveMigrate: Which results in VMIs being updated by live migrating the virtual machine guest into a new pod with all the updated components enabled. Evict: Which results in the VMI's pod being shutdown. If the VMI is controlled by a higher level VirtualMachine object with runStrategy: always , then a new VMI will spin up in a new pod with updated components. The least disruptive way to update VMI workloads is to use LiveMigrate. Any VMI workload that is not live migratable will be left untouched. If live migration is not enabled in the cluster, then the only option available for virt-operator managed VMI updates is the Evict method. Example: Enabling VMI workload updates via LiveMigration apiVersion: kubevirt.io/v1 kind: KubeVirt metadata: name: kubevirt namespace: kubevirt spec: imagePullPolicy: IfNotPresent workloadUpdateStrategy: workloadUpdateMethods: - LiveMigrate Example: Enabling VMI workload updates via Evict with batch tunings The batch tunings allow configuring how quickly VMI's are evicted. In large clusters, it's desirable to ensure that VMI's are evicted in batches in order to distribute load. apiVersion: kubevirt.io/v1 kind: KubeVirt metadata: name: kubevirt namespace: kubevirt spec: imagePullPolicy: IfNotPresent workloadUpdateStrategy: workloadUpdateMethods: - Evict batchEvictSize: 10 batchEvictInterval: \"1m\" Example: Enabling VMI workload updates with both LiveMigrate and Evict When both LiveMigrate and Evict are specified, then any workloads which are live migratable will be guaranteed to be live migrated. Only workloads which are not live migratable will be evicted. apiVersion: kubevirt.io/v1 kind: KubeVirt metadata: name: kubevirt namespace: kubevirt spec: imagePullPolicy: IfNotPresent workloadUpdateStrategy: workloadUpdateMethods: - LiveMigrate - Evict batchEvictSize: 10 batchEvictInterval: \"1m\" Deleting KubeVirt \u00b6 To delete the KubeVirt you should first to delete KubeVirt custom resource and then delete the KubeVirt operator. $ export RELEASE=v0.17.0 $ kubectl delete -n kubevirt kubevirt kubevirt --wait=true # --wait=true should anyway be default $ kubectl delete apiservices v1alpha3.subresources.kubevirt.io # this needs to be deleted to avoid stuck terminating namespaces $ kubectl delete mutatingwebhookconfigurations virt-api-mutator # not blocking but would be left over $ kubectl delete validatingwebhookconfigurations virt-api-validator # not blocking but would be left over $ kubectl delete -f https://github.com/kubevirt/kubevirt/releases/download/${RELEASE}/kubevirt-operator.yaml --wait=false Note: If by mistake you deleted the operator first, the KV custom resource will get stuck in the Terminating state, to fix it, delete manually finalizer from the resource. Note: The apiservice and the webhookconfigurations need to be deleted manually due to a bug. $ kubectl -n kubevirt patch kv kubevirt --type=json -p '[{ \"op\": \"remove\", \"path\": \"/metadata/finalizers\" }]'","title":"Updating and deletion"},{"location":"operations/updating_and_deletion/#updating-and-deletion","text":"","title":"Updating and deletion"},{"location":"operations/updating_and_deletion/#updating-kubevirt-control-plane","text":"Zero downtime rolling updates are supported starting with release v0.17.0 onward. Updating from any release prior to the KubeVirt v0.17.0 release is not supported. Note: Updating is only supported from N-1 to N release. Updates are triggered one of two ways. By changing the imageTag value in the KubeVirt CR's spec. For example, updating from v0.17.0-alpha.1 to v0.17.0 is as simple as patching the KubeVirt CR with the imageTag: v0.17.0 value. From there the KubeVirt operator will begin the process of rolling out the new version of KubeVirt. Existing VM/VMIs will remain uninterrupted both during and after the update succeeds. $ kubectl patch kv kubevirt -n kubevirt --type=json -p '[{ \"op\": \"add\", \"path\": \"/spec/imageTag\", \"value\": \"v0.17.0\" }]' Or, by updating the kubevirt operator if no imageTag value is set. When no imageTag value is set in the kubevirt CR, the system assumes that the version of KubeVirt is locked to the version of the operator. This means that updating the operator will result in the underlying KubeVirt installation being updated as well. $ export RELEASE=v0.26.0 $ kubectl apply -f https://github.com/kubevirt/kubevirt/releases/download/${RELEASE}/kubevirt-operator.yaml The first way provides a fine granular approach where you have full control over what version of KubeVirt is installed independently of what version of the KubeVirt operator you might be running. The second approach allows you to lock both the operator and operand to the same version. Newer KubeVirt may require additional or extended RBAC rules. In this case, the #1 update method may fail, because the virt-operator present in the cluster doesn't have these RBAC rules itself. In this case, you need to update the virt-operator first, and then proceed to update kubevirt. See this issue for more details .","title":"Updating KubeVirt Control Plane"},{"location":"operations/updating_and_deletion/#updating-kubevirt-workloads","text":"Workload updates are supported as an opt in feature starting with v0.39.0 By default, when KubeVirt is updated this only involves the control plane components. Any existing VirtualMachineInstance (VMI) workloads that are running before an update occurs remain 100% untouched. The workloads continue to run and are not interrupted as part of the default update process. It's important to note that these VMI workloads do involve components such as libvirt, qemu, and virt-launcher, which can optionally be updated during the KubeVirt update process as well. However that requires opting in to having virt-operator perform automated actions on workloads. Opting in to VMI updates involves configuring the workloadUpdateStrategy field on the KubeVirt CR. This field controls the methods virt-operator will use to when updating the VMI workload pods. There are two methods supported. LiveMigrate: Which results in VMIs being updated by live migrating the virtual machine guest into a new pod with all the updated components enabled. Evict: Which results in the VMI's pod being shutdown. If the VMI is controlled by a higher level VirtualMachine object with runStrategy: always , then a new VMI will spin up in a new pod with updated components. The least disruptive way to update VMI workloads is to use LiveMigrate. Any VMI workload that is not live migratable will be left untouched. If live migration is not enabled in the cluster, then the only option available for virt-operator managed VMI updates is the Evict method. Example: Enabling VMI workload updates via LiveMigration apiVersion: kubevirt.io/v1 kind: KubeVirt metadata: name: kubevirt namespace: kubevirt spec: imagePullPolicy: IfNotPresent workloadUpdateStrategy: workloadUpdateMethods: - LiveMigrate Example: Enabling VMI workload updates via Evict with batch tunings The batch tunings allow configuring how quickly VMI's are evicted. In large clusters, it's desirable to ensure that VMI's are evicted in batches in order to distribute load. apiVersion: kubevirt.io/v1 kind: KubeVirt metadata: name: kubevirt namespace: kubevirt spec: imagePullPolicy: IfNotPresent workloadUpdateStrategy: workloadUpdateMethods: - Evict batchEvictSize: 10 batchEvictInterval: \"1m\" Example: Enabling VMI workload updates with both LiveMigrate and Evict When both LiveMigrate and Evict are specified, then any workloads which are live migratable will be guaranteed to be live migrated. Only workloads which are not live migratable will be evicted. apiVersion: kubevirt.io/v1 kind: KubeVirt metadata: name: kubevirt namespace: kubevirt spec: imagePullPolicy: IfNotPresent workloadUpdateStrategy: workloadUpdateMethods: - LiveMigrate - Evict batchEvictSize: 10 batchEvictInterval: \"1m\"","title":"Updating KubeVirt Workloads"},{"location":"operations/updating_and_deletion/#deleting-kubevirt","text":"To delete the KubeVirt you should first to delete KubeVirt custom resource and then delete the KubeVirt operator. $ export RELEASE=v0.17.0 $ kubectl delete -n kubevirt kubevirt kubevirt --wait=true # --wait=true should anyway be default $ kubectl delete apiservices v1alpha3.subresources.kubevirt.io # this needs to be deleted to avoid stuck terminating namespaces $ kubectl delete mutatingwebhookconfigurations virt-api-mutator # not blocking but would be left over $ kubectl delete validatingwebhookconfigurations virt-api-validator # not blocking but would be left over $ kubectl delete -f https://github.com/kubevirt/kubevirt/releases/download/${RELEASE}/kubevirt-operator.yaml --wait=false Note: If by mistake you deleted the operator first, the KV custom resource will get stuck in the Terminating state, to fix it, delete manually finalizer from the resource. Note: The apiservice and the webhookconfigurations need to be deleted manually due to a bug. $ kubectl -n kubevirt patch kv kubevirt --type=json -p '[{ \"op\": \"remove\", \"path\": \"/metadata/finalizers\" }]'","title":"Deleting KubeVirt"},{"location":"operations/virtctl_client_tool/","text":"virtctl Client Tool \u00b6 Retrieving the virtctl client tool \u00b6 Basic VirtualMachineInstance operations can be performed with the stock kubectl utility. However, the virtctl binary utility is required to use advanced features such as: Serial and graphical console access It also provides convenience commands for: Starting and stopping VirtualMachineInstances Live migrating VirtualMachineInstances Uploading virtual machine disk images There are two ways to get it: the most recent version of the tool can be retrieved from the official release page it can be installed as a kubectl plugin using krew Example: export VERSION=v0.41.0 wget https://github.com/kubevirt/kubevirt/releases/download/${VERSION}/virtctl-${VERSION}-linux-amd64 Install virtctl with krew \u00b6 It is required to install krew plugin manager beforehand. If krew is installed, virtctl can be installed via krew : $ kubectl krew install virt Then virtctl can be used as a kubectl plugin. For a list of available commands run: $ kubectl virt help Every occurrence throughout this guide of $ ./virtctl <command>... should then be read as $ kubectl virt <command>...","title":"virtctl Client Tool"},{"location":"operations/virtctl_client_tool/#virtctl-client-tool","text":"","title":"virtctl Client Tool"},{"location":"operations/virtctl_client_tool/#retrieving-the-virtctl-client-tool","text":"Basic VirtualMachineInstance operations can be performed with the stock kubectl utility. However, the virtctl binary utility is required to use advanced features such as: Serial and graphical console access It also provides convenience commands for: Starting and stopping VirtualMachineInstances Live migrating VirtualMachineInstances Uploading virtual machine disk images There are two ways to get it: the most recent version of the tool can be retrieved from the official release page it can be installed as a kubectl plugin using krew Example: export VERSION=v0.41.0 wget https://github.com/kubevirt/kubevirt/releases/download/${VERSION}/virtctl-${VERSION}-linux-amd64","title":"Retrieving the virtctl client tool"},{"location":"operations/virtctl_client_tool/#install-virtctl-with-krew","text":"It is required to install krew plugin manager beforehand. If krew is installed, virtctl can be installed via krew : $ kubectl krew install virt Then virtctl can be used as a kubectl plugin. For a list of available commands run: $ kubectl virt help Every occurrence throughout this guide of $ ./virtctl <command>... should then be read as $ kubectl virt <command>...","title":"Install virtctl with krew"},{"location":"virtual_machines/accessing_virtual_machines/","text":"Accessing Virtual Machines \u00b6 Graphical and Serial Console Access \u00b6 Once a virtual machine is started you are able to connect to the consoles it exposes. Usually there are two types of consoles: Serial Console Graphical Console (VNC) Note: You need to have virtctl installed to gain access to the VirtualMachineInstance. Accessing the serial console \u00b6 The serial console of a virtual machine can be accessed by using the console command: $ virtctl console --kubeconfig=$KUBECONFIG testvmi Accessing the graphical console (VNC) \u00b6 Accessing the graphical console of a virtual machine is usually done through VNC, which requires remote-viewer . Once the tool is installed you can access the graphical console using: $ virtctl vnc --kubeconfig=$KUBECONFIG testvmi If you need to open only a vnc-proxy without executing the remote-viewer command, it can be done using: $ virtctl vnc --kubeconfig=$KUBECONFIG --proxy-only testvmi this would print the port number on your machine where you can manually connect using any of the vnc viewers Debugging console access \u00b6 Should the connection fail, you can use the -v flag to get more output from both virtctl and the remote-viewer tool, to troubleshoot the problem. $ virtctl vnc --kubeconfig=$KUBECONFIG testvmi -v 4 Note: If you are using virtctl via ssh on a remote machine, you need to forward the X session to your machine (Look up the -X and -Y flags of ssh if you are not familiar with that). As an alternative you can proxy the apiserver port with ssh to your machine (either direct or in combination with kubectl proxy ) RBAC Permissions for Console/VNC Access \u00b6 Using Default RBAC ClusterRoles \u00b6 Every KubeVirt installation after version v0.5.1 comes a set of default RBAC cluster roles that can be used to grant users access to VirtualMachineInstances. The kubevirt.io:admin and kubevirt.io:edit ClusterRoles have console and VNC access permissions built into them. By binding either of these roles to a user, they will have the ability to use virtctl to access console and VNC. With Custom RBAC ClusterRole \u00b6 The default KubeVirt ClusterRoles give access to more than just console in VNC. In the event that an Admin would like to craft a custom role that targets only console and VNC, the ClusterRole below demonstrates how that can be done. apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRole metadata: name: allow-vnc-console-access rules: - apiGroups: - subresources.kubevirt.io resources: - virtualmachineinstances/console - virtualmachineinstances/vnc verbs: - get The ClusterRole above provides access to virtual machines across all namespaces. In order to reduce the scope to a single namespace, bind this ClusterRole using a RoleBinding that targets a single namespace. SSH Access \u00b6 A common operational pattern used when managing virtual machines is to inject public ssh keys into the virtual machines at boot. This allows automation tools (like ansible) to provision the virtual machine. It also gives operators a way of gaining secure passwordless access to a virtual machine. KubeVirt provides multiple ways to inject ssh public keys into a virtual machine. In general, these methods fall into two categories. Static key injection , which places keys on the virtual machine the first time it is booted, and dynamic injection , which allows keys to be dynamically updated both at boot and during runtime. Static SSH Key Injection via Cloud Init \u00b6 Users creating virtual machines have the ability to provide startup scripts to their virtual machines which allow any number of custom operations to take place. Placing public ssh keys into a cloud-init startup script is one option people have for getting their public keys into the virtual machine, however there are some other options that grant more flexibility. The VM's access credential api allows statically injecting ssh public keys at creation time independently of the cloud-init user data by placing the ssh public key in a Kubernetes secret. This is useful because it allows people creating virtual machines to separate the application data in their cloud-init user data from the credentials used to access the virtual machine. For example, someone can put their ssh key into a Kubernetes secret like this. # Place ssh key into a secret kubectl create secret generic my-pub-key --from-file=key1=/id_rsa.pub Then assign that key to the virtual machine with the access credentials api using the configDrive propagation method. Note here how the cloud-init user data is not touched. KubeVirt is injecting the ssh key into the virtual machine using the machine generated cloud-init metadata, and not the user data. This keeps the application user date separate from credentials. #Create a vm yaml that references the secret in using the access credentials api. cat << END > my-vm.yaml apiVersion: kubevirt.io/v1 kind: VirtualMachine metadata: labels: kubevirt.io/vm: my-vm name: my-vm spec: dataVolumeTemplates: - metadata: creationTimestamp: null name: fedora-dv spec: pvc: accessModes: - ReadWriteOnce resources: requests: storage: 5Gi storageClassName: local source: registry: url: docker://quay.io/kubevirt/fedora-cloud-container-disk-demo running: false template: metadata: labels: kubevirt.io/vm: my-vm spec: domain: devices: disks: - disk: bus: virtio name: disk0 - disk: bus: virtio name: disk1 machine: type: \"\" resources: requests: cpu: 1000m memory: 1G terminationGracePeriodSeconds: 0 accessCredentials: - sshPublicKey: source: secret: secretName: my-pub-key propagationMethod: configDrive: {} volumes: - dataVolume: name: fedora-dv name: disk0 - cloudInitConfigDrive: userData: | #!/bin/bash echo \"Application setup goes here\" name: disk1 END kubectl create -f my-vm.yaml Dynamic SSH Key Injection via Qemu User Agent \u00b6 KubeVirt supports dynamically injecting public ssh keys at run time through the use of the qemu guest agent. This is achieved through the access credentials api by using the qemuGuestAgent propagation method. Note: This requires the qemu guest agent to be installed within the guest Note: When using qemuGuestAgent propagation, the /home/$USER/.ssh/authorized_keys file will be owned by the guest agent. Changes to that file that are made outside of the qemu guest agent's control will get deleted. Note: More information about the motivation behind the access credentials api can be found in the pull request description that introduced this api. In the example below, a secret contains an ssh key. When attached to the VM via the access credential api with the qemuGuestAgent propagation method, the contents of the secret can be updated at any time which will automatically get applied to a running VM. The secret can contain multiple public keys. # Place ssh key into a secret kubectl create secret generic my-pub-key --from-file=key1=/id_rsa.pub Now reference this secret on the VM with the access credentials api using qemuGuestAgent propagation. This example installs and starts the qemu guest agent using a cloud-init script in order to ensure the agent is available. # Create a vm yaml that references the secret in using the access credentials api. cat << END > my-vm.yaml apiVersion: kubevirt.io/v1 kind: VirtualMachine metadata: labels: kubevirt.io/vm: my-vm name: my-vm spec: dataVolumeTemplates: - metadata: creationTimestamp: null name: fedora-dv spec: pvc: accessModes: - ReadWriteOnce resources: requests: storage: 5Gi storageClassName: local source: registry: url: docker://quay.io/kubevirt/fedora-cloud-container-disk-demo running: false template: metadata: labels: kubevirt.io/vm: my-vm spec: domain: devices: disks: - disk: bus: virtio name: disk0 - disk: bus: virtio name: disk1 machine: type: \"\" resources: requests: cpu: 1000m memory: 1G terminationGracePeriodSeconds: 0 accessCredentials: - sshPublicKey: source: secret: secretName: my-pub-key propagationMethod: qemuGuestAgent: users: - \"fedora\" volumes: - dataVolume: name: fedora-dv name: disk0 - cloudInitConfigDrive: userData: | #!/bin/bash sudo setenforce Permissive sudo yum install -y qemu-guest-agent sudo systemctl start qemu-guest-agent name: disk1 END kubectl create -f my-vm.yaml","title":"Accessing Virtual Machines"},{"location":"virtual_machines/accessing_virtual_machines/#accessing-virtual-machines","text":"","title":"Accessing Virtual Machines"},{"location":"virtual_machines/accessing_virtual_machines/#graphical-and-serial-console-access","text":"Once a virtual machine is started you are able to connect to the consoles it exposes. Usually there are two types of consoles: Serial Console Graphical Console (VNC) Note: You need to have virtctl installed to gain access to the VirtualMachineInstance.","title":"Graphical and Serial Console Access"},{"location":"virtual_machines/accessing_virtual_machines/#accessing-the-serial-console","text":"The serial console of a virtual machine can be accessed by using the console command: $ virtctl console --kubeconfig=$KUBECONFIG testvmi","title":"Accessing the serial console"},{"location":"virtual_machines/accessing_virtual_machines/#accessing-the-graphical-console-vnc","text":"Accessing the graphical console of a virtual machine is usually done through VNC, which requires remote-viewer . Once the tool is installed you can access the graphical console using: $ virtctl vnc --kubeconfig=$KUBECONFIG testvmi If you need to open only a vnc-proxy without executing the remote-viewer command, it can be done using: $ virtctl vnc --kubeconfig=$KUBECONFIG --proxy-only testvmi this would print the port number on your machine where you can manually connect using any of the vnc viewers","title":"Accessing the graphical console (VNC)"},{"location":"virtual_machines/accessing_virtual_machines/#debugging-console-access","text":"Should the connection fail, you can use the -v flag to get more output from both virtctl and the remote-viewer tool, to troubleshoot the problem. $ virtctl vnc --kubeconfig=$KUBECONFIG testvmi -v 4 Note: If you are using virtctl via ssh on a remote machine, you need to forward the X session to your machine (Look up the -X and -Y flags of ssh if you are not familiar with that). As an alternative you can proxy the apiserver port with ssh to your machine (either direct or in combination with kubectl proxy )","title":"Debugging console access"},{"location":"virtual_machines/accessing_virtual_machines/#rbac-permissions-for-consolevnc-access","text":"","title":"RBAC Permissions for Console/VNC Access"},{"location":"virtual_machines/accessing_virtual_machines/#using-default-rbac-clusterroles","text":"Every KubeVirt installation after version v0.5.1 comes a set of default RBAC cluster roles that can be used to grant users access to VirtualMachineInstances. The kubevirt.io:admin and kubevirt.io:edit ClusterRoles have console and VNC access permissions built into them. By binding either of these roles to a user, they will have the ability to use virtctl to access console and VNC.","title":"Using Default RBAC ClusterRoles"},{"location":"virtual_machines/accessing_virtual_machines/#with-custom-rbac-clusterrole","text":"The default KubeVirt ClusterRoles give access to more than just console in VNC. In the event that an Admin would like to craft a custom role that targets only console and VNC, the ClusterRole below demonstrates how that can be done. apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRole metadata: name: allow-vnc-console-access rules: - apiGroups: - subresources.kubevirt.io resources: - virtualmachineinstances/console - virtualmachineinstances/vnc verbs: - get The ClusterRole above provides access to virtual machines across all namespaces. In order to reduce the scope to a single namespace, bind this ClusterRole using a RoleBinding that targets a single namespace.","title":"With Custom RBAC ClusterRole"},{"location":"virtual_machines/accessing_virtual_machines/#ssh-access","text":"A common operational pattern used when managing virtual machines is to inject public ssh keys into the virtual machines at boot. This allows automation tools (like ansible) to provision the virtual machine. It also gives operators a way of gaining secure passwordless access to a virtual machine. KubeVirt provides multiple ways to inject ssh public keys into a virtual machine. In general, these methods fall into two categories. Static key injection , which places keys on the virtual machine the first time it is booted, and dynamic injection , which allows keys to be dynamically updated both at boot and during runtime.","title":"SSH Access"},{"location":"virtual_machines/accessing_virtual_machines/#static-ssh-key-injection-via-cloud-init","text":"Users creating virtual machines have the ability to provide startup scripts to their virtual machines which allow any number of custom operations to take place. Placing public ssh keys into a cloud-init startup script is one option people have for getting their public keys into the virtual machine, however there are some other options that grant more flexibility. The VM's access credential api allows statically injecting ssh public keys at creation time independently of the cloud-init user data by placing the ssh public key in a Kubernetes secret. This is useful because it allows people creating virtual machines to separate the application data in their cloud-init user data from the credentials used to access the virtual machine. For example, someone can put their ssh key into a Kubernetes secret like this. # Place ssh key into a secret kubectl create secret generic my-pub-key --from-file=key1=/id_rsa.pub Then assign that key to the virtual machine with the access credentials api using the configDrive propagation method. Note here how the cloud-init user data is not touched. KubeVirt is injecting the ssh key into the virtual machine using the machine generated cloud-init metadata, and not the user data. This keeps the application user date separate from credentials. #Create a vm yaml that references the secret in using the access credentials api. cat << END > my-vm.yaml apiVersion: kubevirt.io/v1 kind: VirtualMachine metadata: labels: kubevirt.io/vm: my-vm name: my-vm spec: dataVolumeTemplates: - metadata: creationTimestamp: null name: fedora-dv spec: pvc: accessModes: - ReadWriteOnce resources: requests: storage: 5Gi storageClassName: local source: registry: url: docker://quay.io/kubevirt/fedora-cloud-container-disk-demo running: false template: metadata: labels: kubevirt.io/vm: my-vm spec: domain: devices: disks: - disk: bus: virtio name: disk0 - disk: bus: virtio name: disk1 machine: type: \"\" resources: requests: cpu: 1000m memory: 1G terminationGracePeriodSeconds: 0 accessCredentials: - sshPublicKey: source: secret: secretName: my-pub-key propagationMethod: configDrive: {} volumes: - dataVolume: name: fedora-dv name: disk0 - cloudInitConfigDrive: userData: | #!/bin/bash echo \"Application setup goes here\" name: disk1 END kubectl create -f my-vm.yaml","title":"Static SSH Key Injection via Cloud Init"},{"location":"virtual_machines/accessing_virtual_machines/#dynamic-ssh-key-injection-via-qemu-user-agent","text":"KubeVirt supports dynamically injecting public ssh keys at run time through the use of the qemu guest agent. This is achieved through the access credentials api by using the qemuGuestAgent propagation method. Note: This requires the qemu guest agent to be installed within the guest Note: When using qemuGuestAgent propagation, the /home/$USER/.ssh/authorized_keys file will be owned by the guest agent. Changes to that file that are made outside of the qemu guest agent's control will get deleted. Note: More information about the motivation behind the access credentials api can be found in the pull request description that introduced this api. In the example below, a secret contains an ssh key. When attached to the VM via the access credential api with the qemuGuestAgent propagation method, the contents of the secret can be updated at any time which will automatically get applied to a running VM. The secret can contain multiple public keys. # Place ssh key into a secret kubectl create secret generic my-pub-key --from-file=key1=/id_rsa.pub Now reference this secret on the VM with the access credentials api using qemuGuestAgent propagation. This example installs and starts the qemu guest agent using a cloud-init script in order to ensure the agent is available. # Create a vm yaml that references the secret in using the access credentials api. cat << END > my-vm.yaml apiVersion: kubevirt.io/v1 kind: VirtualMachine metadata: labels: kubevirt.io/vm: my-vm name: my-vm spec: dataVolumeTemplates: - metadata: creationTimestamp: null name: fedora-dv spec: pvc: accessModes: - ReadWriteOnce resources: requests: storage: 5Gi storageClassName: local source: registry: url: docker://quay.io/kubevirt/fedora-cloud-container-disk-demo running: false template: metadata: labels: kubevirt.io/vm: my-vm spec: domain: devices: disks: - disk: bus: virtio name: disk0 - disk: bus: virtio name: disk1 machine: type: \"\" resources: requests: cpu: 1000m memory: 1G terminationGracePeriodSeconds: 0 accessCredentials: - sshPublicKey: source: secret: secretName: my-pub-key propagationMethod: qemuGuestAgent: users: - \"fedora\" volumes: - dataVolume: name: fedora-dv name: disk0 - cloudInitConfigDrive: userData: | #!/bin/bash sudo setenforce Permissive sudo yum install -y qemu-guest-agent sudo systemctl start qemu-guest-agent name: disk1 END kubectl create -f my-vm.yaml","title":"Dynamic SSH Key Injection via Qemu User Agent"},{"location":"virtual_machines/boot_from_external_source/","text":"Booting From External Source \u00b6 When installing a new guest virtual machine OS, it is often useful to boot directly from a kernel and initrd stored in the host physical machine OS, allowing command line arguments to be passed directly to the installer. Booting from an external source is supported in Kubevirt starting from version v0.42.0-rc.0 . This enables the capability to define a Virtual Machine that will use a custom kernel / initrd binary, with possible custom arguments, during its boot process. The binaries are provided though a container image. The container is pulled from the container registry and resides on the local node hosting the VMs. Use cases \u00b6 Some use cases for this may be: - For a kernel developer it may be very convenient to launch VMs that are defined to boot from the latest kernel binary that is often being changed. - Initrd can be set with files that need to reside on-memory during all the VM's life-cycle. Workflow \u00b6 Defining an external boot source can be done in the following way: apiVersion: kubevirt.io/v1 kind: VirtualMachine metadata: name: ext-kernel-boot-vm spec: runStrategy: Manual template: spec: domain: devices: {} firmware: kernelBoot: container: image: vmi_ext_boot/kernel_initrd_binaries_container:latest initrdPath: /boot/initramfs-virt kernelPath: /boot/vmlinuz-virt imagePullPolicy: Always imagePullSecret: IfNotPresent kernelArgs: console=ttyS0 resources: requests: memory: 1Gi Notes: initrdPath and kernelPath define the path for the binaries inside the container. Kernel and Initrd binaries must be owned by qemu user & group. To change ownership: chown qemu:qemu <binary> when <binary> is the binary file. kernelArgs can be provided even if a kernel binary is not provided (i.e. kernelPath not defined). These arguments will be passed to the default kernel the VM boots from. imagePullSecret and imagePullPolicy are optional if imagePullPolicy is Always and the container image is updated then the VM will be booted into the new kernel when VM restarts","title":"Booting From External Source"},{"location":"virtual_machines/boot_from_external_source/#booting-from-external-source","text":"When installing a new guest virtual machine OS, it is often useful to boot directly from a kernel and initrd stored in the host physical machine OS, allowing command line arguments to be passed directly to the installer. Booting from an external source is supported in Kubevirt starting from version v0.42.0-rc.0 . This enables the capability to define a Virtual Machine that will use a custom kernel / initrd binary, with possible custom arguments, during its boot process. The binaries are provided though a container image. The container is pulled from the container registry and resides on the local node hosting the VMs.","title":"Booting From External Source"},{"location":"virtual_machines/boot_from_external_source/#use-cases","text":"Some use cases for this may be: - For a kernel developer it may be very convenient to launch VMs that are defined to boot from the latest kernel binary that is often being changed. - Initrd can be set with files that need to reside on-memory during all the VM's life-cycle.","title":"Use cases"},{"location":"virtual_machines/boot_from_external_source/#workflow","text":"Defining an external boot source can be done in the following way: apiVersion: kubevirt.io/v1 kind: VirtualMachine metadata: name: ext-kernel-boot-vm spec: runStrategy: Manual template: spec: domain: devices: {} firmware: kernelBoot: container: image: vmi_ext_boot/kernel_initrd_binaries_container:latest initrdPath: /boot/initramfs-virt kernelPath: /boot/vmlinuz-virt imagePullPolicy: Always imagePullSecret: IfNotPresent kernelArgs: console=ttyS0 resources: requests: memory: 1Gi Notes: initrdPath and kernelPath define the path for the binaries inside the container. Kernel and Initrd binaries must be owned by qemu user & group. To change ownership: chown qemu:qemu <binary> when <binary> is the binary file. kernelArgs can be provided even if a kernel binary is not provided (i.e. kernelPath not defined). These arguments will be passed to the default kernel the VM boots from. imagePullSecret and imagePullPolicy are optional if imagePullPolicy is Always and the container image is updated then the VM will be booted into the new kernel when VM restarts","title":"Workflow"},{"location":"virtual_machines/dedicated_cpu_resources/","text":"Dedicated CPU resources \u00b6 Certain workloads, requiring a predictable latency and enhanced performance during its execution would benefit from obtaining dedicated CPU resources. KubeVirt, relying on the Kubernetes CPU manager, is able to pin guest's vCPUs to the host's pCPUs. Kubernetes CPU manager \u00b6 Kubernetes CPU manager is a mechanism that affects the scheduling of workloads, placing it on a host which can allocate Guaranteed resources and pin certain Pod's containers to host pCPUs, if the following requirements are met: Pod's QoS is Guaranteed resources requests and limits are equal all containers in the Pod express CPU and memory requirements Requested number of CPUs is an Integer Additional information: Enabling the CPU manager on Kubernetes Enabling the CPU manager on OKD Kubernetes blog explaining the feature Requesting dedicated CPU resources \u00b6 Setting spec.domain.cpu.dedicatedCpuPlacement to true in a VMI spec will indicate the desire to allocate dedicated CPU resource to the VMI Kubevirt will verify that all the necessary conditions are met, for the Kubernetes CPU manager to pin the virt-launcher container to dedicated host CPUs. Once, virt-launcher is running, the VMI's vCPUs will be pinned to the pCPUS that has been dedicated for the virt-launcher container. Expressing the desired amount of VMI's vCPUs can be done by either setting the guest topology in spec.domain.cpu ( sockets , cores , threads ) or spec.domain.resources.[requests/limits].cpu to a whole number integer ([1-9]+) indicating the number of vCPUs requested for the VMI. Number of vCPUs is counted as sockets * cores * threads or if spec.domain.cpu is empty then it takes value from spec.domain.resources.requests.cpu or spec.domain.resources.limits.cpu . Note: Users should not specify both spec.domain.cpu and spec.domain.resources.[requests/limits].cpu Note: spec.domain.resources.requests.cpu must be equal to spec.domain.resources.limits.cpu Note: Multiple cpu-bound microbenchmarks show a significant performance advantage when using spec.domain.cpu.sockets instead of spec.domain.cpu.cores . All inconsistent requirements will be rejected. apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance spec: domain: cpu: sockets: 2 cores: 1 threads: 1 dedicatedCpuPlacement: true resources: limits: memory: 2Gi [...] OR apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance spec: domain: cpu: dedicatedCpuPlacement: true resources: limits: cpu: 2 memory: 2Gi [...] Requesting dedicated CPU for QEMU emulator \u00b6 A number of QEMU threads, such as QEMU main event loop, async I/O operation completion, etc., also execute on the same physical CPUs as the VMI's vCPUs. This may affect the expected latency of a vCPU. In order to enhance the real-time support in KubeVirt and provide improved latency, KubeVirt will allocate an additional dedicated CPU, exclusively for the emulator thread, to which it will be pinned. This will effectively \"isolate\" the emulator thread from the vCPUs of the VMI. In case ioThreadsPolicy is set to auto IOThreads will also be \"isolated\" and placed on the same physical CPU as the QEMU emulator thread. This functionality can be enabled by specifying isolateEmulatorThread: true inside VMI spec's Spec.Domain.CPU section. Naturally, this setting has to be specified in a combination with a dedicatedCpuPlacement: true . Example: apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance spec: domain: cpu: dedicatedCpuPlacement: true isolateEmulatorThread: true resources: limits: cpu: 2 memory: 2Gi Identifying nodes with a running CPU manager \u00b6 At this time, Kubernetes doesn't label the nodes that has CPU manager running on it. KubeVirt has a mechanism to identify which nodes has the CPU manager running and manually add a cpumanager=true label. This label will be removed when KubeVirt will identify that CPU manager is no longer running on the node. This automatic identification should be viewed as a temporary workaround until Kubernetes will provide the required functionality. Therefore, this feature should be manually enabled by activating the CPUManager feature gate to the KubeVirt CR. When automatic identification is disabled, cluster administrator may manually add the above label to all the nodes when CPU Manager is running. Nodes' labels are view-able: kubectl describe nodes Administrators may manually label a missing node: kubectl label node [node_name] cpumanager=true Sidecar containers and CPU allocation overhead \u00b6 Note: In order to run sidecar containers, KubeVirt requires the Sidecar feature gate to be enabled in KubeVirt's CR. According to the Kubernetes CPU manager model, in order the POD would reach the required QOS level Guaranteed , all containers in the POD must express CPU and memory requirements. At this time, Kubevirt often uses a sidecar container to mount VMI's registry disk. It also uses a sidecar container of it's hooking mechanism. These additional resources can be viewed as an overhead and should be taken into account when calculating a node capacity. Note: The current defaults for sidecar's resources: CPU: 200m Memory: 64M As the CPU resource is not expressed as a whole number, CPU manager will not attempt to pin the sidecar container to a host CPU.","title":"Dedicated CPU resources"},{"location":"virtual_machines/dedicated_cpu_resources/#dedicated-cpu-resources","text":"Certain workloads, requiring a predictable latency and enhanced performance during its execution would benefit from obtaining dedicated CPU resources. KubeVirt, relying on the Kubernetes CPU manager, is able to pin guest's vCPUs to the host's pCPUs.","title":"Dedicated CPU resources"},{"location":"virtual_machines/dedicated_cpu_resources/#kubernetes-cpu-manager","text":"Kubernetes CPU manager is a mechanism that affects the scheduling of workloads, placing it on a host which can allocate Guaranteed resources and pin certain Pod's containers to host pCPUs, if the following requirements are met: Pod's QoS is Guaranteed resources requests and limits are equal all containers in the Pod express CPU and memory requirements Requested number of CPUs is an Integer Additional information: Enabling the CPU manager on Kubernetes Enabling the CPU manager on OKD Kubernetes blog explaining the feature","title":"Kubernetes CPU manager"},{"location":"virtual_machines/dedicated_cpu_resources/#requesting-dedicated-cpu-resources","text":"Setting spec.domain.cpu.dedicatedCpuPlacement to true in a VMI spec will indicate the desire to allocate dedicated CPU resource to the VMI Kubevirt will verify that all the necessary conditions are met, for the Kubernetes CPU manager to pin the virt-launcher container to dedicated host CPUs. Once, virt-launcher is running, the VMI's vCPUs will be pinned to the pCPUS that has been dedicated for the virt-launcher container. Expressing the desired amount of VMI's vCPUs can be done by either setting the guest topology in spec.domain.cpu ( sockets , cores , threads ) or spec.domain.resources.[requests/limits].cpu to a whole number integer ([1-9]+) indicating the number of vCPUs requested for the VMI. Number of vCPUs is counted as sockets * cores * threads or if spec.domain.cpu is empty then it takes value from spec.domain.resources.requests.cpu or spec.domain.resources.limits.cpu . Note: Users should not specify both spec.domain.cpu and spec.domain.resources.[requests/limits].cpu Note: spec.domain.resources.requests.cpu must be equal to spec.domain.resources.limits.cpu Note: Multiple cpu-bound microbenchmarks show a significant performance advantage when using spec.domain.cpu.sockets instead of spec.domain.cpu.cores . All inconsistent requirements will be rejected. apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance spec: domain: cpu: sockets: 2 cores: 1 threads: 1 dedicatedCpuPlacement: true resources: limits: memory: 2Gi [...] OR apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance spec: domain: cpu: dedicatedCpuPlacement: true resources: limits: cpu: 2 memory: 2Gi [...]","title":"Requesting dedicated CPU resources"},{"location":"virtual_machines/dedicated_cpu_resources/#requesting-dedicated-cpu-for-qemu-emulator","text":"A number of QEMU threads, such as QEMU main event loop, async I/O operation completion, etc., also execute on the same physical CPUs as the VMI's vCPUs. This may affect the expected latency of a vCPU. In order to enhance the real-time support in KubeVirt and provide improved latency, KubeVirt will allocate an additional dedicated CPU, exclusively for the emulator thread, to which it will be pinned. This will effectively \"isolate\" the emulator thread from the vCPUs of the VMI. In case ioThreadsPolicy is set to auto IOThreads will also be \"isolated\" and placed on the same physical CPU as the QEMU emulator thread. This functionality can be enabled by specifying isolateEmulatorThread: true inside VMI spec's Spec.Domain.CPU section. Naturally, this setting has to be specified in a combination with a dedicatedCpuPlacement: true . Example: apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance spec: domain: cpu: dedicatedCpuPlacement: true isolateEmulatorThread: true resources: limits: cpu: 2 memory: 2Gi","title":"Requesting dedicated CPU for QEMU emulator"},{"location":"virtual_machines/dedicated_cpu_resources/#identifying-nodes-with-a-running-cpu-manager","text":"At this time, Kubernetes doesn't label the nodes that has CPU manager running on it. KubeVirt has a mechanism to identify which nodes has the CPU manager running and manually add a cpumanager=true label. This label will be removed when KubeVirt will identify that CPU manager is no longer running on the node. This automatic identification should be viewed as a temporary workaround until Kubernetes will provide the required functionality. Therefore, this feature should be manually enabled by activating the CPUManager feature gate to the KubeVirt CR. When automatic identification is disabled, cluster administrator may manually add the above label to all the nodes when CPU Manager is running. Nodes' labels are view-able: kubectl describe nodes Administrators may manually label a missing node: kubectl label node [node_name] cpumanager=true","title":"Identifying nodes with a running CPU manager"},{"location":"virtual_machines/dedicated_cpu_resources/#sidecar-containers-and-cpu-allocation-overhead","text":"Note: In order to run sidecar containers, KubeVirt requires the Sidecar feature gate to be enabled in KubeVirt's CR. According to the Kubernetes CPU manager model, in order the POD would reach the required QOS level Guaranteed , all containers in the POD must express CPU and memory requirements. At this time, Kubevirt often uses a sidecar container to mount VMI's registry disk. It also uses a sidecar container of it's hooking mechanism. These additional resources can be viewed as an overhead and should be taken into account when calculating a node capacity. Note: The current defaults for sidecar's resources: CPU: 200m Memory: 64M As the CPU resource is not expressed as a whole number, CPU manager will not attempt to pin the sidecar container to a host CPU.","title":"Sidecar containers and CPU allocation overhead"},{"location":"virtual_machines/disks_and_volumes/","text":"Disks and Volumes \u00b6 Making persistent storage in the cluster ( volumes ) accessible to VMs consists of three parts. First, volumes are specified in spec.volumes . Second, disks are added to the VM by specifying them in spec.domain.devices.disks . Finally, a reference to the specified volume is added to the disk specification by name. Disks \u00b6 Like all other vmi devices a spec.domain.devices.disks element has a mandatory name , and furthermore, the disk's name must reference the name of a volume inside spec.volumes . A disk can be made accessible via four different types: lun disk cdrom floppy DEPRECATED All possible configuration options are available in the Disk API Reference . All types, with the exception of floppy , allow you to specify the bus attribute. The bus attribute determines how the disk will be presented to the guest operating system. floppy disks don't support the bus attribute: they are always attached to the fdc bus. lun \u00b6 A lun disk will expose the volume as a LUN device to the VM. This allows the VM to execute arbitrary iSCSI command passthrough. A minimal example which attaches a PersistentVolumeClaim named mypvc as a lun device to the VM: metadata: name: testvmi-lun apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance spec: domain: resources: requests: memory: 64M devices: disks: - name: mypvcdisk # This makes it a lun device lun: {} volumes: - name: mypvcdisk persistentVolumeClaim: claimName: mypvc disk \u00b6 A disk disk will expose the volume as an ordinary disk to the VM. A minimal example which attaches a PersistentVolumeClaim named mypvc as a disk device to the VM: metadata: name: testvmi-disk apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance spec: domain: resources: requests: memory: 64M devices: disks: - name: mypvcdisk # This makes it a disk disk: {} volumes: - name: mypvcdisk persistentVolumeClaim: claimName: mypvc You can set the disk bus type, overriding the defaults, which in turn depends on the chipset the VM is configured to use: metadata: name: testvmi-disk apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance spec: domain: resources: requests: memory: 64M devices: disks: - name: mypvcdisk # This makes it a disk disk: # This makes it exposed as /dev/vda, being the only and thus first # disk attached to the VM bus: virtio volumes: - name: mypvcdisk persistentVolumeClaim: claimName: mypvc floppy \u00b6 Note : Starting with version 0.16.0, floppy disks are deprecated and will be rejected . They will be removed from the API in a future version. A floppy disk will expose the volume as a floppy drive to the VM. A minimal example which attaches a PersistentVolumeClaim named mypvc as a floppy device to the VM: metadata: name: testvmi-floppy apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance spec: domain: resources: requests: memory: 64M devices: disks: - name: mypvcdisk # This makes it a floppy floppy: {} volumes: - name: mypvcdisk persistentVolumeClaim: claimName: mypvc cdrom \u00b6 A cdrom disk will expose the volume as a cdrom drive to the VM. It is read-only by default. A minimal example which attaches a PersistentVolumeClaim named mypvc as a floppy device to the VM: metadata: name: testvmi-cdrom apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance spec: domain: resources: requests: memory: 64M devices: disks: - name: mypvcdisk # This makes it a cdrom cdrom: # This makes the cdrom writeable readOnly: false # This makes the cdrom be exposed as SATA device bus: sata volumes: - name: mypvcdisk persistentVolumeClaim: claimName: mypvc Volumes \u00b6 Supported volume sources are cloudInitNoCloud cloudInitConfigDrive persistentVolumeClaim dataVolume ephemeral containerDisk emptyDisk hostDisk configMap secret serviceAccount downwardMetrics All possible configuration options are available in the Volume API Reference . cloudInitNoCloud \u00b6 Allows attaching cloudInitNoCloud data-sources to the VM. If the VM contains a proper cloud-init setup, it will pick up the disk as a user-data source. A simple example which attaches a Secret as a cloud-init disk datasource may look like this: metadata: name: testvmi-cloudinitnocloud apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance spec: domain: resources: requests: memory: 64M devices: disks: - name: mybootdisk lun: {} - name: mynoclouddisk disk: {} volumes: - name: mybootdisk persistentVolumeClaim: claimName: mypvc - name: mynoclouddisk cloudInitNoCloud: secretRef: name: testsecret cloudInitConfigDrive \u00b6 Allows attaching cloudInitConfigDrive data-sources to the VM. If the VM contains a proper cloud-init setup, it will pick up the disk as a user-data source. A simple example which attaches a Secret as a cloud-init disk datasource may look like this: metadata: name: testvmi-cloudinitconfigdrive apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance spec: domain: resources: requests: memory: 64M devices: disks: - name: mybootdisk lun: {} - name: myconfigdrivedisk disk: {} volumes: - name: mybootdisk persistentVolumeClaim: claimName: mypvc - name: myconfigdrivedisk cloudInitConfigDrive: secretRef: name: testsecret persistentVolumeClaim \u00b6 Allows connecting a PersistentVolumeClaim to a VM disk. Use a PersistentVolumeClaim when the VirtualMachineInstance's disk needs to persist after the VM terminates. This allows for the VM's data to remain persistent between restarts. A PersistentVolume can be in \"filesystem\" or \"block\" mode: Filesystem: For KubeVirt to be able to consume the disk present on a PersistentVolume's filesystem, the disk must be named disk.img and be placed in the root path of the filesystem. Currently the disk is also required to be in raw format. > Important: The disk.img image file needs to be owned by the user-id 107 in order to avoid permission issues. Note: If the disk.img image file has not been created manually before starting a VM then it will be created automatically with the PersistentVolumeClaim size. Since not every storage provisioner provides volumes with the exact usable amount of space as requested (e.g. due to filesystem overhead), KubeVirt tolerates up to 10% less available space. This can be configured with the pvc-tolerate-less-space-up-to-percent value in the kubevirt-config ConfigMap. Block: Use a block volume for consuming raw block devices. Note: you need to enable the BlockVolume feature gate . A simple example which attaches a PersistentVolumeClaim as a disk may look like this: metadata: name: testvmi-pvc apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance spec: domain: resources: requests: memory: 64M devices: disks: - name: mypvcdisk lun: {} volumes: - name: mypvcdisk persistentVolumeClaim: claimName: mypvc dataVolume \u00b6 DataVolumes are a way to automate importing virtual machine disks onto PVCs during the virtual machine's launch flow. Without using a DataVolume, users have to prepare a PVC with a disk image before assigning it to a VM or VMI manifest. With a DataVolume, both the PVC creation and import is automated on behalf of the user. DataVolume VM Behavior \u00b6 DataVolumes can be defined in the VM spec directly by adding the DataVolumes to the dataVolumeTemplates list. Below is an example. apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachine metadata: labels: kubevirt.io/vm: vm-alpine-datavolume name: vm-alpine-datavolume spec: running: false template: metadata: labels: kubevirt.io/vm: vm-alpine-datavolume spec: domain: devices: disks: - disk: bus: virtio name: datavolumedisk1 resources: requests: memory: 64M volumes: - dataVolume: name: alpine-dv name: datavolumedisk1 dataVolumeTemplates: - metadata: name: alpine-dv spec: pvc: accessModes: - ReadWriteOnce resources: requests: storage: 2Gi source: http: url: http://cdi-http-import-server.kubevirt/images/alpine.iso You can see the DataVolume defined in the dataVolumeTemplates section has two parts. The source and pvc The source part declares that there is a disk image living on an http server that we want to use as a volume for this VM. The pvc part declares the spec that should be used to create the PVC that hosts the source data. When this VM manifest is posted to the cluster, as part of the launch flow a PVC will be created using the spec provided and the source data will be automatically imported into that PVC before the VM starts. When the VM is deleted, the storage provisioned by the DataVolume will automatically be deleted as well. DataVolume VMI Behavior \u00b6 For a VMI object, DataVolumes can be referenced as a volume source for the VMI. When this is done, it is expected that the referenced DataVolume exists in the cluster. The VMI will consume the DataVolume, but the DataVolume's life-cycle will not be tied to the VMI. Below is an example of a DataVolume being referenced by a VMI. It is expected that the DataVolume alpine-datavolume was created prior to posting the VMI manifest to the cluster. It is okay to post the VMI manifest to the cluster while the DataVolume is still having data imported. KubeVirt knows not to start the VMI until all referenced DataVolumes have finished their clone and import phases. apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance metadata: labels: special: vmi-alpine-datavolume name: vmi-alpine-datavolume spec: domain: devices: disks: - disk: bus: virtio name: disk1 machine: type: \"\" resources: requests: memory: 64M terminationGracePeriodSeconds: 0 volumes: - name: disk1 dataVolume: name: alpine-datavolume Enabling DataVolume support. \u00b6 A DataVolume is a custom resource provided by the Containerized Data Importer (CDI) project. KubeVirt integrates with CDI in order to provide users a workflow for dynamically creating PVCs and importing data into those PVCs. In order to take advantage of the DataVolume volume source on a VM or VMI, the DataVolumes feature gate must be enabled in the kubevirt CR. CDI must also be installed. Installing CDI Go to the CDI release page Pick the latest stable release and post the corresponding cdi-controller-deployment.yaml manifest to your cluster. ephemeral \u00b6 An ephemeral volume is a local COW (copy on write) image that uses a network volume as a read-only backing store. With an ephemeral volume, the network backing store is never mutated. Instead all writes are stored on the ephemeral image which exists on local storage. KubeVirt dynamically generates the ephemeral images associated with a VM when the VM starts, and discards the ephemeral images when the VM stops. Ephemeral volumes are useful in any scenario where disk persistence is not desired. The COW image is discarded when VM reaches a final state (e.g., succeeded, failed). Currently, only PersistentVolumeClaim may be used as a backing store of the ephemeral volume. Up-to-date information on supported backing stores can be found in the KubeVirt API . metadata: name: testvmi-ephemeral-pvc apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance spec: domain: resources: requests: memory: 64M devices: disks: - name: mypvcdisk lun: {} volumes: - name: mypvcdisk ephemeral: persistentVolumeClaim: claimName: mypvc containerDisk \u00b6 containerDisk was originally registryDisk, please update your code when needed. The containerDisk feature provides the ability to store and distribute VM disks in the container image registry. containerDisks can be assigned to VMs in the disks section of the VirtualMachineInstance spec. No network shared storage devices are utilized by containerDisks . The disks are pulled from the container registry and reside on the local node hosting the VMs that consume the disks. When to use a containerDisk \u00b6 containerDisks are ephemeral storage devices that can be assigned to any number of active VirtualMachineInstances. This makes them an ideal tool for users who want to replicate a large number of VM workloads that do not require persistent data. containerDisks are commonly used in conjunction with VirtualMachineInstanceReplicaSets. When Not to use a containerDisk \u00b6 containerDisks are not a good solution for any workload that requires persistent root disks across VM restarts. containerDisk Workflow Example \u00b6 Users can inject a VirtualMachineInstance disk into a container image in a way that is consumable by the KubeVirt runtime. Disks must be placed into the /disk directory inside the container. Raw and qcow2 formats are supported. Qcow2 is recommended in order to reduce the container image's size. containerdisks can and should be based on scratch . No content except the image is required. Note: Prior to kubevirt 0.20, the containerDisk image needed to have kubevirt/container-disk-v1alpha as base image. Note: The containerDisk needs to be readable for the user with the UID 107 (qemu). Example: Inject a local VirtualMachineInstance disk into a container image. cat << END > Dockerfile FROM scratch ADD --chown=107:107 fedora25.qcow2 /disk/ END docker build -t vmidisks/fedora25:latest . Example: Inject a remote VirtualMachineInstance disk into a container image. cat << END > Dockerfile FROM scratch ADD --chown=107:107 https://cloud.centos.org/centos/7/images/CentOS-7-x86_64-GenericCloud.qcow2 /disk/ END Example: Upload the ContainerDisk container image to a registry. docker push vmidisks/fedora25:latest Example: Attach the ContainerDisk as an ephemeral disk to a VM. metadata: name: testvmi-containerdisk apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance spec: domain: resources: requests: memory: 64M devices: disks: - name: containerdisk disk: {} volumes: - name: containerdisk containerDisk: image: vmidisks/fedora25:latest Note that a containerDisk is file-based and therefore cannot be attached as a lun device to the VM. Custom disk image path \u00b6 ContainerDisk also allows to store disk images in any folder, when required. The process is the same as previous. The main difference is, that in custom location, kubevirt does not scan for any image. It is your responsibility to provide full path for the disk image. Providing image path is optional. When no path is provided, kubevirt searches for disk images in default location: /disk . Example: Build container disk image: cat << END > Dockerfile FROM scratch ADD fedora25.qcow2 /custom-disk-path/fedora25.qcow2 END docker build -t vmidisks/fedora25:latest . docker push vmidisks/fedora25:latest Create VMI with container disk pointing to the custom location: metadata: name: testvmi-containerdisk apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance spec: domain: resources: requests: memory: 64M devices: disks: - name: containerdisk disk: {} volumes: - name: containerdisk containerDisk: image: vmidisks/fedora25:latest path: /custom-disk-path/fedora25.qcow2 emptyDisk \u00b6 An emptyDisk works similar to an emptyDir in Kubernetes. An extra sparse qcow2 disk will be allocated and it will live as long as the VM. Thus it will survive guest side VM reboots, but not a VM re-creation. The disk capacity needs to be specified. Example: Boot cirros with an extra emptyDisk with a size of 2GiB : apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance metadata: name: testvmi-nocloud spec: terminationGracePeriodSeconds: 5 domain: resources: requests: memory: 64M devices: disks: - name: containerdisk disk: bus: virtio - name: emptydisk disk: bus: virtio volumes: - name: containerdisk containerDisk: image: kubevirt/cirros-registry-disk-demo:latest - name: emptydisk emptyDisk: capacity: 2Gi When to use an emptyDisk \u00b6 Ephemeral VMs very often come with read-only root images and limited tmpfs space. In many cases this is not enough to install application dependencies and provide enough disk space for the application data. While this data is not critical and thus can be lost, it is still needed for the application to function properly during its lifetime. This is where an emptyDisk can be useful. An emptyDisk is often used and mounted somewhere in /var/lib or /var/run . hostDisk \u00b6 A hostDisk volume type provides the ability to create or use a disk image located somewhere on a node. It works similar to a hostPath in Kubernetes and provides two usage types: DiskOrCreate if a disk image does not exist at a given location then create one Disk a disk image must exist at a given location Note: you need to enable the HostDisk feature gate. Example: Create a 1Gi disk image located at /data/disk.img and attach it to a VM. apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance metadata: labels: special: vmi-host-disk name: vmi-host-disk spec: domain: devices: disks: - disk: bus: virtio name: host-disk machine: type: \"\" resources: requests: memory: 64M terminationGracePeriodSeconds: 0 volumes: - hostDisk: capacity: 1Gi path: /data/disk.img type: DiskOrCreate name: host-disk status: {} configMap \u00b6 A configMap is a reference to a ConfigMap in Kubernetes. An extra iso disk will be allocated which has to be mounted on a VM. To mount the configMap users can use cloudInit and the disks serial number. The name needs to be set for a reference to the created kubernetes ConfigMap . Note: Currently, ConfigMap update is not propagate into the VMI. If a ConfigMap is updated, only a pod will be aware of changes, not running VMIs. Note: Due to a Kubernetes CRD issue , you cannot control the paths within the volume where ConfigMap keys are projected. Example: Attach the configMap to a VM and use cloudInit to mount the iso disk: apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance metadata: labels: special: vmi-fedora name: vmi-fedora spec: domain: devices: disks: - disk: bus: virtio name: containerdisk - disk: bus: virtio name: cloudinitdisk - disk: {} name: app-config-disk # set serial serial: CVLY623300HK240D machine: type: \"\" resources: requests: memory: 1024M terminationGracePeriodSeconds: 0 volumes: - name: containerdisk containerDisk: image: kubevirt/fedora-cloud-container-disk-demo:latest - cloudInitNoCloud: userData: |- #cloud-config password: fedora chpasswd: { expire: False } bootcmd: # mount the ConfigMap - \"mkdir /mnt/app-config\" - \"mount /dev/$(lsblk --nodeps -no name,serial | grep CVLY623300HK240D | cut -f1 -d' ') /mnt/app-config\" name: cloudinitdisk - configMap: name: app-config name: app-config-disk status: {} secret \u00b6 A secret is a reference to a Secret in Kubernetes. An extra iso disk will be allocated which has to be mounted on a VM. To mount the secret users can use cloudInit and the disks serial number. The secretName needs to be set for a reference to the created kubernetes Secret . Note: Currently, Secret update propagation is not supported. If a Secret is updated, only a pod will be aware of changes, not running VMIs. Note: Due to a Kubernetes CRD issue , you cannot control the paths within the volume where Secret keys are projected. Example: Attach the secret to a VM and use cloudInit to mount the iso disk: apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance metadata: labels: special: vmi-fedora name: vmi-fedora spec: domain: devices: disks: - disk: bus: virtio name: containerdisk - disk: bus: virtio name: cloudinitdisk - disk: {} name: app-secret-disk # set serial serial: D23YZ9W6WA5DJ487 machine: type: \"\" resources: requests: memory: 1024M terminationGracePeriodSeconds: 0 volumes: - name: containerdisk containerDisk: image: kubevirt/fedora-cloud-container-disk-demo:latest - cloudInitNoCloud: userData: |- #cloud-config password: fedora chpasswd: { expire: False } bootcmd: # mount the Secret - \"mkdir /mnt/app-secret\" - \"mount /dev/$(lsblk --nodeps -no name,serial | grep D23YZ9W6WA5DJ487 | cut -f1 -d' ') /mnt/app-secret\" name: cloudinitdisk - secret: secretName: app-secret name: app-secret-disk status: {} serviceAccount \u00b6 A serviceAccount volume references a Kubernetes ServiceAccount . A new iso disk will be allocated with the content of the service account ( namespace , token and ca.crt ), which needs to be mounted in the VM. For automatic mounting, see the configMap and secret examples above. Example: apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance metadata: labels: special: vmi-fedora name: vmi-fedora spec: domain: devices: disks: - disk: bus: virtio name: containerdisk - disk: bus: virtio name: serviceaccountdisk machine: type: \"\" resources: requests: memory: 1024M terminationGracePeriodSeconds: 0 volumes: - name: containerdisk containerDisk: image: kubevirt/fedora-cloud-container-disk-demo:latest - name: serviceaccountdisk serviceAccount: serviceAccountName: default downwardMetrics \u00b6 A downwardMetrics volume exposes a limited set of VM and host metrics to the guest as a raw block volume. The format of the block volume is compatible with vhostmd . Getting a limited set of host and VM metrics is in some cases required to allow third-parties diagnosing performance issues on their appliances. One prominent example is SAP HANA. Example: apiVersion: kubevirt.io/v1 kind: VirtualMachineInstance metadata: labels: special: vmi-fedora name: vmi-fedora spec: domain: devices: disks: - disk: bus: virtio name: containerdisk - disk: bus: virtio name: metrics machine: type: \"\" resources: requests: memory: 1024M terminationGracePeriodSeconds: 0 volumes: - name: containerdisk containerDisk: image: kubevirt/fedora-cloud-container-disk-demo:latest - name: metrics downwardMetrics: {} The vm-dump-metrics tool can be used to read the metrics: $ dnf install -y vm-dump-metrics $ vm-dump-metrics <metrics> <metric type=\"string\" context=\"host\"> <name>HostName</name> <value>node01</value> [...] <metric type=\"int64\" context=\"host\" unit=\"s\"> <name>Time</name> <value>1619008605</value> </metric> <metric type=\"string\" context=\"host\"> <name>VirtualizationVendor</name> <value>kubevirt.io</value> </metric> </metrics> Note: The DownwardMetrics feature gate must be enabled to use this volume. Available starting with KubeVirt v0.42.0. High Performance Features \u00b6 IOThreads \u00b6 Libvirt has the ability to use IOThreads for dedicated disk access (for supported devices). These are dedicated event loop threads that perform block I/O requests and improve scalability on SMP systems. KubeVirt exposes this libvirt feature through the ioThreadsPolicy setting. Additionally, each Disk device exposes a dedicatedIOThread setting. This is a boolean that indicates the specified disk should be allocated an exclusive IOThread that will never be shared with other disks. Currently valid policies are shared and auto . If ioThreadsPolicy is omitted entirely, use of IOThreads will be disabled. However, if any disk requests a dedicated IOThread, ioThreadsPolicy will be enabled and default to shared . Shared \u00b6 An ioThreadsPolicy of shared indicates that KubeVirt should use one thread that will be shared by all disk devices. This policy stems from the fact that large numbers of IOThreads is generally not useful as additional context switching is incurred for each thread. Disks with dedicatedIOThread set to true will not use the shared thread, but will instead be allocated an exclusive thread. This is generally useful if a specific Disk is expected to have heavy I/O traffic, e.g. a database spindle. Auto \u00b6 auto IOThreads indicates that KubeVirt should use a pool of IOThreads and allocate disks to IOThreads in a round-robin fashion. The pool size is generally limited to twice the number of VCPU's allocated to the VM. This essentially attempts to dedicate disks to separate IOThreads, but only up to a reasonable limit. This would come in to play for systems with a large number of disks and a smaller number of CPU's for instance. As a caveat to the size of the IOThread pool, disks with dedicatedIOThread will always be guaranteed their own thread. This effectively diminishes the upper limit of the number of threads allocated to the rest of the disks. For example, a VM with 2 CPUs would normally use 4 IOThreads for all disks. However if one disk had dedicatedIOThread set to true, then KubeVirt would only use 3 IOThreads for the shared pool. There is always guaranteed to be at least one thread for disks that will use the shared IOThreads pool. Thus if a sufficiently large number of disks have dedicated IOThreads assigned, auto and shared policies would essentially result in the same layout. IOThreads with Dedicated (pinned) CPUs \u00b6 When guest's vCPUs are pinned to a host's physical CPUs, it is also best to pin the IOThreads to specific CPUs to prevent these from floating between the CPUs. KubeVirt will automatically calculate and pin each IOThread to a CPU or a set of CPUs, depending on the ration between them. In case there are more IOThreads than CPUs, each IOThread will be pinned to a CPU, in a round-robin fashion. Otherwise, when there are fewer IOThreads than CPU, each IOThread will be pinned to a set of CPUs. IOThreads with QEMU Emulator thread and Dedicated (pinned) CPUs \u00b6 To further improve the vCPUs latency, KubeVirt can allocate an additional dedicated physical CPU 1 , exclusively for the emulator thread, to which it will be pinned. This will effectively \"isolate\" the emulator thread from the vCPUs of the VMI. When ioThreadsPolicy is set to auto IOThreads will also be \"isolated\" from the vCPUs and placed on the same physical CPU as the QEMU emulator thread. Examples \u00b6 Shared IOThreads \u00b6 apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance metadata: labels: special: vmi-shared name: vmi-shared spec: domain: ioThreadsPolicy: shared cpu: cores: 2 devices: disks: - disk: bus: virtio name: vmi-shared_disk - disk: bus: virtio name: emptydisk dedicatedIOThread: true - disk: bus: virtio name: emptydisk2 dedicatedIOThread: true - disk: bus: virtio name: emptydisk3 - disk: bus: virtio name: emptydisk4 - disk: bus: virtio name: emptydisk5 - disk: bus: virtio name: emptydisk6 machine: type: \"\" resources: requests: memory: 64M volumes: - name: vmi-shared_disk persistentVolumeClaim: claimName: vmi-shared_pvc - emptyDisk: capacity: 1Gi name: emptydisk - emptyDisk: capacity: 1Gi name: emptydisk2 - emptyDisk: capacity: 1Gi name: emptydisk3 - emptyDisk: capacity: 1Gi name: emptydisk4 - emptyDisk: capacity: 1Gi name: emptydisk5 - emptyDisk: capacity: 1Gi name: emptydisk6 In this example, emptydisk and emptydisk2 both request a dedicated IOThread. vmi-shared_disk, and emptydisk 3 through 6 will all shared one IOThread. mypvc: 1 emptydisk: 2 emptydisk2: 3 emptydisk3: 1 emptydisk4: 1 emptydisk5: 1 emptydisk6: 1 Auto IOThreads \u00b6 apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance metadata: labels: special: vmi-shared name: vmi-shared spec: domain: ioThreadsPolicy: auto cpu: cores: 2 devices: disks: - disk: bus: virtio name: mydisk - disk: bus: virtio name: emptydisk dedicatedIOThread: true - disk: bus: virtio name: emptydisk2 dedicatedIOThread: true - disk: bus: virtio name: emptydisk3 - disk: bus: virtio name: emptydisk4 - disk: bus: virtio name: emptydisk5 - disk: bus: virtio name: emptydisk6 machine: type: \"\" resources: requests: memory: 64M volumes: - name: mydisk persistentVolumeClaim: claimName: mypvc - emptyDisk: capacity: 1Gi name: emptydisk - emptyDisk: capacity: 1Gi name: emptydisk2 - emptyDisk: capacity: 1Gi name: emptydisk3 - emptyDisk: capacity: 1Gi name: emptydisk4 - emptyDisk: capacity: 1Gi name: emptydisk5 - emptyDisk: capacity: 1Gi name: emptydisk6 This VM is identical to the first, except it requests auto IOThreads. emptydisk and emptydisk2 will still be allocated individual IOThreads, but the rest of the disks will be split across 2 separate iothreads (twice the number of CPU cores is 4). Disks will be assigned to IOThreads like this: mypvc: 1 emptydisk: 3 emptydisk2: 4 emptydisk3: 2 emptydisk4: 1 emptydisk5: 2 emptydisk6: 1 Virtio Block Multi-Queue \u00b6 Block Multi-Queue is a framework for the Linux block layer that maps Device I/O queries to multiple queues. This splits I/O processing up across multiple threads, and therefor multiple CPUs. libvirt recommends that the number of queues used should match the number of CPUs allocated for optimal performance. This feature is enabled by the BlockMultiQueue setting under Devices : spec: domain: devices: blockMultiQueue: true disks: - disk: bus: virtio name: mydisk Note: Due to the way KubeVirt implements CPU allocation, blockMultiQueue can only be used if a specific CPU allocation is requested. If a specific number of CPUs hasn't been allocated to a VirtualMachine, KubeVirt will use all CPU's on the node on a best effort basis. In that case the amount of CPU allocation to a VM at the host level could change over time. If blockMultiQueue were to request a number of queues to match all the CPUs on a node, that could lead to over-allocation scenarios. To avoid this, KubeVirt enforces that a specific slice of CPU resources is requested in order to take advantage of this feature. Example \u00b6 metadata: name: testvmi-disk apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance spec: domain: resources: requests: memory: 64M cpu: 4 devices: blockMultiQueue: true disks: - name: mypvcdisk disk: bus: virtio volumes: - name: mypvcdisk persistentVolumeClaim: claimName: mypvc This example will enable Block Multi-Queue for the disk mypvcdisk and allocate 4 queues (to match the number of CPUs requested). Disk device cache \u00b6 KubeVirt supports none and writethrough KVM/QEMU cache modes. none I/O from the guest is not cached on the host. Use this option for guests with large I/O requirements. This option is generally the best choice. writethrough I/O from the guest is cached on the host but written through to the physical medium. Important: none cache mode is set as default if the file system supports direct I/O, otherwise, writethrough is used. Note: It is possible to force a specific cache mode, although if none mode has been chosen and the file system does not support direct I/O then started VMI will return an error. Example: force writethrough cache mode apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance metadata: labels: special: vmi-pvc name: vmi-pvc spec: domain: devices: disks: - disk: bus: virtio name: pvcdisk cache: writethrough machine: type: \"\" resources: requests: memory: 64M terminationGracePeriodSeconds: 0 volumes: - name: pvcdisk persistentVolumeClaim: claimName: disk-alpine status: {}","title":"Disks and Volumes"},{"location":"virtual_machines/disks_and_volumes/#disks-and-volumes","text":"Making persistent storage in the cluster ( volumes ) accessible to VMs consists of three parts. First, volumes are specified in spec.volumes . Second, disks are added to the VM by specifying them in spec.domain.devices.disks . Finally, a reference to the specified volume is added to the disk specification by name.","title":"Disks and Volumes"},{"location":"virtual_machines/disks_and_volumes/#disks","text":"Like all other vmi devices a spec.domain.devices.disks element has a mandatory name , and furthermore, the disk's name must reference the name of a volume inside spec.volumes . A disk can be made accessible via four different types: lun disk cdrom floppy DEPRECATED All possible configuration options are available in the Disk API Reference . All types, with the exception of floppy , allow you to specify the bus attribute. The bus attribute determines how the disk will be presented to the guest operating system. floppy disks don't support the bus attribute: they are always attached to the fdc bus.","title":"Disks"},{"location":"virtual_machines/disks_and_volumes/#lun","text":"A lun disk will expose the volume as a LUN device to the VM. This allows the VM to execute arbitrary iSCSI command passthrough. A minimal example which attaches a PersistentVolumeClaim named mypvc as a lun device to the VM: metadata: name: testvmi-lun apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance spec: domain: resources: requests: memory: 64M devices: disks: - name: mypvcdisk # This makes it a lun device lun: {} volumes: - name: mypvcdisk persistentVolumeClaim: claimName: mypvc","title":"lun"},{"location":"virtual_machines/disks_and_volumes/#disk","text":"A disk disk will expose the volume as an ordinary disk to the VM. A minimal example which attaches a PersistentVolumeClaim named mypvc as a disk device to the VM: metadata: name: testvmi-disk apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance spec: domain: resources: requests: memory: 64M devices: disks: - name: mypvcdisk # This makes it a disk disk: {} volumes: - name: mypvcdisk persistentVolumeClaim: claimName: mypvc You can set the disk bus type, overriding the defaults, which in turn depends on the chipset the VM is configured to use: metadata: name: testvmi-disk apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance spec: domain: resources: requests: memory: 64M devices: disks: - name: mypvcdisk # This makes it a disk disk: # This makes it exposed as /dev/vda, being the only and thus first # disk attached to the VM bus: virtio volumes: - name: mypvcdisk persistentVolumeClaim: claimName: mypvc","title":"disk"},{"location":"virtual_machines/disks_and_volumes/#floppy","text":"Note : Starting with version 0.16.0, floppy disks are deprecated and will be rejected . They will be removed from the API in a future version. A floppy disk will expose the volume as a floppy drive to the VM. A minimal example which attaches a PersistentVolumeClaim named mypvc as a floppy device to the VM: metadata: name: testvmi-floppy apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance spec: domain: resources: requests: memory: 64M devices: disks: - name: mypvcdisk # This makes it a floppy floppy: {} volumes: - name: mypvcdisk persistentVolumeClaim: claimName: mypvc","title":"floppy"},{"location":"virtual_machines/disks_and_volumes/#cdrom","text":"A cdrom disk will expose the volume as a cdrom drive to the VM. It is read-only by default. A minimal example which attaches a PersistentVolumeClaim named mypvc as a floppy device to the VM: metadata: name: testvmi-cdrom apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance spec: domain: resources: requests: memory: 64M devices: disks: - name: mypvcdisk # This makes it a cdrom cdrom: # This makes the cdrom writeable readOnly: false # This makes the cdrom be exposed as SATA device bus: sata volumes: - name: mypvcdisk persistentVolumeClaim: claimName: mypvc","title":"cdrom"},{"location":"virtual_machines/disks_and_volumes/#volumes","text":"Supported volume sources are cloudInitNoCloud cloudInitConfigDrive persistentVolumeClaim dataVolume ephemeral containerDisk emptyDisk hostDisk configMap secret serviceAccount downwardMetrics All possible configuration options are available in the Volume API Reference .","title":"Volumes"},{"location":"virtual_machines/disks_and_volumes/#cloudinitnocloud","text":"Allows attaching cloudInitNoCloud data-sources to the VM. If the VM contains a proper cloud-init setup, it will pick up the disk as a user-data source. A simple example which attaches a Secret as a cloud-init disk datasource may look like this: metadata: name: testvmi-cloudinitnocloud apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance spec: domain: resources: requests: memory: 64M devices: disks: - name: mybootdisk lun: {} - name: mynoclouddisk disk: {} volumes: - name: mybootdisk persistentVolumeClaim: claimName: mypvc - name: mynoclouddisk cloudInitNoCloud: secretRef: name: testsecret","title":"cloudInitNoCloud"},{"location":"virtual_machines/disks_and_volumes/#cloudinitconfigdrive","text":"Allows attaching cloudInitConfigDrive data-sources to the VM. If the VM contains a proper cloud-init setup, it will pick up the disk as a user-data source. A simple example which attaches a Secret as a cloud-init disk datasource may look like this: metadata: name: testvmi-cloudinitconfigdrive apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance spec: domain: resources: requests: memory: 64M devices: disks: - name: mybootdisk lun: {} - name: myconfigdrivedisk disk: {} volumes: - name: mybootdisk persistentVolumeClaim: claimName: mypvc - name: myconfigdrivedisk cloudInitConfigDrive: secretRef: name: testsecret","title":"cloudInitConfigDrive"},{"location":"virtual_machines/disks_and_volumes/#persistentvolumeclaim","text":"Allows connecting a PersistentVolumeClaim to a VM disk. Use a PersistentVolumeClaim when the VirtualMachineInstance's disk needs to persist after the VM terminates. This allows for the VM's data to remain persistent between restarts. A PersistentVolume can be in \"filesystem\" or \"block\" mode: Filesystem: For KubeVirt to be able to consume the disk present on a PersistentVolume's filesystem, the disk must be named disk.img and be placed in the root path of the filesystem. Currently the disk is also required to be in raw format. > Important: The disk.img image file needs to be owned by the user-id 107 in order to avoid permission issues. Note: If the disk.img image file has not been created manually before starting a VM then it will be created automatically with the PersistentVolumeClaim size. Since not every storage provisioner provides volumes with the exact usable amount of space as requested (e.g. due to filesystem overhead), KubeVirt tolerates up to 10% less available space. This can be configured with the pvc-tolerate-less-space-up-to-percent value in the kubevirt-config ConfigMap. Block: Use a block volume for consuming raw block devices. Note: you need to enable the BlockVolume feature gate . A simple example which attaches a PersistentVolumeClaim as a disk may look like this: metadata: name: testvmi-pvc apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance spec: domain: resources: requests: memory: 64M devices: disks: - name: mypvcdisk lun: {} volumes: - name: mypvcdisk persistentVolumeClaim: claimName: mypvc","title":"persistentVolumeClaim"},{"location":"virtual_machines/disks_and_volumes/#datavolume","text":"DataVolumes are a way to automate importing virtual machine disks onto PVCs during the virtual machine's launch flow. Without using a DataVolume, users have to prepare a PVC with a disk image before assigning it to a VM or VMI manifest. With a DataVolume, both the PVC creation and import is automated on behalf of the user.","title":"dataVolume"},{"location":"virtual_machines/disks_and_volumes/#datavolume-vm-behavior","text":"DataVolumes can be defined in the VM spec directly by adding the DataVolumes to the dataVolumeTemplates list. Below is an example. apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachine metadata: labels: kubevirt.io/vm: vm-alpine-datavolume name: vm-alpine-datavolume spec: running: false template: metadata: labels: kubevirt.io/vm: vm-alpine-datavolume spec: domain: devices: disks: - disk: bus: virtio name: datavolumedisk1 resources: requests: memory: 64M volumes: - dataVolume: name: alpine-dv name: datavolumedisk1 dataVolumeTemplates: - metadata: name: alpine-dv spec: pvc: accessModes: - ReadWriteOnce resources: requests: storage: 2Gi source: http: url: http://cdi-http-import-server.kubevirt/images/alpine.iso You can see the DataVolume defined in the dataVolumeTemplates section has two parts. The source and pvc The source part declares that there is a disk image living on an http server that we want to use as a volume for this VM. The pvc part declares the spec that should be used to create the PVC that hosts the source data. When this VM manifest is posted to the cluster, as part of the launch flow a PVC will be created using the spec provided and the source data will be automatically imported into that PVC before the VM starts. When the VM is deleted, the storage provisioned by the DataVolume will automatically be deleted as well.","title":"DataVolume VM Behavior"},{"location":"virtual_machines/disks_and_volumes/#datavolume-vmi-behavior","text":"For a VMI object, DataVolumes can be referenced as a volume source for the VMI. When this is done, it is expected that the referenced DataVolume exists in the cluster. The VMI will consume the DataVolume, but the DataVolume's life-cycle will not be tied to the VMI. Below is an example of a DataVolume being referenced by a VMI. It is expected that the DataVolume alpine-datavolume was created prior to posting the VMI manifest to the cluster. It is okay to post the VMI manifest to the cluster while the DataVolume is still having data imported. KubeVirt knows not to start the VMI until all referenced DataVolumes have finished their clone and import phases. apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance metadata: labels: special: vmi-alpine-datavolume name: vmi-alpine-datavolume spec: domain: devices: disks: - disk: bus: virtio name: disk1 machine: type: \"\" resources: requests: memory: 64M terminationGracePeriodSeconds: 0 volumes: - name: disk1 dataVolume: name: alpine-datavolume","title":"DataVolume VMI Behavior"},{"location":"virtual_machines/disks_and_volumes/#enabling-datavolume-support","text":"A DataVolume is a custom resource provided by the Containerized Data Importer (CDI) project. KubeVirt integrates with CDI in order to provide users a workflow for dynamically creating PVCs and importing data into those PVCs. In order to take advantage of the DataVolume volume source on a VM or VMI, the DataVolumes feature gate must be enabled in the kubevirt CR. CDI must also be installed. Installing CDI Go to the CDI release page Pick the latest stable release and post the corresponding cdi-controller-deployment.yaml manifest to your cluster.","title":"Enabling DataVolume support."},{"location":"virtual_machines/disks_and_volumes/#ephemeral","text":"An ephemeral volume is a local COW (copy on write) image that uses a network volume as a read-only backing store. With an ephemeral volume, the network backing store is never mutated. Instead all writes are stored on the ephemeral image which exists on local storage. KubeVirt dynamically generates the ephemeral images associated with a VM when the VM starts, and discards the ephemeral images when the VM stops. Ephemeral volumes are useful in any scenario where disk persistence is not desired. The COW image is discarded when VM reaches a final state (e.g., succeeded, failed). Currently, only PersistentVolumeClaim may be used as a backing store of the ephemeral volume. Up-to-date information on supported backing stores can be found in the KubeVirt API . metadata: name: testvmi-ephemeral-pvc apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance spec: domain: resources: requests: memory: 64M devices: disks: - name: mypvcdisk lun: {} volumes: - name: mypvcdisk ephemeral: persistentVolumeClaim: claimName: mypvc","title":"ephemeral"},{"location":"virtual_machines/disks_and_volumes/#containerdisk","text":"containerDisk was originally registryDisk, please update your code when needed. The containerDisk feature provides the ability to store and distribute VM disks in the container image registry. containerDisks can be assigned to VMs in the disks section of the VirtualMachineInstance spec. No network shared storage devices are utilized by containerDisks . The disks are pulled from the container registry and reside on the local node hosting the VMs that consume the disks.","title":"containerDisk"},{"location":"virtual_machines/disks_and_volumes/#when-to-use-a-containerdisk","text":"containerDisks are ephemeral storage devices that can be assigned to any number of active VirtualMachineInstances. This makes them an ideal tool for users who want to replicate a large number of VM workloads that do not require persistent data. containerDisks are commonly used in conjunction with VirtualMachineInstanceReplicaSets.","title":"When to use a containerDisk"},{"location":"virtual_machines/disks_and_volumes/#when-not-to-use-a-containerdisk","text":"containerDisks are not a good solution for any workload that requires persistent root disks across VM restarts.","title":"When Not to use a containerDisk"},{"location":"virtual_machines/disks_and_volumes/#containerdisk-workflow-example","text":"Users can inject a VirtualMachineInstance disk into a container image in a way that is consumable by the KubeVirt runtime. Disks must be placed into the /disk directory inside the container. Raw and qcow2 formats are supported. Qcow2 is recommended in order to reduce the container image's size. containerdisks can and should be based on scratch . No content except the image is required. Note: Prior to kubevirt 0.20, the containerDisk image needed to have kubevirt/container-disk-v1alpha as base image. Note: The containerDisk needs to be readable for the user with the UID 107 (qemu). Example: Inject a local VirtualMachineInstance disk into a container image. cat << END > Dockerfile FROM scratch ADD --chown=107:107 fedora25.qcow2 /disk/ END docker build -t vmidisks/fedora25:latest . Example: Inject a remote VirtualMachineInstance disk into a container image. cat << END > Dockerfile FROM scratch ADD --chown=107:107 https://cloud.centos.org/centos/7/images/CentOS-7-x86_64-GenericCloud.qcow2 /disk/ END Example: Upload the ContainerDisk container image to a registry. docker push vmidisks/fedora25:latest Example: Attach the ContainerDisk as an ephemeral disk to a VM. metadata: name: testvmi-containerdisk apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance spec: domain: resources: requests: memory: 64M devices: disks: - name: containerdisk disk: {} volumes: - name: containerdisk containerDisk: image: vmidisks/fedora25:latest Note that a containerDisk is file-based and therefore cannot be attached as a lun device to the VM.","title":"containerDisk Workflow Example"},{"location":"virtual_machines/disks_and_volumes/#custom-disk-image-path","text":"ContainerDisk also allows to store disk images in any folder, when required. The process is the same as previous. The main difference is, that in custom location, kubevirt does not scan for any image. It is your responsibility to provide full path for the disk image. Providing image path is optional. When no path is provided, kubevirt searches for disk images in default location: /disk . Example: Build container disk image: cat << END > Dockerfile FROM scratch ADD fedora25.qcow2 /custom-disk-path/fedora25.qcow2 END docker build -t vmidisks/fedora25:latest . docker push vmidisks/fedora25:latest Create VMI with container disk pointing to the custom location: metadata: name: testvmi-containerdisk apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance spec: domain: resources: requests: memory: 64M devices: disks: - name: containerdisk disk: {} volumes: - name: containerdisk containerDisk: image: vmidisks/fedora25:latest path: /custom-disk-path/fedora25.qcow2","title":"Custom disk image path"},{"location":"virtual_machines/disks_and_volumes/#emptydisk","text":"An emptyDisk works similar to an emptyDir in Kubernetes. An extra sparse qcow2 disk will be allocated and it will live as long as the VM. Thus it will survive guest side VM reboots, but not a VM re-creation. The disk capacity needs to be specified. Example: Boot cirros with an extra emptyDisk with a size of 2GiB : apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance metadata: name: testvmi-nocloud spec: terminationGracePeriodSeconds: 5 domain: resources: requests: memory: 64M devices: disks: - name: containerdisk disk: bus: virtio - name: emptydisk disk: bus: virtio volumes: - name: containerdisk containerDisk: image: kubevirt/cirros-registry-disk-demo:latest - name: emptydisk emptyDisk: capacity: 2Gi","title":"emptyDisk"},{"location":"virtual_machines/disks_and_volumes/#when-to-use-an-emptydisk","text":"Ephemeral VMs very often come with read-only root images and limited tmpfs space. In many cases this is not enough to install application dependencies and provide enough disk space for the application data. While this data is not critical and thus can be lost, it is still needed for the application to function properly during its lifetime. This is where an emptyDisk can be useful. An emptyDisk is often used and mounted somewhere in /var/lib or /var/run .","title":"When to use an emptyDisk"},{"location":"virtual_machines/disks_and_volumes/#hostdisk","text":"A hostDisk volume type provides the ability to create or use a disk image located somewhere on a node. It works similar to a hostPath in Kubernetes and provides two usage types: DiskOrCreate if a disk image does not exist at a given location then create one Disk a disk image must exist at a given location Note: you need to enable the HostDisk feature gate. Example: Create a 1Gi disk image located at /data/disk.img and attach it to a VM. apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance metadata: labels: special: vmi-host-disk name: vmi-host-disk spec: domain: devices: disks: - disk: bus: virtio name: host-disk machine: type: \"\" resources: requests: memory: 64M terminationGracePeriodSeconds: 0 volumes: - hostDisk: capacity: 1Gi path: /data/disk.img type: DiskOrCreate name: host-disk status: {}","title":"hostDisk"},{"location":"virtual_machines/disks_and_volumes/#configmap","text":"A configMap is a reference to a ConfigMap in Kubernetes. An extra iso disk will be allocated which has to be mounted on a VM. To mount the configMap users can use cloudInit and the disks serial number. The name needs to be set for a reference to the created kubernetes ConfigMap . Note: Currently, ConfigMap update is not propagate into the VMI. If a ConfigMap is updated, only a pod will be aware of changes, not running VMIs. Note: Due to a Kubernetes CRD issue , you cannot control the paths within the volume where ConfigMap keys are projected. Example: Attach the configMap to a VM and use cloudInit to mount the iso disk: apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance metadata: labels: special: vmi-fedora name: vmi-fedora spec: domain: devices: disks: - disk: bus: virtio name: containerdisk - disk: bus: virtio name: cloudinitdisk - disk: {} name: app-config-disk # set serial serial: CVLY623300HK240D machine: type: \"\" resources: requests: memory: 1024M terminationGracePeriodSeconds: 0 volumes: - name: containerdisk containerDisk: image: kubevirt/fedora-cloud-container-disk-demo:latest - cloudInitNoCloud: userData: |- #cloud-config password: fedora chpasswd: { expire: False } bootcmd: # mount the ConfigMap - \"mkdir /mnt/app-config\" - \"mount /dev/$(lsblk --nodeps -no name,serial | grep CVLY623300HK240D | cut -f1 -d' ') /mnt/app-config\" name: cloudinitdisk - configMap: name: app-config name: app-config-disk status: {}","title":"configMap"},{"location":"virtual_machines/disks_and_volumes/#secret","text":"A secret is a reference to a Secret in Kubernetes. An extra iso disk will be allocated which has to be mounted on a VM. To mount the secret users can use cloudInit and the disks serial number. The secretName needs to be set for a reference to the created kubernetes Secret . Note: Currently, Secret update propagation is not supported. If a Secret is updated, only a pod will be aware of changes, not running VMIs. Note: Due to a Kubernetes CRD issue , you cannot control the paths within the volume where Secret keys are projected. Example: Attach the secret to a VM and use cloudInit to mount the iso disk: apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance metadata: labels: special: vmi-fedora name: vmi-fedora spec: domain: devices: disks: - disk: bus: virtio name: containerdisk - disk: bus: virtio name: cloudinitdisk - disk: {} name: app-secret-disk # set serial serial: D23YZ9W6WA5DJ487 machine: type: \"\" resources: requests: memory: 1024M terminationGracePeriodSeconds: 0 volumes: - name: containerdisk containerDisk: image: kubevirt/fedora-cloud-container-disk-demo:latest - cloudInitNoCloud: userData: |- #cloud-config password: fedora chpasswd: { expire: False } bootcmd: # mount the Secret - \"mkdir /mnt/app-secret\" - \"mount /dev/$(lsblk --nodeps -no name,serial | grep D23YZ9W6WA5DJ487 | cut -f1 -d' ') /mnt/app-secret\" name: cloudinitdisk - secret: secretName: app-secret name: app-secret-disk status: {}","title":"secret"},{"location":"virtual_machines/disks_and_volumes/#serviceaccount","text":"A serviceAccount volume references a Kubernetes ServiceAccount . A new iso disk will be allocated with the content of the service account ( namespace , token and ca.crt ), which needs to be mounted in the VM. For automatic mounting, see the configMap and secret examples above. Example: apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance metadata: labels: special: vmi-fedora name: vmi-fedora spec: domain: devices: disks: - disk: bus: virtio name: containerdisk - disk: bus: virtio name: serviceaccountdisk machine: type: \"\" resources: requests: memory: 1024M terminationGracePeriodSeconds: 0 volumes: - name: containerdisk containerDisk: image: kubevirt/fedora-cloud-container-disk-demo:latest - name: serviceaccountdisk serviceAccount: serviceAccountName: default","title":"serviceAccount"},{"location":"virtual_machines/disks_and_volumes/#downwardmetrics","text":"A downwardMetrics volume exposes a limited set of VM and host metrics to the guest as a raw block volume. The format of the block volume is compatible with vhostmd . Getting a limited set of host and VM metrics is in some cases required to allow third-parties diagnosing performance issues on their appliances. One prominent example is SAP HANA. Example: apiVersion: kubevirt.io/v1 kind: VirtualMachineInstance metadata: labels: special: vmi-fedora name: vmi-fedora spec: domain: devices: disks: - disk: bus: virtio name: containerdisk - disk: bus: virtio name: metrics machine: type: \"\" resources: requests: memory: 1024M terminationGracePeriodSeconds: 0 volumes: - name: containerdisk containerDisk: image: kubevirt/fedora-cloud-container-disk-demo:latest - name: metrics downwardMetrics: {} The vm-dump-metrics tool can be used to read the metrics: $ dnf install -y vm-dump-metrics $ vm-dump-metrics <metrics> <metric type=\"string\" context=\"host\"> <name>HostName</name> <value>node01</value> [...] <metric type=\"int64\" context=\"host\" unit=\"s\"> <name>Time</name> <value>1619008605</value> </metric> <metric type=\"string\" context=\"host\"> <name>VirtualizationVendor</name> <value>kubevirt.io</value> </metric> </metrics> Note: The DownwardMetrics feature gate must be enabled to use this volume. Available starting with KubeVirt v0.42.0.","title":"downwardMetrics"},{"location":"virtual_machines/disks_and_volumes/#high-performance-features","text":"","title":"High Performance Features"},{"location":"virtual_machines/disks_and_volumes/#iothreads","text":"Libvirt has the ability to use IOThreads for dedicated disk access (for supported devices). These are dedicated event loop threads that perform block I/O requests and improve scalability on SMP systems. KubeVirt exposes this libvirt feature through the ioThreadsPolicy setting. Additionally, each Disk device exposes a dedicatedIOThread setting. This is a boolean that indicates the specified disk should be allocated an exclusive IOThread that will never be shared with other disks. Currently valid policies are shared and auto . If ioThreadsPolicy is omitted entirely, use of IOThreads will be disabled. However, if any disk requests a dedicated IOThread, ioThreadsPolicy will be enabled and default to shared .","title":"IOThreads"},{"location":"virtual_machines/disks_and_volumes/#shared","text":"An ioThreadsPolicy of shared indicates that KubeVirt should use one thread that will be shared by all disk devices. This policy stems from the fact that large numbers of IOThreads is generally not useful as additional context switching is incurred for each thread. Disks with dedicatedIOThread set to true will not use the shared thread, but will instead be allocated an exclusive thread. This is generally useful if a specific Disk is expected to have heavy I/O traffic, e.g. a database spindle.","title":"Shared"},{"location":"virtual_machines/disks_and_volumes/#auto","text":"auto IOThreads indicates that KubeVirt should use a pool of IOThreads and allocate disks to IOThreads in a round-robin fashion. The pool size is generally limited to twice the number of VCPU's allocated to the VM. This essentially attempts to dedicate disks to separate IOThreads, but only up to a reasonable limit. This would come in to play for systems with a large number of disks and a smaller number of CPU's for instance. As a caveat to the size of the IOThread pool, disks with dedicatedIOThread will always be guaranteed their own thread. This effectively diminishes the upper limit of the number of threads allocated to the rest of the disks. For example, a VM with 2 CPUs would normally use 4 IOThreads for all disks. However if one disk had dedicatedIOThread set to true, then KubeVirt would only use 3 IOThreads for the shared pool. There is always guaranteed to be at least one thread for disks that will use the shared IOThreads pool. Thus if a sufficiently large number of disks have dedicated IOThreads assigned, auto and shared policies would essentially result in the same layout.","title":"Auto"},{"location":"virtual_machines/disks_and_volumes/#iothreads-with-dedicated-pinned-cpus","text":"When guest's vCPUs are pinned to a host's physical CPUs, it is also best to pin the IOThreads to specific CPUs to prevent these from floating between the CPUs. KubeVirt will automatically calculate and pin each IOThread to a CPU or a set of CPUs, depending on the ration between them. In case there are more IOThreads than CPUs, each IOThread will be pinned to a CPU, in a round-robin fashion. Otherwise, when there are fewer IOThreads than CPU, each IOThread will be pinned to a set of CPUs.","title":"IOThreads with Dedicated (pinned) CPUs"},{"location":"virtual_machines/disks_and_volumes/#iothreads-with-qemu-emulator-thread-and-dedicated-pinned-cpus","text":"To further improve the vCPUs latency, KubeVirt can allocate an additional dedicated physical CPU 1 , exclusively for the emulator thread, to which it will be pinned. This will effectively \"isolate\" the emulator thread from the vCPUs of the VMI. When ioThreadsPolicy is set to auto IOThreads will also be \"isolated\" from the vCPUs and placed on the same physical CPU as the QEMU emulator thread.","title":"IOThreads with QEMU Emulator thread and Dedicated (pinned) CPUs"},{"location":"virtual_machines/disks_and_volumes/#examples","text":"","title":"Examples"},{"location":"virtual_machines/disks_and_volumes/#shared-iothreads","text":"apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance metadata: labels: special: vmi-shared name: vmi-shared spec: domain: ioThreadsPolicy: shared cpu: cores: 2 devices: disks: - disk: bus: virtio name: vmi-shared_disk - disk: bus: virtio name: emptydisk dedicatedIOThread: true - disk: bus: virtio name: emptydisk2 dedicatedIOThread: true - disk: bus: virtio name: emptydisk3 - disk: bus: virtio name: emptydisk4 - disk: bus: virtio name: emptydisk5 - disk: bus: virtio name: emptydisk6 machine: type: \"\" resources: requests: memory: 64M volumes: - name: vmi-shared_disk persistentVolumeClaim: claimName: vmi-shared_pvc - emptyDisk: capacity: 1Gi name: emptydisk - emptyDisk: capacity: 1Gi name: emptydisk2 - emptyDisk: capacity: 1Gi name: emptydisk3 - emptyDisk: capacity: 1Gi name: emptydisk4 - emptyDisk: capacity: 1Gi name: emptydisk5 - emptyDisk: capacity: 1Gi name: emptydisk6 In this example, emptydisk and emptydisk2 both request a dedicated IOThread. vmi-shared_disk, and emptydisk 3 through 6 will all shared one IOThread. mypvc: 1 emptydisk: 2 emptydisk2: 3 emptydisk3: 1 emptydisk4: 1 emptydisk5: 1 emptydisk6: 1","title":"Shared IOThreads"},{"location":"virtual_machines/disks_and_volumes/#auto-iothreads","text":"apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance metadata: labels: special: vmi-shared name: vmi-shared spec: domain: ioThreadsPolicy: auto cpu: cores: 2 devices: disks: - disk: bus: virtio name: mydisk - disk: bus: virtio name: emptydisk dedicatedIOThread: true - disk: bus: virtio name: emptydisk2 dedicatedIOThread: true - disk: bus: virtio name: emptydisk3 - disk: bus: virtio name: emptydisk4 - disk: bus: virtio name: emptydisk5 - disk: bus: virtio name: emptydisk6 machine: type: \"\" resources: requests: memory: 64M volumes: - name: mydisk persistentVolumeClaim: claimName: mypvc - emptyDisk: capacity: 1Gi name: emptydisk - emptyDisk: capacity: 1Gi name: emptydisk2 - emptyDisk: capacity: 1Gi name: emptydisk3 - emptyDisk: capacity: 1Gi name: emptydisk4 - emptyDisk: capacity: 1Gi name: emptydisk5 - emptyDisk: capacity: 1Gi name: emptydisk6 This VM is identical to the first, except it requests auto IOThreads. emptydisk and emptydisk2 will still be allocated individual IOThreads, but the rest of the disks will be split across 2 separate iothreads (twice the number of CPU cores is 4). Disks will be assigned to IOThreads like this: mypvc: 1 emptydisk: 3 emptydisk2: 4 emptydisk3: 2 emptydisk4: 1 emptydisk5: 2 emptydisk6: 1","title":"Auto IOThreads"},{"location":"virtual_machines/disks_and_volumes/#virtio-block-multi-queue","text":"Block Multi-Queue is a framework for the Linux block layer that maps Device I/O queries to multiple queues. This splits I/O processing up across multiple threads, and therefor multiple CPUs. libvirt recommends that the number of queues used should match the number of CPUs allocated for optimal performance. This feature is enabled by the BlockMultiQueue setting under Devices : spec: domain: devices: blockMultiQueue: true disks: - disk: bus: virtio name: mydisk Note: Due to the way KubeVirt implements CPU allocation, blockMultiQueue can only be used if a specific CPU allocation is requested. If a specific number of CPUs hasn't been allocated to a VirtualMachine, KubeVirt will use all CPU's on the node on a best effort basis. In that case the amount of CPU allocation to a VM at the host level could change over time. If blockMultiQueue were to request a number of queues to match all the CPUs on a node, that could lead to over-allocation scenarios. To avoid this, KubeVirt enforces that a specific slice of CPU resources is requested in order to take advantage of this feature.","title":"Virtio Block Multi-Queue"},{"location":"virtual_machines/disks_and_volumes/#example","text":"metadata: name: testvmi-disk apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance spec: domain: resources: requests: memory: 64M cpu: 4 devices: blockMultiQueue: true disks: - name: mypvcdisk disk: bus: virtio volumes: - name: mypvcdisk persistentVolumeClaim: claimName: mypvc This example will enable Block Multi-Queue for the disk mypvcdisk and allocate 4 queues (to match the number of CPUs requested).","title":"Example"},{"location":"virtual_machines/disks_and_volumes/#disk-device-cache","text":"KubeVirt supports none and writethrough KVM/QEMU cache modes. none I/O from the guest is not cached on the host. Use this option for guests with large I/O requirements. This option is generally the best choice. writethrough I/O from the guest is cached on the host but written through to the physical medium. Important: none cache mode is set as default if the file system supports direct I/O, otherwise, writethrough is used. Note: It is possible to force a specific cache mode, although if none mode has been chosen and the file system does not support direct I/O then started VMI will return an error. Example: force writethrough cache mode apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance metadata: labels: special: vmi-pvc name: vmi-pvc spec: domain: devices: disks: - disk: bus: virtio name: pvcdisk cache: writethrough machine: type: \"\" resources: requests: memory: 64M terminationGracePeriodSeconds: 0 volumes: - name: pvcdisk persistentVolumeClaim: claimName: disk-alpine status: {}","title":"Disk device cache"},{"location":"virtual_machines/dns/","text":"DNS records \u00b6 In order to create unique DNS records per VirtualMachineInstance, it is possible to set spec.hostname and spec.subdomain . If a subdomain is set and a headless service with a name, matching the subdomain, exists, kube-dns will create unique DNS entries for every VirtualMachineInstance which matches the selector of the service. Have a look at the DNS for Services and Pods documentation for additional information. The following example consists of a VirtualMachine and a headless Service which matches the labels and the subdomain of the VirtualMachineInstance: apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance metadata: name: vmi-fedora labels: expose: me spec: hostname: \"myvmi\" subdomain: \"mysubdomain\" domain: devices: disks: - disk: bus: virtio name: containerdisk - disk: bus: virtio name: cloudinitdisk resources: requests: memory: 1024M terminationGracePeriodSeconds: 0 volumes: - name: containerdisk containerDisk: image: kubevirt/fedora-cloud-registry-disk-demo:latest - cloudInitNoCloud: userDataBase64: IyEvYmluL2Jhc2gKZWNobyAiZmVkb3JhOmZlZG9yYSIgfCBjaHBhc3N3ZAo= name: cloudinitdisk --- apiVersion: v1 kind: Service metadata: name: mysubdomain spec: selector: expose: me clusterIP: None ports: - name: foo # Actually, no port is needed. port: 1234 targetPort: 1234 As a consequence, when we enter the VirtualMachineInstance via e.g. virtctl console vmi-fedora and ping myvmi.mysubdomain we see that we find a DNS entry for myvmi.mysubdomain.default.svc.cluster.local which points to 10.244.0.57 , which is the IP of the VirtualMachineInstance (not of the Service): [fedora@myvmi ~]$ ping myvmi.mysubdomain PING myvmi.mysubdomain.default.svc.cluster.local (10.244.0.57) 56(84) bytes of data. 64 bytes from myvmi.mysubdomain.default.svc.cluster.local (10.244.0.57): icmp_seq=1 ttl=64 time=0.029 ms [fedora@myvmi ~]$ ip a 2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000 link/ether 0a:58:0a:f4:00:39 brd ff:ff:ff:ff:ff:ff inet 10.244.0.57/24 brd 10.244.0.255 scope global dynamic eth0 valid_lft 86313556sec preferred_lft 86313556sec inet6 fe80::858:aff:fef4:39/64 scope link valid_lft forever preferred_lft forever So spec.hostname and spec.subdomain get translated to a DNS A-record of the form <vmi.spec.hostname>.<vmi.spec.subdomain>.<vmi.metadata.namespace>.svc.cluster.local . If no spec.hostname is set, then we fall back to the VirtualMachineInstance name itself. The resulting DNS A-record looks like this then: <vmi.metadata.name>.<vmi.spec.subdomain>.<vmi.metadata.namespace>.svc.cluster.local .","title":"DNS records"},{"location":"virtual_machines/dns/#dns-records","text":"In order to create unique DNS records per VirtualMachineInstance, it is possible to set spec.hostname and spec.subdomain . If a subdomain is set and a headless service with a name, matching the subdomain, exists, kube-dns will create unique DNS entries for every VirtualMachineInstance which matches the selector of the service. Have a look at the DNS for Services and Pods documentation for additional information. The following example consists of a VirtualMachine and a headless Service which matches the labels and the subdomain of the VirtualMachineInstance: apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance metadata: name: vmi-fedora labels: expose: me spec: hostname: \"myvmi\" subdomain: \"mysubdomain\" domain: devices: disks: - disk: bus: virtio name: containerdisk - disk: bus: virtio name: cloudinitdisk resources: requests: memory: 1024M terminationGracePeriodSeconds: 0 volumes: - name: containerdisk containerDisk: image: kubevirt/fedora-cloud-registry-disk-demo:latest - cloudInitNoCloud: userDataBase64: IyEvYmluL2Jhc2gKZWNobyAiZmVkb3JhOmZlZG9yYSIgfCBjaHBhc3N3ZAo= name: cloudinitdisk --- apiVersion: v1 kind: Service metadata: name: mysubdomain spec: selector: expose: me clusterIP: None ports: - name: foo # Actually, no port is needed. port: 1234 targetPort: 1234 As a consequence, when we enter the VirtualMachineInstance via e.g. virtctl console vmi-fedora and ping myvmi.mysubdomain we see that we find a DNS entry for myvmi.mysubdomain.default.svc.cluster.local which points to 10.244.0.57 , which is the IP of the VirtualMachineInstance (not of the Service): [fedora@myvmi ~]$ ping myvmi.mysubdomain PING myvmi.mysubdomain.default.svc.cluster.local (10.244.0.57) 56(84) bytes of data. 64 bytes from myvmi.mysubdomain.default.svc.cluster.local (10.244.0.57): icmp_seq=1 ttl=64 time=0.029 ms [fedora@myvmi ~]$ ip a 2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000 link/ether 0a:58:0a:f4:00:39 brd ff:ff:ff:ff:ff:ff inet 10.244.0.57/24 brd 10.244.0.255 scope global dynamic eth0 valid_lft 86313556sec preferred_lft 86313556sec inet6 fe80::858:aff:fef4:39/64 scope link valid_lft forever preferred_lft forever So spec.hostname and spec.subdomain get translated to a DNS A-record of the form <vmi.spec.hostname>.<vmi.spec.subdomain>.<vmi.metadata.namespace>.svc.cluster.local . If no spec.hostname is set, then we fall back to the VirtualMachineInstance name itself. The resulting DNS A-record looks like this then: <vmi.metadata.name>.<vmi.spec.subdomain>.<vmi.metadata.namespace>.svc.cluster.local .","title":"DNS records"},{"location":"virtual_machines/guest_agent_information/","text":"Guest Agent information \u00b6 Guest Agent (GA) is an optional component that can run inside of Virtual Machines. The GA provides plenty of additional runtime information about the running operating system (OS). More technical detail about available GA commands is available here . Guest Agent info in Virtual Machine status \u00b6 GA presence in the Virtual Machine is signaled with a condition in the VirtualMachineInstance status. The condition tells that the GA is connected and can be used. GA condition on VirtualMachineInstance status: conditions: - lastProbeTime: \"2020-02-28T10:22:59Z\" lastTransitionTime: null status: \"True\" type: AgentConnected When the GA is connected, additional OS information is shown in the status. This information comprises: guest info, which contains OS runtime data interfaces info, which shows QEMU interfaces merged with GA interfaces info. Below is the example of the information shown in the VirtualMachineInstance status. GA info with merged into status status: guestOSInfo: id: fedora kernelRelease: 4.18.16-300.fc29.x86_64 kernelVersion: '#1 SMP Sat Oct 20 23:24:08 UTC 2018' name: Fedora prettyName: Fedora 29 (Cloud Edition) version: \"29\" versionId: \"29\" interfaces: - interfaceName: eth0 ipAddress: 10.244.0.23/24 ipAddresses: - 10.244.0.23/24 - fe80::858:aff:fef4:17/64 mac: 0a:58:0a:f4:00:17 name: default When the Guest Agent is not present in the Virtual Machine, the information is not shown. No error is reported because the Guest Agent is an optional component. Guest Agent info available through the API \u00b6 The data shown in the VirtualMachineInstance status are a subset of the information available. The rest of the data is available via the REST API exposed in the Kubernetes kube-api server. There are three new subresources added to the VirtualMachineInstance object: - guestosinfo - userlist - filesystemlist The whole GA data is returned via guestosinfo subresource available behind the API endpoint. /apis/subresources.kubevirt.io/v1alpha3/namespaces/{namespace}/virtualmachineinstances/{name}/guestosinfo GuestOSInfo sample data: { \"fsInfo\": { \"disks\": [ { \"diskName\": \"vda1\", \"fileSystemType\": \"ext4\", \"mountPoint\": \"/\", \"totalBytes\": 0, \"usedBytes\": 0 } ] }, \"guestAgentVersion\": \"2.11.2\", \"hostname\": \"testvmi6m5krnhdlggc9mxfsrnhlxqckgv5kqrwcwpgr5mdpv76grrk\", \"metadata\": { \"creationTimestamp\": null }, \"os\": { \"id\": \"fedora\", \"kernelRelease\": \"4.18.16-300.fc29.x86_64\", \"kernelVersion\": \"#1 SMP Sat Oct 20 23:24:08 UTC 2018\", \"machine\": \"x86_64\", \"name\": \"Fedora\", \"prettyName\": \"Fedora 29 (Cloud Edition)\", \"version\": \"29 (Cloud Edition)\", \"versionId\": \"29\" }, \"timezone\": \"UTC, 0\" } Items FSInfo and UserList are capped to the max capacity of 10 items, as a precaution for VMs with thousands of users. Full list of Filesystems is available through the subresource filesystemlist which is available as endpoint. /apis/subresources.kubevirt.io/v1alpha3/namespaces/{namespace}/virtualmachineinstances/{name}/filesystemlist Filesystem sample data: { \"items\": [ { \"diskName\": \"vda1\", \"fileSystemType\": \"ext4\", \"mountPoint\": \"/\", \"totalBytes\": 3927900160, \"usedBytes\": 1029201920 } ], \"metadata\": {} } Full list of the Users is available through the subresource userlist which is available as endpoint. /apis/subresources.kubevirt.io/v1alpha3/namespaces/{namespace}/virtualmachineinstances/{name}/userlist Userlist sample data: { \"items\": [ { \"loginTime\": 1580467675.876078, \"userName\": \"fedora\" } ], \"metadata\": {} } User LoginTime is in fractional seconds since epoch time. It is left for the consumer to convert to the desired format.","title":"Guest Agent information"},{"location":"virtual_machines/guest_agent_information/#guest-agent-information","text":"Guest Agent (GA) is an optional component that can run inside of Virtual Machines. The GA provides plenty of additional runtime information about the running operating system (OS). More technical detail about available GA commands is available here .","title":"Guest Agent information"},{"location":"virtual_machines/guest_agent_information/#guest-agent-info-in-virtual-machine-status","text":"GA presence in the Virtual Machine is signaled with a condition in the VirtualMachineInstance status. The condition tells that the GA is connected and can be used. GA condition on VirtualMachineInstance status: conditions: - lastProbeTime: \"2020-02-28T10:22:59Z\" lastTransitionTime: null status: \"True\" type: AgentConnected When the GA is connected, additional OS information is shown in the status. This information comprises: guest info, which contains OS runtime data interfaces info, which shows QEMU interfaces merged with GA interfaces info. Below is the example of the information shown in the VirtualMachineInstance status. GA info with merged into status status: guestOSInfo: id: fedora kernelRelease: 4.18.16-300.fc29.x86_64 kernelVersion: '#1 SMP Sat Oct 20 23:24:08 UTC 2018' name: Fedora prettyName: Fedora 29 (Cloud Edition) version: \"29\" versionId: \"29\" interfaces: - interfaceName: eth0 ipAddress: 10.244.0.23/24 ipAddresses: - 10.244.0.23/24 - fe80::858:aff:fef4:17/64 mac: 0a:58:0a:f4:00:17 name: default When the Guest Agent is not present in the Virtual Machine, the information is not shown. No error is reported because the Guest Agent is an optional component.","title":"Guest Agent info in Virtual Machine status"},{"location":"virtual_machines/guest_agent_information/#guest-agent-info-available-through-the-api","text":"The data shown in the VirtualMachineInstance status are a subset of the information available. The rest of the data is available via the REST API exposed in the Kubernetes kube-api server. There are three new subresources added to the VirtualMachineInstance object: - guestosinfo - userlist - filesystemlist The whole GA data is returned via guestosinfo subresource available behind the API endpoint. /apis/subresources.kubevirt.io/v1alpha3/namespaces/{namespace}/virtualmachineinstances/{name}/guestosinfo GuestOSInfo sample data: { \"fsInfo\": { \"disks\": [ { \"diskName\": \"vda1\", \"fileSystemType\": \"ext4\", \"mountPoint\": \"/\", \"totalBytes\": 0, \"usedBytes\": 0 } ] }, \"guestAgentVersion\": \"2.11.2\", \"hostname\": \"testvmi6m5krnhdlggc9mxfsrnhlxqckgv5kqrwcwpgr5mdpv76grrk\", \"metadata\": { \"creationTimestamp\": null }, \"os\": { \"id\": \"fedora\", \"kernelRelease\": \"4.18.16-300.fc29.x86_64\", \"kernelVersion\": \"#1 SMP Sat Oct 20 23:24:08 UTC 2018\", \"machine\": \"x86_64\", \"name\": \"Fedora\", \"prettyName\": \"Fedora 29 (Cloud Edition)\", \"version\": \"29 (Cloud Edition)\", \"versionId\": \"29\" }, \"timezone\": \"UTC, 0\" } Items FSInfo and UserList are capped to the max capacity of 10 items, as a precaution for VMs with thousands of users. Full list of Filesystems is available through the subresource filesystemlist which is available as endpoint. /apis/subresources.kubevirt.io/v1alpha3/namespaces/{namespace}/virtualmachineinstances/{name}/filesystemlist Filesystem sample data: { \"items\": [ { \"diskName\": \"vda1\", \"fileSystemType\": \"ext4\", \"mountPoint\": \"/\", \"totalBytes\": 3927900160, \"usedBytes\": 1029201920 } ], \"metadata\": {} } Full list of the Users is available through the subresource userlist which is available as endpoint. /apis/subresources.kubevirt.io/v1alpha3/namespaces/{namespace}/virtualmachineinstances/{name}/userlist Userlist sample data: { \"items\": [ { \"loginTime\": 1580467675.876078, \"userName\": \"fedora\" } ], \"metadata\": {} } User LoginTime is in fractional seconds since epoch time. It is left for the consumer to convert to the desired format.","title":"Guest Agent info available through the API"},{"location":"virtual_machines/guest_operating_system_information/","text":"Guest Operating System Information \u00b6 Guest operating system identity for the VirtualMachineInstance will be provided by the label kubevirt.io/os : metadata: name: myvmi labels: kubevirt.io/os: win2k12r2 The kubevirt.io/os label is based on the short OS identifier from libosinfo database. The following Short IDs are currently supported: Short ID Name Version Family ID win2k12r2 Microsoft Windows Server 2012 R2 6.3 winnt https://www.microsoft.com/en-us/evalcenter/evaluate-windows-server-2012-r2 Use with presets \u00b6 A VirtualMachineInstancePreset representing an operating system with a kubevirt.io/os label could be applied on any given VirtualMachineInstance that have and match the kubevirt.io/os label. Default presets for the OS identifiers above are included in the current release. Windows Server 2012R2 VirtualMachineInstancePreset Example \u00b6 apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstancePreset metadata: name: windows-server-2012r2 selector: matchLabels: kubevirt.io/os: win2k12r2 spec: domain: cpu: cores: 2 resources: requests: memory: 2G features: acpi: {} apic: {} hyperv: relaxed: {} vapic: {} spinlocks: spinlocks: 8191 clock: utc: {} timer: hpet: present: false pit: tickPolicy: delay rtc: tickPolicy: catchup hyperv: {} --- apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance metadata: labels: kubevirt.io/os: win2k12r2 name: windows2012r2 spec: terminationGracePeriodSeconds: 0 domain: firmware: uuid: 5d307ca9-b3ef-428c-8861-06e72d69f223 devices: disks: - name: server2012r2 disk: dev: vda volumes: - name: server2012r2 persistentVolumeClaim: claimName: my-windows-image Once the VirtualMachineInstancePreset is applied to the VirtualMachineInstance , the resulting resource would look like this: apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance metadata: annotations: presets.virtualmachineinstances.kubevirt.io/presets-applied: kubevirt.io/v1alpha3 virtualmachineinstancepreset.kubevirt.io/windows-server-2012r2: kubevirt.io/v1alpha3 labels: kubevirt.io/os: win2k12r2 name: windows2012r2 spec: terminationGracePeriodSeconds: 0 domain: cpu: cores: 2 resources: requests: memory: 2G features: acpi: {} apic: {} hyperv: relaxed: {} vapic: {} spinlocks: spinlocks: 8191 clock: utc: {} timer: hpet: present: false pit: tickPolicy: delay rtc: tickPolicy: catchup hyperv: {} firmware: uuid: 5d307ca9-b3ef-428c-8861-06e72d69f223 devices: disks: - name: server2012r2 disk: dev: vda volumes: - name: server2012r2 persistentVolumeClaim: claimName: my-windows-image For more information see VirtualMachineInstancePresets HyperV optimizations \u00b6 KubeVirt supports quite a lot of so-called \"HyperV enlightenments\", which are optimizations for Windows Guests. Some of these optimization may require an up to date host kernel support to work properly, or to deliver the maximum performance gains. KubeVirt can perform extra checks on the hosts before to run Hyper-V enabled VMs, to make sure the host has no known issues with Hyper-V support, properly expose all the required features and thus we can expect optimal performance. These checks are disabled by default for backward compatibility and because they depend on the node-feature-discovery and on extra configuration. To enable strict host checking, the user may expand the featureGates field in the KubeVirt CR by adding the HypervStrictCheck to it. apiVersion: kubevirt.io/v1alpha3 kind: Kubevirt metadata: name: kubevirt namespace: kubevirt spec: ... configuration: developerConfiguration: featureGates: - \"HypervStrictCheck\" Alternatively, users can edit an existing kubevirt CR: kubectl edit kubevirt kubevirt -n kubevirt ... spec: configuration: developerConfiguration: featureGates: - \"HypervStrictCheck\" - \"CPUManager\"","title":"Guest Operating System Information"},{"location":"virtual_machines/guest_operating_system_information/#guest-operating-system-information","text":"Guest operating system identity for the VirtualMachineInstance will be provided by the label kubevirt.io/os : metadata: name: myvmi labels: kubevirt.io/os: win2k12r2 The kubevirt.io/os label is based on the short OS identifier from libosinfo database. The following Short IDs are currently supported: Short ID Name Version Family ID win2k12r2 Microsoft Windows Server 2012 R2 6.3 winnt https://www.microsoft.com/en-us/evalcenter/evaluate-windows-server-2012-r2","title":"Guest Operating System Information"},{"location":"virtual_machines/guest_operating_system_information/#use-with-presets","text":"A VirtualMachineInstancePreset representing an operating system with a kubevirt.io/os label could be applied on any given VirtualMachineInstance that have and match the kubevirt.io/os label. Default presets for the OS identifiers above are included in the current release.","title":"Use with presets"},{"location":"virtual_machines/guest_operating_system_information/#windows-server-2012r2-virtualmachineinstancepreset-example","text":"apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstancePreset metadata: name: windows-server-2012r2 selector: matchLabels: kubevirt.io/os: win2k12r2 spec: domain: cpu: cores: 2 resources: requests: memory: 2G features: acpi: {} apic: {} hyperv: relaxed: {} vapic: {} spinlocks: spinlocks: 8191 clock: utc: {} timer: hpet: present: false pit: tickPolicy: delay rtc: tickPolicy: catchup hyperv: {} --- apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance metadata: labels: kubevirt.io/os: win2k12r2 name: windows2012r2 spec: terminationGracePeriodSeconds: 0 domain: firmware: uuid: 5d307ca9-b3ef-428c-8861-06e72d69f223 devices: disks: - name: server2012r2 disk: dev: vda volumes: - name: server2012r2 persistentVolumeClaim: claimName: my-windows-image Once the VirtualMachineInstancePreset is applied to the VirtualMachineInstance , the resulting resource would look like this: apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance metadata: annotations: presets.virtualmachineinstances.kubevirt.io/presets-applied: kubevirt.io/v1alpha3 virtualmachineinstancepreset.kubevirt.io/windows-server-2012r2: kubevirt.io/v1alpha3 labels: kubevirt.io/os: win2k12r2 name: windows2012r2 spec: terminationGracePeriodSeconds: 0 domain: cpu: cores: 2 resources: requests: memory: 2G features: acpi: {} apic: {} hyperv: relaxed: {} vapic: {} spinlocks: spinlocks: 8191 clock: utc: {} timer: hpet: present: false pit: tickPolicy: delay rtc: tickPolicy: catchup hyperv: {} firmware: uuid: 5d307ca9-b3ef-428c-8861-06e72d69f223 devices: disks: - name: server2012r2 disk: dev: vda volumes: - name: server2012r2 persistentVolumeClaim: claimName: my-windows-image For more information see VirtualMachineInstancePresets","title":"Windows Server 2012R2 VirtualMachineInstancePreset Example"},{"location":"virtual_machines/guest_operating_system_information/#hyperv-optimizations","text":"KubeVirt supports quite a lot of so-called \"HyperV enlightenments\", which are optimizations for Windows Guests. Some of these optimization may require an up to date host kernel support to work properly, or to deliver the maximum performance gains. KubeVirt can perform extra checks on the hosts before to run Hyper-V enabled VMs, to make sure the host has no known issues with Hyper-V support, properly expose all the required features and thus we can expect optimal performance. These checks are disabled by default for backward compatibility and because they depend on the node-feature-discovery and on extra configuration. To enable strict host checking, the user may expand the featureGates field in the KubeVirt CR by adding the HypervStrictCheck to it. apiVersion: kubevirt.io/v1alpha3 kind: Kubevirt metadata: name: kubevirt namespace: kubevirt spec: ... configuration: developerConfiguration: featureGates: - \"HypervStrictCheck\" Alternatively, users can edit an existing kubevirt CR: kubectl edit kubevirt kubevirt -n kubevirt ... spec: configuration: developerConfiguration: featureGates: - \"HypervStrictCheck\" - \"CPUManager\"","title":"HyperV optimizations"},{"location":"virtual_machines/host-devices/","text":"Host Devices Assignment \u00b6 KubeVirt provides a mechanism for assigning host devices to a virtual machine. This mechanism is generic and allows various types of PCI devices, such as accelerators (including GPUs) or any other devices attached to a PCI bus, to be assigned. It also allows Linux Mediated devices , such as pre-configured virtual GPUs to be assigned using the same mechanism. Host preparation for PCI Passthrough \u00b6 Host Devices passthrough requires the virtualization extension and the IOMMU extension (Intel VT-d or AMD IOMMU) to be enabled in the BIOS. To enable IOMMU, depending on the CPU type, a host should be booted with an additional kernel parameter, intel_iommu=on for Intel and amd_iommu=on for AMD. Append these parameters to the end of the GRUB_CMDLINE_LINUX line in the grub configuration file. # vi /etc/default/grub ... GRUB_CMDLINE_LINUX=\"nofb splash=quiet console=tty0 ... intel_iommu=on ... # grub2-mkconfig -o /boot/grub2/grub.cfg # reboot The vfio-pci kernel module should be enabled on the host. # modprobe vfio-pci Preparation of PCI devices for passthrough \u00b6 At this time, KubeVirt is only able to assign PCI devices that are using the vfio-pci driver. To prepare a specific device for device assignment, it should first be unbound from its original driver and bound to the vfio-pci driver. Find the PCI address of the desired device: $ lspci -DD|grep NVIDIA 0000.65:00.0 3D controller [0302]: NVIDIA Corporation TU104GL [Tesla T4] [10de:1eb8] (rev a1) Bind that device to the vfio-pci driver: echo 0000:65:00.0 > /sys/bus/pci/drivers/nvidia/unbind echo \"vfio-pci\" > /sys/bus/pci/devices/0000\\:65\\:00.0/driver_override echo 0000:65:00.0 > /sys/bus/pci/drivers/vfio-pci/bind Preparation of mediated devices such as vGPU \u00b6 At this time, configuration of a Mediated device (mdev) should be done according to the vendor directions. Once the mdev is configured, KubeVirt will be able to discover and use it for device assignment. Listing permitted devices \u00b6 Administrators can control which host devices are exposed and permitted to be used in the cluster. Permitted host devices in the cluster will need to be allowlisted in KubeVirt CR by its vendor:product selector for PCI devices or mediated device names. configuration: permittedHostDevices: pciHostDevices: - pciVendorSelector: \"10DE:1EB8\" resourceName: \"nvidia.com/TU104GL_Tesla_T4\" externalResourceProvider: true - pciVendorSelector: \"8086:6F54\" resourceName: \"intel.com/qat\" mediatedDevices: - mdevNameSelector: \"GRID T4-1Q\" resourceName: \"nvidia.com/GRID_T4-1Q\" pciVendorSelector is a PCI vendor ID and product ID tuple in the form vendor_id:product_id . This tuple can identify specific types of devices on a host. For example, the identifier 10de:1eb8 , shown above, can be found using lspci . $ lspci -nnv|grep -i nvidia 65:00.0 3D controller [0302]: NVIDIA Corporation TU104GL [Tesla T4] [10de:1eb8] (rev a1) mdevNameSelector is a name of a Mediated device type that can identify specific types of Mediated devices on a host. You can see what mediated types a given PCI device supports by examining the contents of /sys/bus/pci/devices/SLOT:BUS:DOMAIN.FUNCTION/mdev_supported_types/TYPE/name . For example, if you have an NVIDIA T4 GPU on your system, and you substitute in the SLOT , BUS , DOMAIN , and FUNCTION values that are correct for your system into the above path name, you will see that a TYPE of nvidia-226 contains the selector string GRID T4-2A in its name file. Taking GRID T4-2A and specifying it as the mdevNameSelector allows KubeVirt to find a corresponding mediated device by matching it against /sys/class/mdev_bus/SLOT:BUS:DOMAIN.FUNCTION/$mdevUUID/mdev_type/name for some values of SLOT:BUS:DOMAIN.FUNCTION and $mdevUUID . External providers: externalResourceProvider field indicates that this resource is being provided by an external device plugin. In this case, KubeVirt will only permit the usage of this device in the cluster but will leave the allocation and monitoring to an external device plugin. Starting a Virtual Machine \u00b6 Host devices can be assigned to virtual machines via the gpus and hostDevices fields. The deviceNames can reference both PCI and Mediated device resource names. kind: VirtualMachineInstance spec: domain: devices: gpus: - deviceName: nvidia.com/TU104GL_Tesla_T4 name: gpu1 - deviceName: nvidia.com/GRID_T4-1Q name: gpu2 hostDevices: - deviceName: intel.com/qat name: quickaccess1","title":"Host Devices Assignment"},{"location":"virtual_machines/host-devices/#host-devices-assignment","text":"KubeVirt provides a mechanism for assigning host devices to a virtual machine. This mechanism is generic and allows various types of PCI devices, such as accelerators (including GPUs) or any other devices attached to a PCI bus, to be assigned. It also allows Linux Mediated devices , such as pre-configured virtual GPUs to be assigned using the same mechanism.","title":"Host Devices Assignment"},{"location":"virtual_machines/host-devices/#host-preparation-for-pci-passthrough","text":"Host Devices passthrough requires the virtualization extension and the IOMMU extension (Intel VT-d or AMD IOMMU) to be enabled in the BIOS. To enable IOMMU, depending on the CPU type, a host should be booted with an additional kernel parameter, intel_iommu=on for Intel and amd_iommu=on for AMD. Append these parameters to the end of the GRUB_CMDLINE_LINUX line in the grub configuration file. # vi /etc/default/grub ... GRUB_CMDLINE_LINUX=\"nofb splash=quiet console=tty0 ... intel_iommu=on ... # grub2-mkconfig -o /boot/grub2/grub.cfg # reboot The vfio-pci kernel module should be enabled on the host. # modprobe vfio-pci","title":"Host preparation for PCI Passthrough"},{"location":"virtual_machines/host-devices/#preparation-of-pci-devices-for-passthrough","text":"At this time, KubeVirt is only able to assign PCI devices that are using the vfio-pci driver. To prepare a specific device for device assignment, it should first be unbound from its original driver and bound to the vfio-pci driver. Find the PCI address of the desired device: $ lspci -DD|grep NVIDIA 0000.65:00.0 3D controller [0302]: NVIDIA Corporation TU104GL [Tesla T4] [10de:1eb8] (rev a1) Bind that device to the vfio-pci driver: echo 0000:65:00.0 > /sys/bus/pci/drivers/nvidia/unbind echo \"vfio-pci\" > /sys/bus/pci/devices/0000\\:65\\:00.0/driver_override echo 0000:65:00.0 > /sys/bus/pci/drivers/vfio-pci/bind","title":"Preparation of PCI devices for passthrough"},{"location":"virtual_machines/host-devices/#preparation-of-mediated-devices-such-as-vgpu","text":"At this time, configuration of a Mediated device (mdev) should be done according to the vendor directions. Once the mdev is configured, KubeVirt will be able to discover and use it for device assignment.","title":"Preparation of mediated devices such as vGPU"},{"location":"virtual_machines/host-devices/#listing-permitted-devices","text":"Administrators can control which host devices are exposed and permitted to be used in the cluster. Permitted host devices in the cluster will need to be allowlisted in KubeVirt CR by its vendor:product selector for PCI devices or mediated device names. configuration: permittedHostDevices: pciHostDevices: - pciVendorSelector: \"10DE:1EB8\" resourceName: \"nvidia.com/TU104GL_Tesla_T4\" externalResourceProvider: true - pciVendorSelector: \"8086:6F54\" resourceName: \"intel.com/qat\" mediatedDevices: - mdevNameSelector: \"GRID T4-1Q\" resourceName: \"nvidia.com/GRID_T4-1Q\" pciVendorSelector is a PCI vendor ID and product ID tuple in the form vendor_id:product_id . This tuple can identify specific types of devices on a host. For example, the identifier 10de:1eb8 , shown above, can be found using lspci . $ lspci -nnv|grep -i nvidia 65:00.0 3D controller [0302]: NVIDIA Corporation TU104GL [Tesla T4] [10de:1eb8] (rev a1) mdevNameSelector is a name of a Mediated device type that can identify specific types of Mediated devices on a host. You can see what mediated types a given PCI device supports by examining the contents of /sys/bus/pci/devices/SLOT:BUS:DOMAIN.FUNCTION/mdev_supported_types/TYPE/name . For example, if you have an NVIDIA T4 GPU on your system, and you substitute in the SLOT , BUS , DOMAIN , and FUNCTION values that are correct for your system into the above path name, you will see that a TYPE of nvidia-226 contains the selector string GRID T4-2A in its name file. Taking GRID T4-2A and specifying it as the mdevNameSelector allows KubeVirt to find a corresponding mediated device by matching it against /sys/class/mdev_bus/SLOT:BUS:DOMAIN.FUNCTION/$mdevUUID/mdev_type/name for some values of SLOT:BUS:DOMAIN.FUNCTION and $mdevUUID . External providers: externalResourceProvider field indicates that this resource is being provided by an external device plugin. In this case, KubeVirt will only permit the usage of this device in the cluster but will leave the allocation and monitoring to an external device plugin.","title":"Listing permitted devices"},{"location":"virtual_machines/host-devices/#starting-a-virtual-machine","text":"Host devices can be assigned to virtual machines via the gpus and hostDevices fields. The deviceNames can reference both PCI and Mediated device resource names. kind: VirtualMachineInstance spec: domain: devices: gpus: - deviceName: nvidia.com/TU104GL_Tesla_T4 name: gpu1 - deviceName: nvidia.com/GRID_T4-1Q name: gpu2 hostDevices: - deviceName: intel.com/qat name: quickaccess1","title":"Starting a Virtual Machine"},{"location":"virtual_machines/interfaces_and_networks/","text":"Interfaces and Networks \u00b6 Connecting a virtual machine to a network consists of two parts. First, networks are specified in spec.networks . Then, interfaces backed by the networks are added to the VM by specifying them in spec.domain.devices.interfaces . Each interface must have a corresponding network with the same name. An interface defines a virtual network interface of a virtual machine (also called a frontend). A network specifies the backend of an interface and declares which logical or physical device it is connected to (also called as backend). There are multiple ways of configuring an interface as well as a network . All possible configuration options are available in the Interface API Reference and Network API Reference . Backend \u00b6 Network backends are configured in spec.networks . A network must have a unique name. Additional fields declare which logical or physical device the network relates to. Each network should declare its type by defining one of the following fields: Type Description pod Default Kubernetes network multus Secondary network provided using Multus pod \u00b6 A pod network represents the default pod eth0 interface configured by cluster network solution that is present in each pod. kind: VM spec: domain: devices: interfaces: - name: default masquerade: {} networks: - name: default pod: {} # Stock pod network multus \u00b6 It is also possible to connect VMIs to secondary networks using Multus . This assumes that multus is installed across your cluster and a corresponding NetworkAttachmentDefinition CRD was created. The following example defines a network which uses the ovs-cni plugin , which will connect the VMI to Open vSwitch's bridge br1 and VLAN 100. Other CNI plugins such as ptp, bridge, macvlan or Flannel might be used as well. For their installation and usage refer to the respective project documentation. First the NetworkAttachmentDefinition needs to be created. That is usually done by an administrator. Users can then reference the definition. apiVersion: \"k8s.cni.cncf.io/v1\" kind: NetworkAttachmentDefinition metadata: name: ovs-vlan-100 spec: config: '{ \"cniVersion\": \"0.3.1\", \"type\": \"ovs\", \"bridge\": \"br1\", \"vlan\": 100 }' With following definition, the VMI will be connected to the default pod network and to the secondary Open vSwitch network. kind: VM spec: domain: devices: interfaces: - name: default masquerade: {} bootOrder: 1 # attempt to boot from an external tftp server dhcpOptions: bootFileName: default_image.bin tftpServerName: tftp.example.com - name: ovs-net bridge: {} bootOrder: 2 # if first attempt failed, try to PXE-boot from this L2 networks networks: - name: default pod: {} # Stock pod network - name: ovs-net multus: # Secondary multus network networkName: ovs-vlan-100 It is also possible to define a multus network as the default pod network with Multus . A version of multus after this Pull Request is required (currently master). Note the following: A multus default network and a pod network type are mutually exclusive. The virt-launcher pod that starts the VMI will not have the pod network configured. The multus delegate chosen as default must return at least one IP address. Create a NetworkAttachmentDefinition with IPAM. apiVersion: \"k8s.cni.cncf.io/v1\" kind: NetworkAttachmentDefinition metadata: name: macvlan-test spec: config: '{ \"type\": \"macvlan\", \"master\": \"eth0\", \"mode\": \"bridge\", \"ipam\": { \"type\": \"host-local\", \"subnet\": \"10.250.250.0/24\" } }' Define a VMI with a Multus network as the default. kind: VM spec: domain: devices: interfaces: - name: test1 bridge: {} networks: - name: test1 multus: # Multus network as default default: true networkName: macvlan-test Frontend \u00b6 Network interfaces are configured in spec.domain.devices.interfaces . They describe properties of virtual interfaces as \"seen\" inside guest instances. The same network backend may be connected to a virtual machine in multiple different ways, each with their own connectivity guarantees and characteristics. Each interface should declare its type by defining on of the following fields: Type Description bridge Connect using a linux bridge slirp Connect using QEMU user networking mode sriov Pass through a SR-IOV PCI device via vfio masquerade Connect using Iptables rules to nat the traffic Each interface may also have additional configuration fields that modify properties \"seen\" inside guest instances, as listed below: Name Format Default value Description model One of: e1000 , e1000e , ne2k_pci , pcnet , rtl8139 , virtio virtio NIC type macAddress ff:ff:ff:ff:ff:ff or FF-FF-FF-FF-FF-FF MAC address as seen inside the guest system, for example: de:ad:00:00:be:af ports empty List of ports to be forwarded to the virtual machine. pciAddress 0000:81:00.1 Set network interface PCI address, for example: 0000:81:00.1 kind: VM spec: domain: devices: interfaces: - name: default model: e1000 # expose e1000 NIC to the guest masquerade: {} # connect through a masquerade ports: - name: http port: 80 networks: - name: default pod: {} Note: If a specific MAC address is configured for a virtual machine interface, it's passed to the underlying CNI plugin that is expected to configure the backend to allow for this particular MAC address. Not every plugin has native support for custom MAC addresses. Note: For some CNI plugins without native support for custom MAC addresses, there is a workaround, which is to use the tuning CNI plugin to adjust pod interface MAC address. This can be used as follows: apiVersion: \"k8s.cni.cncf.io/v1\" kind: NetworkAttachmentDefinition metadata: name: ptp-mac spec: config: '{ \"cniVersion\": \"0.3.1\", \"name\": \"ptp-mac\", \"plugins\": [ { \"type\": \"ptp\", \"ipam\": { \"type\": \"host-local\", \"subnet\": \"10.1.1.0/24\" } }, { \"type\": \"tuning\" } ] }' This approach may not work for all plugins. For example, OKD SDN is not compatible with tuning plugin. Plugins that handle custom MAC addresses natively: ovs . Plugins that are compatible with tuning plugin: flannel , ptp , bridge . Plugins that don't need special MAC address treatment: sriov (in vfio mode). Ports \u00b6 Declare ports listen by the virtual machine Note: When using the slirp interface only the configured ports will be forwarded to the virtual machine. Name Format Required Description name no Name port 1 - 65535 yes Port to expose protocol TCP,UDP no Connection protocol Tip: Use e1000 model if your guest image doesn't ship with virtio drivers. Note: Windows machines need the latest virtio network driver to configure the correct MTU on the interface. If spec.domain.devices.interfaces is omitted, the virtual machine is connected using the default pod network interface of bridge type. If you'd like to have a virtual machine instance without any network connectivity, you can use the autoattachPodInterface field as follows: kind: VM spec: domain: devices: autoattachPodInterface: false bridge \u00b6 In bridge mode, virtual machines are connected to the network backend through a linux \"bridge\". The pod network IPv4 address is delegated to the virtual machine via DHCPv4. The virtual machine should be configured to use DHCP to acquire IPv4 addresses. Note: If a specific MAC address is not configured in the virtual machine interface spec the MAC address from the relevant pod interface is delegated to the virtual machine. kind: VM spec: domain: devices: interfaces: - name: red bridge: {} # connect through a bridge networks: - name: red multus: networkName: red At this time, bridge mode doesn't support additional configuration fields. Note: due to IPv4 address delegation, in bridge mode the pod doesn't have an IP address configured, which may introduce issues with third-party solutions that may rely on it. For example, Istio may not work in this mode. Note: admin can forbid using bridge interface type for pod networks via a designated configuration flag. To achieve it, the admin should set the following option to false : apiVersion: kubevirt.io/v1alpha3 kind: Kubevirt metadata: name: kubevirt namespace: kubevirt spec: configuration: networkConfiguration: permitBridgeInterfaceOnPodNetwork: \"false\" Note: binding the pod network using bridge interface type may cause issues. Other than the third-party issue mentioned in the above note, live migration is not allowed with a pod network binding of bridge interface type, and also some CNI plugins might not allow to use a custom MAC address for your VM instances. If you think you may be affected by any of issues mentioned above, consider changing the default interface type to masquerade , and disabling the bridge type for pod network, as shown in the example above. slirp \u00b6 In slirp mode, virtual machines are connected to the network backend using QEMU user networking mode. In this mode, QEMU allocates internal IP addresses to virtual machines and hides them behind NAT. kind: VM spec: domain: devices: interfaces: - name: red slirp: {} # connect using SLIRP mode networks: - name: red pod: {} At this time, slirp mode doesn't support additional configuration fields. Note: in slirp mode, the only supported protocols are TCP and UDP. ICMP is not supported. More information about SLIRP mode can be found in QEMU Wiki . masquerade \u00b6 In masquerade mode, KubeVirt allocates internal IP addresses to virtual machines and hides them behind NAT. All the traffic exiting virtual machines is \"NAT'ed\" using pod IP addresses. A guest operating system should be configured to use DHCP to acquire IPv4 addresses. To allow traffic of specific ports into virtual machines, the template ports section of the interface should be configured as follows. If the ports section is missing, all ports forwarded into the VM. kind: VM spec: domain: devices: interfaces: - name: red masquerade: {} # connect using masquerade mode ports: - port: 80 # allow incoming traffic on port 80 to get into the virtual machine networks: - name: red pod: {} Note: Masquerade is only allowed to connect to the pod network. Note: The network CIDR can be configured in the pod network section using the vmNetworkCIDR attribute. masquerade - IPv4 and IPv6 dual-stack support \u00b6 It is currently experimental, but masquerade mode can be used in IPv4 and IPv6 dual-stack clusters to provide a VM with an IP connectivity over both protocols. As with the IPv4 masquerade mode, the VM can be contacted using the pod's IP address - which will be in this case two IP addresses, one IPv4 and one IPv6. Outgoing traffic is also \"NAT'ed\" to the pod's respective IP address from the given family. Unlike in IPv4, the configuration of the IPv6 address and the default route is not automatic; it should be configured via cloud init, as shown below: kind: VM spec: domain: devices: disks: - disk: bus: virtio name: cloudinitdisk interfaces: - name: red masquerade: {} # connect using masquerade mode ports: - port: 80 # allow incoming traffic on port 80 to get into the virtual machine networks: - name: red pod: {} volumes: - cloudInitNoCloud: networkData: | version: 2 ethernets: eth0: dhcp4: true addresses: [ fd10:0:2::2/120 ] gateway6: fd10:0:2::1 userData: |- #!/bin/bash echo \"fedora\" |passwd fedora --stdin Note: The IPv6 address for the VM and default gateway must be the ones shown above. virtio-net multiqueue \u00b6 Setting the networkInterfaceMultiqueue to true will enable the multi-queue functionality, increasing the number of vhost queue, for interfaces configured with a virtio model. kind: VM spec: domain: devices: networkInterfaceMultiqueue: true Users of a Virtual Machine with multiple vCPUs may benefit of increased network throughput and performance. Currently, the number of queues is being determined by the number of vCPUs of a VM. This is because multi-queue support optimizes RX interrupt affinity and TX queue selection in order to make a specific queue private to a specific vCPU. Without enabling the feature, network performance does not scale as the number of vCPUs increases. Guests cannot transmit or retrieve packets in parallel, as virtio-net has only one TX and RX queue. NOTE : Although the virtio-net multiqueue feature provides a performance benefit, it has some limitations and therefore should not be unconditionally enabled Some known limitations \u00b6 Guest OS is limited to ~200 MSI vectors. Each NIC queue requires a MSI vector, as well as any virtio device or assigned PCI device. Defining an instance with multiple virtio NICs and vCPUs might lead to a possibility of hitting the guest MSI limit. virtio-net multiqueue works well for incoming traffic, but can occasionally cause a performance degradation, for outgoing traffic. Specifically, this may occur when sending packets under 1,500 bytes over the Transmission Control Protocol (TCP) stream. Enabling virtio-net multiqueue increases the total network throughput, but in parallel it also increases the CPU consumption. Enabling virtio-net multiqueue in the host QEMU config, does not enable the functionality in the guest OS. The guest OS administrator needs to manually turn it on for each guest NIC that requires this feature, using ethtool. MSI vectors would still be consumed (wasted), if multiqueue was enabled in the host, but has not been enabled in the guest OS by the administrator. In case the number of vNICs in a guest instance is proportional to the number of vCPUs, enabling the multiqueue feature is less important. Each virtio-net queue consumes 64 KiB of kernel memory for the vhost driver. NOTE : Virtio-net multiqueue should be enabled in the guest OS manually, using ethtool. For example: ethtool -L <NIC> combined #num_of_queues More information please refer to KVM/QEMU MultiQueue . sriov \u00b6 In sriov mode, virtual machines are directly exposed to an SR-IOV PCI device, usually allocated by Intel SR-IOV device plugin . The device is passed through into the guest operating system as a host device, using the vfio userspace interface, to maintain high networking performance. How to expose SR-IOV VFs to KubeVirt \u00b6 To simplify procedure, please use OpenShift SR-IOV operator to deploy and configure SR-IOV components in your cluster. On how to use the operator, please refer to their respective documentation . Note: KubeVirt relies on VFIO userspace driver to pass PCI devices into VMI guest. Because of that, when configuring SR-IOV operator policies, make sure you define a pool of VF resources that uses driver: vfio . Once the operator is deployed, an SriovNetworkNodePolicy must be provisioned, in which the list of SR-IOV devices to expose (with respective configurations) is defined. Please refer to the following SriovNetworkNodePolicy for an example: apiVersion: sriovnetwork.openshift.io/v1 kind: SriovNetworkNodePolicy metadata: name: policy-1 namespace: sriov-network-operator spec: deviceType: vfio-pci mtu: 9000 nicSelector: pfNames: - ens1f0 nodeSelector: sriov: \"true\" numVfs: 8 priority: 90 resourceName: sriov-nic The policy above will configure the SR-IOV device plugin, allowing the PF named ens1f0 to be exposed in the SRIOV capable nodes as a resource named sriov-nic . Start an SR-IOV VM \u00b6 Once all the SR-IOV components are deployed, it is needed to indicate how to configure the SR-IOV network. Refer to the following SriovNetwork for an example: apiVersion: sriovnetwork.openshift.io/v1 kind: SriovNetwork metadata: name: sriov-net namespace: sriov-network-operator spec: ipam: | {} networkNamespace: default resourceName: sriov-nic spoofChk: \"off\" Finally, to create a VM that will attach to the aforementioned Network, refer to the following VMI spec: --- apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance metadata: labels: special: vmi-perf name: vmi-perf spec: domain: cpu: sockets: 2 cores: 1 threads: 1 dedicatedCpuPlacement: true resources: requests: memory: \"4Gi\" limits: memory: \"4Gi\" devices: disks: - disk: bus: virtio name: containerdisk - disk: bus: virtio name: cloudinitdisk interfaces: - masquerade: {} name: default - name: sriov-net sriov: {} rng: {} machine: type: \"\" networks: - name: default pod: {} - multus: networkName: default/sriov-net name: sriov-net terminationGracePeriodSeconds: 0 volumes: - containerDisk: image: docker.io/kubevirt/fedora-cloud-container-disk-demo:latest name: containerdisk - cloudInitNoCloud: userData: | #!/bin/bash echo \"centos\" |passwd centos --stdin dhclient eth1 name: cloudinitdisk Note: for some NICs (e.g. Mellanox), the kernel module needs to be installed in the guest VM. Note: Placement on dedicated CPUs can only be achieved if the Kubernetes CPU manager is running on the SR-IOV capable workers. For further details please refer to the dedicated cpu resources documentation . Macvtap \u00b6 In macvtap mode, virtual machines are directly exposed to the Kubernetes nodes L2 network. This is achieved by 'extending' an existing network interface with a virtual device that has its own MAC address. Macvtap interfaces are feature gated; to enable the feature, follow these instructions, in order to activate the Macvtap feature gate (case sensitive). Limitations \u00b6 Live migration is not seamless, see issue #5912 How to expose host interface to the macvtap device plugin \u00b6 To simplify the procedure, please use the Cluster Network Addons Operator to deploy and configure the macvtap components in your cluster. The aforementioned operator effectively deploys the macvtap-cni cni / device plugin combo. There are two different alternatives to configure which host interfaces get exposed to the user, enabling them to create macvtap interfaces on top of; - select the host interfaces: indicates which host interfaces are exposed. - expose all interfaces: all interfaces of all hosts are exposed. Both options are configured via the macvtap-deviceplugin-config ConfigMap, and more information on how to configure it can be found in the macvtap-cni repo. You can find a minimal example, in which the eth0 interface of the Kubernetes nodes is exposed, via the master attribute. kind: ConfigMap apiVersion: v1 metadata: name: macvtap-deviceplugin-config data: DP_MACVTAP_CONF: | [ { \"name\" : \"dataplane\", \"master\" : \"eth0\", \"mode\" : \"bridge\", \"capacity\" : 50 }, ] This step can be omitted, since the default configuration of the aforementioned ConfigMap is to expose all host interfaces (which is represented by the following configuration): kind: ConfigMap apiVersion: v1 metadata: name: macvtap-deviceplugin-config data: DP_MACVTAP_CONF: '[]' Start a VM with macvtap interfaces \u00b6 Once the macvtap components are deployed, it is needed to indicate how to configure the macvtap network. Refer to the following NetworkAttachmentDefinition for a simple example: --- kind: NetworkAttachmentDefinition apiVersion: k8s.cni.cncf.io/v1 metadata: name: macvtapnetwork annotations: k8s.v1.cni.cncf.io/resourceName: macvtap.network.kubevirt.io/eth0 spec: config: '{ \"cniVersion\": \"0.3.1\", \"name\": \"macvtapnetwork\", \"type\": \"macvtap\" \"mtu\": 1500 }' The requested k8s.v1.cni.cncf.io/resourceName annotation must point to an exposed host interface (via the master attribute, on the macvtap-deviceplugin-config ConfigMap ). Finally, to create a VM that will attach to the aforementioned Network, refer to the following VMI spec: --- apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance metadata: labels: special: vmi-host-network name: vmi-host-network spec: domain: devices: disks: - disk: bus: virtio name: containerdisk - disk: bus: virtio name: cloudinitdisk interfaces: - macvtap: {} name: hostnetwork rng: {} machine: type: \"\" resources: requests: memory: 1024M networks: - multus: networkName: macvtapnetwork name: hostnetwork terminationGracePeriodSeconds: 0 volumes: - containerDisk: image: docker.io/kubevirt/fedora-cloud-container-disk-demo:devel name: containerdisk - cloudInitNoCloud: userData: |- #!/bin/bash echo \"fedora\" |passwd fedora --stdin name: cloudinitdisk The requested multus networkName - i.e. macvtapnetwork - must match the name of the provisioned NetworkAttachmentDefinition . Note: VMIs with macvtap interfaces can be migrated, but their MAC addresses must be statically set. Security \u00b6 MAC spoof check \u00b6 MAC spoofing refers to the ability to generate traffic with an arbitrary source MAC address. An attacker may use this option to generate attacks on the network. In order to protect against such scenarios, it is possible to enable the mac-spoof-check support in CNI plugins that support it. The pod primary network which is served by the cluster network provider is not covered by this documentation. Please refer to the relevant provider to check how to enable spoofing check. The following text refers to the secondary networks, served using multus. There are two known CNI plugins that support mac-spoof-check: sriov-cni : Through the spoofchk parameter . cnv-bridge: Through the macspoofchk . Note: cnv-bridge is provided by CNAO . The bridge-cni is planned to support the macspoofchk options as well. The configuration is to be done on the NetworkAttachmentDefinition by the operator and any interface that refers to it, will have this feature enabled. Below is an example of using the cnv-bridge CNI with macspoofchk enabled: apiVersion: \"k8s.cni.cncf.io/v1\" kind: NetworkAttachmentDefinition metadata: name: br-spoof-check spec: config: '{ \"cniVersion\": \"0.3.1\", \"name\": \"br-spoof-check\", \"type\": \"cnv-bridge\", \"bridge\": \"br10\", \"macspoofchk\": true }' On the VMI, the network section should point to this NetworkAttachmentDefinition by name: networks: - name: default pod: {} - multus: networkName: br-spoof-check name: br10 Limitations \u00b6 The cnv-bridge CNI supports mac-spoof-check through nftables, therefore the node must support nftables and have the nft binary deployed.","title":"Interfaces and Networks"},{"location":"virtual_machines/interfaces_and_networks/#interfaces-and-networks","text":"Connecting a virtual machine to a network consists of two parts. First, networks are specified in spec.networks . Then, interfaces backed by the networks are added to the VM by specifying them in spec.domain.devices.interfaces . Each interface must have a corresponding network with the same name. An interface defines a virtual network interface of a virtual machine (also called a frontend). A network specifies the backend of an interface and declares which logical or physical device it is connected to (also called as backend). There are multiple ways of configuring an interface as well as a network . All possible configuration options are available in the Interface API Reference and Network API Reference .","title":"Interfaces and Networks"},{"location":"virtual_machines/interfaces_and_networks/#backend","text":"Network backends are configured in spec.networks . A network must have a unique name. Additional fields declare which logical or physical device the network relates to. Each network should declare its type by defining one of the following fields: Type Description pod Default Kubernetes network multus Secondary network provided using Multus","title":"Backend"},{"location":"virtual_machines/interfaces_and_networks/#pod","text":"A pod network represents the default pod eth0 interface configured by cluster network solution that is present in each pod. kind: VM spec: domain: devices: interfaces: - name: default masquerade: {} networks: - name: default pod: {} # Stock pod network","title":"pod"},{"location":"virtual_machines/interfaces_and_networks/#multus","text":"It is also possible to connect VMIs to secondary networks using Multus . This assumes that multus is installed across your cluster and a corresponding NetworkAttachmentDefinition CRD was created. The following example defines a network which uses the ovs-cni plugin , which will connect the VMI to Open vSwitch's bridge br1 and VLAN 100. Other CNI plugins such as ptp, bridge, macvlan or Flannel might be used as well. For their installation and usage refer to the respective project documentation. First the NetworkAttachmentDefinition needs to be created. That is usually done by an administrator. Users can then reference the definition. apiVersion: \"k8s.cni.cncf.io/v1\" kind: NetworkAttachmentDefinition metadata: name: ovs-vlan-100 spec: config: '{ \"cniVersion\": \"0.3.1\", \"type\": \"ovs\", \"bridge\": \"br1\", \"vlan\": 100 }' With following definition, the VMI will be connected to the default pod network and to the secondary Open vSwitch network. kind: VM spec: domain: devices: interfaces: - name: default masquerade: {} bootOrder: 1 # attempt to boot from an external tftp server dhcpOptions: bootFileName: default_image.bin tftpServerName: tftp.example.com - name: ovs-net bridge: {} bootOrder: 2 # if first attempt failed, try to PXE-boot from this L2 networks networks: - name: default pod: {} # Stock pod network - name: ovs-net multus: # Secondary multus network networkName: ovs-vlan-100 It is also possible to define a multus network as the default pod network with Multus . A version of multus after this Pull Request is required (currently master). Note the following: A multus default network and a pod network type are mutually exclusive. The virt-launcher pod that starts the VMI will not have the pod network configured. The multus delegate chosen as default must return at least one IP address. Create a NetworkAttachmentDefinition with IPAM. apiVersion: \"k8s.cni.cncf.io/v1\" kind: NetworkAttachmentDefinition metadata: name: macvlan-test spec: config: '{ \"type\": \"macvlan\", \"master\": \"eth0\", \"mode\": \"bridge\", \"ipam\": { \"type\": \"host-local\", \"subnet\": \"10.250.250.0/24\" } }' Define a VMI with a Multus network as the default. kind: VM spec: domain: devices: interfaces: - name: test1 bridge: {} networks: - name: test1 multus: # Multus network as default default: true networkName: macvlan-test","title":"multus"},{"location":"virtual_machines/interfaces_and_networks/#frontend","text":"Network interfaces are configured in spec.domain.devices.interfaces . They describe properties of virtual interfaces as \"seen\" inside guest instances. The same network backend may be connected to a virtual machine in multiple different ways, each with their own connectivity guarantees and characteristics. Each interface should declare its type by defining on of the following fields: Type Description bridge Connect using a linux bridge slirp Connect using QEMU user networking mode sriov Pass through a SR-IOV PCI device via vfio masquerade Connect using Iptables rules to nat the traffic Each interface may also have additional configuration fields that modify properties \"seen\" inside guest instances, as listed below: Name Format Default value Description model One of: e1000 , e1000e , ne2k_pci , pcnet , rtl8139 , virtio virtio NIC type macAddress ff:ff:ff:ff:ff:ff or FF-FF-FF-FF-FF-FF MAC address as seen inside the guest system, for example: de:ad:00:00:be:af ports empty List of ports to be forwarded to the virtual machine. pciAddress 0000:81:00.1 Set network interface PCI address, for example: 0000:81:00.1 kind: VM spec: domain: devices: interfaces: - name: default model: e1000 # expose e1000 NIC to the guest masquerade: {} # connect through a masquerade ports: - name: http port: 80 networks: - name: default pod: {} Note: If a specific MAC address is configured for a virtual machine interface, it's passed to the underlying CNI plugin that is expected to configure the backend to allow for this particular MAC address. Not every plugin has native support for custom MAC addresses. Note: For some CNI plugins without native support for custom MAC addresses, there is a workaround, which is to use the tuning CNI plugin to adjust pod interface MAC address. This can be used as follows: apiVersion: \"k8s.cni.cncf.io/v1\" kind: NetworkAttachmentDefinition metadata: name: ptp-mac spec: config: '{ \"cniVersion\": \"0.3.1\", \"name\": \"ptp-mac\", \"plugins\": [ { \"type\": \"ptp\", \"ipam\": { \"type\": \"host-local\", \"subnet\": \"10.1.1.0/24\" } }, { \"type\": \"tuning\" } ] }' This approach may not work for all plugins. For example, OKD SDN is not compatible with tuning plugin. Plugins that handle custom MAC addresses natively: ovs . Plugins that are compatible with tuning plugin: flannel , ptp , bridge . Plugins that don't need special MAC address treatment: sriov (in vfio mode).","title":"Frontend"},{"location":"virtual_machines/interfaces_and_networks/#ports","text":"Declare ports listen by the virtual machine Note: When using the slirp interface only the configured ports will be forwarded to the virtual machine. Name Format Required Description name no Name port 1 - 65535 yes Port to expose protocol TCP,UDP no Connection protocol Tip: Use e1000 model if your guest image doesn't ship with virtio drivers. Note: Windows machines need the latest virtio network driver to configure the correct MTU on the interface. If spec.domain.devices.interfaces is omitted, the virtual machine is connected using the default pod network interface of bridge type. If you'd like to have a virtual machine instance without any network connectivity, you can use the autoattachPodInterface field as follows: kind: VM spec: domain: devices: autoattachPodInterface: false","title":"Ports"},{"location":"virtual_machines/interfaces_and_networks/#bridge","text":"In bridge mode, virtual machines are connected to the network backend through a linux \"bridge\". The pod network IPv4 address is delegated to the virtual machine via DHCPv4. The virtual machine should be configured to use DHCP to acquire IPv4 addresses. Note: If a specific MAC address is not configured in the virtual machine interface spec the MAC address from the relevant pod interface is delegated to the virtual machine. kind: VM spec: domain: devices: interfaces: - name: red bridge: {} # connect through a bridge networks: - name: red multus: networkName: red At this time, bridge mode doesn't support additional configuration fields. Note: due to IPv4 address delegation, in bridge mode the pod doesn't have an IP address configured, which may introduce issues with third-party solutions that may rely on it. For example, Istio may not work in this mode. Note: admin can forbid using bridge interface type for pod networks via a designated configuration flag. To achieve it, the admin should set the following option to false : apiVersion: kubevirt.io/v1alpha3 kind: Kubevirt metadata: name: kubevirt namespace: kubevirt spec: configuration: networkConfiguration: permitBridgeInterfaceOnPodNetwork: \"false\" Note: binding the pod network using bridge interface type may cause issues. Other than the third-party issue mentioned in the above note, live migration is not allowed with a pod network binding of bridge interface type, and also some CNI plugins might not allow to use a custom MAC address for your VM instances. If you think you may be affected by any of issues mentioned above, consider changing the default interface type to masquerade , and disabling the bridge type for pod network, as shown in the example above.","title":"bridge"},{"location":"virtual_machines/interfaces_and_networks/#slirp","text":"In slirp mode, virtual machines are connected to the network backend using QEMU user networking mode. In this mode, QEMU allocates internal IP addresses to virtual machines and hides them behind NAT. kind: VM spec: domain: devices: interfaces: - name: red slirp: {} # connect using SLIRP mode networks: - name: red pod: {} At this time, slirp mode doesn't support additional configuration fields. Note: in slirp mode, the only supported protocols are TCP and UDP. ICMP is not supported. More information about SLIRP mode can be found in QEMU Wiki .","title":"slirp"},{"location":"virtual_machines/interfaces_and_networks/#masquerade","text":"In masquerade mode, KubeVirt allocates internal IP addresses to virtual machines and hides them behind NAT. All the traffic exiting virtual machines is \"NAT'ed\" using pod IP addresses. A guest operating system should be configured to use DHCP to acquire IPv4 addresses. To allow traffic of specific ports into virtual machines, the template ports section of the interface should be configured as follows. If the ports section is missing, all ports forwarded into the VM. kind: VM spec: domain: devices: interfaces: - name: red masquerade: {} # connect using masquerade mode ports: - port: 80 # allow incoming traffic on port 80 to get into the virtual machine networks: - name: red pod: {} Note: Masquerade is only allowed to connect to the pod network. Note: The network CIDR can be configured in the pod network section using the vmNetworkCIDR attribute.","title":"masquerade"},{"location":"virtual_machines/interfaces_and_networks/#masquerade-ipv4-and-ipv6-dual-stack-support","text":"It is currently experimental, but masquerade mode can be used in IPv4 and IPv6 dual-stack clusters to provide a VM with an IP connectivity over both protocols. As with the IPv4 masquerade mode, the VM can be contacted using the pod's IP address - which will be in this case two IP addresses, one IPv4 and one IPv6. Outgoing traffic is also \"NAT'ed\" to the pod's respective IP address from the given family. Unlike in IPv4, the configuration of the IPv6 address and the default route is not automatic; it should be configured via cloud init, as shown below: kind: VM spec: domain: devices: disks: - disk: bus: virtio name: cloudinitdisk interfaces: - name: red masquerade: {} # connect using masquerade mode ports: - port: 80 # allow incoming traffic on port 80 to get into the virtual machine networks: - name: red pod: {} volumes: - cloudInitNoCloud: networkData: | version: 2 ethernets: eth0: dhcp4: true addresses: [ fd10:0:2::2/120 ] gateway6: fd10:0:2::1 userData: |- #!/bin/bash echo \"fedora\" |passwd fedora --stdin Note: The IPv6 address for the VM and default gateway must be the ones shown above.","title":"masquerade - IPv4 and IPv6 dual-stack support"},{"location":"virtual_machines/interfaces_and_networks/#virtio-net-multiqueue","text":"Setting the networkInterfaceMultiqueue to true will enable the multi-queue functionality, increasing the number of vhost queue, for interfaces configured with a virtio model. kind: VM spec: domain: devices: networkInterfaceMultiqueue: true Users of a Virtual Machine with multiple vCPUs may benefit of increased network throughput and performance. Currently, the number of queues is being determined by the number of vCPUs of a VM. This is because multi-queue support optimizes RX interrupt affinity and TX queue selection in order to make a specific queue private to a specific vCPU. Without enabling the feature, network performance does not scale as the number of vCPUs increases. Guests cannot transmit or retrieve packets in parallel, as virtio-net has only one TX and RX queue. NOTE : Although the virtio-net multiqueue feature provides a performance benefit, it has some limitations and therefore should not be unconditionally enabled","title":"virtio-net multiqueue"},{"location":"virtual_machines/interfaces_and_networks/#some-known-limitations","text":"Guest OS is limited to ~200 MSI vectors. Each NIC queue requires a MSI vector, as well as any virtio device or assigned PCI device. Defining an instance with multiple virtio NICs and vCPUs might lead to a possibility of hitting the guest MSI limit. virtio-net multiqueue works well for incoming traffic, but can occasionally cause a performance degradation, for outgoing traffic. Specifically, this may occur when sending packets under 1,500 bytes over the Transmission Control Protocol (TCP) stream. Enabling virtio-net multiqueue increases the total network throughput, but in parallel it also increases the CPU consumption. Enabling virtio-net multiqueue in the host QEMU config, does not enable the functionality in the guest OS. The guest OS administrator needs to manually turn it on for each guest NIC that requires this feature, using ethtool. MSI vectors would still be consumed (wasted), if multiqueue was enabled in the host, but has not been enabled in the guest OS by the administrator. In case the number of vNICs in a guest instance is proportional to the number of vCPUs, enabling the multiqueue feature is less important. Each virtio-net queue consumes 64 KiB of kernel memory for the vhost driver. NOTE : Virtio-net multiqueue should be enabled in the guest OS manually, using ethtool. For example: ethtool -L <NIC> combined #num_of_queues More information please refer to KVM/QEMU MultiQueue .","title":"Some known limitations"},{"location":"virtual_machines/interfaces_and_networks/#sriov","text":"In sriov mode, virtual machines are directly exposed to an SR-IOV PCI device, usually allocated by Intel SR-IOV device plugin . The device is passed through into the guest operating system as a host device, using the vfio userspace interface, to maintain high networking performance.","title":"sriov"},{"location":"virtual_machines/interfaces_and_networks/#how-to-expose-sr-iov-vfs-to-kubevirt","text":"To simplify procedure, please use OpenShift SR-IOV operator to deploy and configure SR-IOV components in your cluster. On how to use the operator, please refer to their respective documentation . Note: KubeVirt relies on VFIO userspace driver to pass PCI devices into VMI guest. Because of that, when configuring SR-IOV operator policies, make sure you define a pool of VF resources that uses driver: vfio . Once the operator is deployed, an SriovNetworkNodePolicy must be provisioned, in which the list of SR-IOV devices to expose (with respective configurations) is defined. Please refer to the following SriovNetworkNodePolicy for an example: apiVersion: sriovnetwork.openshift.io/v1 kind: SriovNetworkNodePolicy metadata: name: policy-1 namespace: sriov-network-operator spec: deviceType: vfio-pci mtu: 9000 nicSelector: pfNames: - ens1f0 nodeSelector: sriov: \"true\" numVfs: 8 priority: 90 resourceName: sriov-nic The policy above will configure the SR-IOV device plugin, allowing the PF named ens1f0 to be exposed in the SRIOV capable nodes as a resource named sriov-nic .","title":"How to expose SR-IOV VFs to KubeVirt"},{"location":"virtual_machines/interfaces_and_networks/#start-an-sr-iov-vm","text":"Once all the SR-IOV components are deployed, it is needed to indicate how to configure the SR-IOV network. Refer to the following SriovNetwork for an example: apiVersion: sriovnetwork.openshift.io/v1 kind: SriovNetwork metadata: name: sriov-net namespace: sriov-network-operator spec: ipam: | {} networkNamespace: default resourceName: sriov-nic spoofChk: \"off\" Finally, to create a VM that will attach to the aforementioned Network, refer to the following VMI spec: --- apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance metadata: labels: special: vmi-perf name: vmi-perf spec: domain: cpu: sockets: 2 cores: 1 threads: 1 dedicatedCpuPlacement: true resources: requests: memory: \"4Gi\" limits: memory: \"4Gi\" devices: disks: - disk: bus: virtio name: containerdisk - disk: bus: virtio name: cloudinitdisk interfaces: - masquerade: {} name: default - name: sriov-net sriov: {} rng: {} machine: type: \"\" networks: - name: default pod: {} - multus: networkName: default/sriov-net name: sriov-net terminationGracePeriodSeconds: 0 volumes: - containerDisk: image: docker.io/kubevirt/fedora-cloud-container-disk-demo:latest name: containerdisk - cloudInitNoCloud: userData: | #!/bin/bash echo \"centos\" |passwd centos --stdin dhclient eth1 name: cloudinitdisk Note: for some NICs (e.g. Mellanox), the kernel module needs to be installed in the guest VM. Note: Placement on dedicated CPUs can only be achieved if the Kubernetes CPU manager is running on the SR-IOV capable workers. For further details please refer to the dedicated cpu resources documentation .","title":"Start an SR-IOV VM"},{"location":"virtual_machines/interfaces_and_networks/#macvtap","text":"In macvtap mode, virtual machines are directly exposed to the Kubernetes nodes L2 network. This is achieved by 'extending' an existing network interface with a virtual device that has its own MAC address. Macvtap interfaces are feature gated; to enable the feature, follow these instructions, in order to activate the Macvtap feature gate (case sensitive).","title":"Macvtap"},{"location":"virtual_machines/interfaces_and_networks/#limitations","text":"Live migration is not seamless, see issue #5912","title":"Limitations"},{"location":"virtual_machines/interfaces_and_networks/#how-to-expose-host-interface-to-the-macvtap-device-plugin","text":"To simplify the procedure, please use the Cluster Network Addons Operator to deploy and configure the macvtap components in your cluster. The aforementioned operator effectively deploys the macvtap-cni cni / device plugin combo. There are two different alternatives to configure which host interfaces get exposed to the user, enabling them to create macvtap interfaces on top of; - select the host interfaces: indicates which host interfaces are exposed. - expose all interfaces: all interfaces of all hosts are exposed. Both options are configured via the macvtap-deviceplugin-config ConfigMap, and more information on how to configure it can be found in the macvtap-cni repo. You can find a minimal example, in which the eth0 interface of the Kubernetes nodes is exposed, via the master attribute. kind: ConfigMap apiVersion: v1 metadata: name: macvtap-deviceplugin-config data: DP_MACVTAP_CONF: | [ { \"name\" : \"dataplane\", \"master\" : \"eth0\", \"mode\" : \"bridge\", \"capacity\" : 50 }, ] This step can be omitted, since the default configuration of the aforementioned ConfigMap is to expose all host interfaces (which is represented by the following configuration): kind: ConfigMap apiVersion: v1 metadata: name: macvtap-deviceplugin-config data: DP_MACVTAP_CONF: '[]'","title":"How to expose host interface to the macvtap device plugin"},{"location":"virtual_machines/interfaces_and_networks/#start-a-vm-with-macvtap-interfaces","text":"Once the macvtap components are deployed, it is needed to indicate how to configure the macvtap network. Refer to the following NetworkAttachmentDefinition for a simple example: --- kind: NetworkAttachmentDefinition apiVersion: k8s.cni.cncf.io/v1 metadata: name: macvtapnetwork annotations: k8s.v1.cni.cncf.io/resourceName: macvtap.network.kubevirt.io/eth0 spec: config: '{ \"cniVersion\": \"0.3.1\", \"name\": \"macvtapnetwork\", \"type\": \"macvtap\" \"mtu\": 1500 }' The requested k8s.v1.cni.cncf.io/resourceName annotation must point to an exposed host interface (via the master attribute, on the macvtap-deviceplugin-config ConfigMap ). Finally, to create a VM that will attach to the aforementioned Network, refer to the following VMI spec: --- apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance metadata: labels: special: vmi-host-network name: vmi-host-network spec: domain: devices: disks: - disk: bus: virtio name: containerdisk - disk: bus: virtio name: cloudinitdisk interfaces: - macvtap: {} name: hostnetwork rng: {} machine: type: \"\" resources: requests: memory: 1024M networks: - multus: networkName: macvtapnetwork name: hostnetwork terminationGracePeriodSeconds: 0 volumes: - containerDisk: image: docker.io/kubevirt/fedora-cloud-container-disk-demo:devel name: containerdisk - cloudInitNoCloud: userData: |- #!/bin/bash echo \"fedora\" |passwd fedora --stdin name: cloudinitdisk The requested multus networkName - i.e. macvtapnetwork - must match the name of the provisioned NetworkAttachmentDefinition . Note: VMIs with macvtap interfaces can be migrated, but their MAC addresses must be statically set.","title":"Start a VM with macvtap interfaces"},{"location":"virtual_machines/interfaces_and_networks/#security","text":"","title":"Security"},{"location":"virtual_machines/interfaces_and_networks/#mac-spoof-check","text":"MAC spoofing refers to the ability to generate traffic with an arbitrary source MAC address. An attacker may use this option to generate attacks on the network. In order to protect against such scenarios, it is possible to enable the mac-spoof-check support in CNI plugins that support it. The pod primary network which is served by the cluster network provider is not covered by this documentation. Please refer to the relevant provider to check how to enable spoofing check. The following text refers to the secondary networks, served using multus. There are two known CNI plugins that support mac-spoof-check: sriov-cni : Through the spoofchk parameter . cnv-bridge: Through the macspoofchk . Note: cnv-bridge is provided by CNAO . The bridge-cni is planned to support the macspoofchk options as well. The configuration is to be done on the NetworkAttachmentDefinition by the operator and any interface that refers to it, will have this feature enabled. Below is an example of using the cnv-bridge CNI with macspoofchk enabled: apiVersion: \"k8s.cni.cncf.io/v1\" kind: NetworkAttachmentDefinition metadata: name: br-spoof-check spec: config: '{ \"cniVersion\": \"0.3.1\", \"name\": \"br-spoof-check\", \"type\": \"cnv-bridge\", \"bridge\": \"br10\", \"macspoofchk\": true }' On the VMI, the network section should point to this NetworkAttachmentDefinition by name: networks: - name: default pod: {} - multus: networkName: br-spoof-check name: br10","title":"MAC spoof check"},{"location":"virtual_machines/interfaces_and_networks/#limitations_1","text":"The cnv-bridge CNI supports mac-spoof-check through nftables, therefore the node must support nftables and have the nft binary deployed.","title":"Limitations"},{"location":"virtual_machines/istio_service_mesh/","text":"Istio service mesh \u00b6 Service mesh allows to monitor, visualize and control traffic between pods. Kubevirt supports running VMs as a part of Istio service mesh. Limitations \u00b6 Istio service mesh is only supported with a pod network masquerade binding. Istio uses a list of ports for its own purposes, these ports must not be explicitly specified in a VMI interface. Istio only supports IPv4. Prerequisites \u00b6 This guide assumes that Istio is already deployed and uses Istio CNI Plugin. See Istio documentation for more information. Optionally, istioctl binary for troubleshooting. See Istio installation inctructions . The target namespace where the VM is created must be labelled with istio-injection=enabled label. If Multus is used to manage CNI, the following NetworkAttachmentDefinition is required in the application namespace: apiVersion: \"k8s.cni.cncf.io/v1\" kind: NetworkAttachmentDefinition metadata: name: istio-cni Create a VirtualMachineInstance with enabled Istio proxy injecton \u00b6 The example below specifies a VMI with masquerade network interface and sidecar.istio.io/inject annotation to register the VM to the service mesh. apiVersion: kubevirt.io/v1 kind: VirtualMachineInstance metadata: annotations: sidecar.istio.io/inject: \"true\" labels: app: vmi-istio name: vmi-istio spec: domain: devices: interfaces: - name: default masquerade: {} disks: - disk: bus: virtio name: containerdisk resources: requests: memory: 1024M networks: - name: default pod: {} terminationGracePeriodSeconds: 0 volumes: - name: containerdisk containerDisk: image: registry:5000/kubevirt/fedora-cloud-container-disk-demo:devel Istio expects each application to be associated with at least one Kubernetes service. Create the following Service exposing port 8080: apiVersion: v1 kind: Service metadata: name: vmi-istio spec: selector: app: vmi-istio ports: - port: 8080 name: http protocol: TCP Note: Each Istio enabled VMI must feature the sidecar.istio.io/inject annotation instructing KubeVirt to perform necessary network configuration. Verification \u00b6 Verify istio-proxy sidecar is deployed and able to synchronize with Istio control plane using istioctl proxy-status command. See Istio Debbuging Envoy and Istiod documentation section for more information about proxy-status subcommand. $ kubectl get pods NAME READY STATUS RESTARTS AGE virt-launcher-vmi-istio-ncx7r 3/3 Running 0 7s $ kubectl get pods virt-launcher-vmi-istio-ncx7r -o jsonpath='{.spec.containers[*].name}' compute volumecontainerdisk istio-proxy $ istioctl proxy-status NAME CDS LDS EDS RDS ISTIOD VERSION ... virt-launcher-vmi-istio-ncx7r.default SYNCED SYNCED SYNCED SYNCED istiod-7c4d8c7757-hshj5 1.10.0 Troubleshooting \u00b6 Istio sidecar is not deployed \u00b6 $ kubectl get pods NAME READY STATUS RESTARTS AGE virt-launcher-vmi-istio-jnw6p 2/2 Running 0 37s $ kubectl get pods virt-launcher-vmi-istio-jnw6p -o jsonpath='{.spec.containers[*].name}' compute volumecontainerdisk Resolution: Make sure the istio-injection=enabled is added to the target namespace. If the issue persists, consult relevant part of Istio documentation . Istio sidecar is not ready \u00b6 $ kubectl get pods NAME READY STATUS RESTARTS AGE virt-launcher-vmi-istio-lg5gp 2/3 Running 0 90s $ kubectl describe pod virt-launcher-vmi-istio-lg5gp ... Warning Unhealthy 2d8h (x3 over 2d8h) kubelet Readiness probe failed: Get \"http://10.244.186.222:15021/healthz/ready\": dial tcp 10.244.186.222:15021: connect: no route to host Warning Unhealthy 2d8h (x4 over 2d8h) kubelet Readiness probe failed: Get \"http://10.244.186.222:15021/healthz/ready\": context deadline exceeded (Client.Timeout exceeded while awaiting headers) Resolution: Make sure the sidecar.istio.io/inject: \"true\" annotation is defined in the created VMI and that masquerade binding is used for pod network interface. Virt-launcher pod for VMI is stuck at initialization phase \u00b6 $ kubectl get pods NAME READY STATUS RESTARTS AGE virt-launcher-vmi-istio-44mws 0/3 Init:0/3 0 29s $ kubectl describe pod virt-launcher-vmi-istio-44mws ... Multus: [default/virt-launcher-vmi-istio-44mws]: error loading k8s delegates k8s args: TryLoadPodDelegates: error in getting k8s network for pod: GetNetworkDelegates: failed getting the delegate: getKubernetesDelegate: cannot find a network-attachment-definition (istio-cni) in namespace (default): network-attachment-definitions.k8s.cni.cncf.io \"istio-cni\" not found Resolution: Make sure the istio-cni NetworkAttachmentDefinition (provided in the Prerequisites section) is created in the target namespace.","title":"Istio service mesh"},{"location":"virtual_machines/istio_service_mesh/#istio-service-mesh","text":"Service mesh allows to monitor, visualize and control traffic between pods. Kubevirt supports running VMs as a part of Istio service mesh.","title":"Istio service mesh"},{"location":"virtual_machines/istio_service_mesh/#limitations","text":"Istio service mesh is only supported with a pod network masquerade binding. Istio uses a list of ports for its own purposes, these ports must not be explicitly specified in a VMI interface. Istio only supports IPv4.","title":"Limitations"},{"location":"virtual_machines/istio_service_mesh/#prerequisites","text":"This guide assumes that Istio is already deployed and uses Istio CNI Plugin. See Istio documentation for more information. Optionally, istioctl binary for troubleshooting. See Istio installation inctructions . The target namespace where the VM is created must be labelled with istio-injection=enabled label. If Multus is used to manage CNI, the following NetworkAttachmentDefinition is required in the application namespace: apiVersion: \"k8s.cni.cncf.io/v1\" kind: NetworkAttachmentDefinition metadata: name: istio-cni","title":"Prerequisites"},{"location":"virtual_machines/istio_service_mesh/#create-a-virtualmachineinstance-with-enabled-istio-proxy-injecton","text":"The example below specifies a VMI with masquerade network interface and sidecar.istio.io/inject annotation to register the VM to the service mesh. apiVersion: kubevirt.io/v1 kind: VirtualMachineInstance metadata: annotations: sidecar.istio.io/inject: \"true\" labels: app: vmi-istio name: vmi-istio spec: domain: devices: interfaces: - name: default masquerade: {} disks: - disk: bus: virtio name: containerdisk resources: requests: memory: 1024M networks: - name: default pod: {} terminationGracePeriodSeconds: 0 volumes: - name: containerdisk containerDisk: image: registry:5000/kubevirt/fedora-cloud-container-disk-demo:devel Istio expects each application to be associated with at least one Kubernetes service. Create the following Service exposing port 8080: apiVersion: v1 kind: Service metadata: name: vmi-istio spec: selector: app: vmi-istio ports: - port: 8080 name: http protocol: TCP Note: Each Istio enabled VMI must feature the sidecar.istio.io/inject annotation instructing KubeVirt to perform necessary network configuration.","title":"Create a VirtualMachineInstance with enabled Istio proxy injecton"},{"location":"virtual_machines/istio_service_mesh/#verification","text":"Verify istio-proxy sidecar is deployed and able to synchronize with Istio control plane using istioctl proxy-status command. See Istio Debbuging Envoy and Istiod documentation section for more information about proxy-status subcommand. $ kubectl get pods NAME READY STATUS RESTARTS AGE virt-launcher-vmi-istio-ncx7r 3/3 Running 0 7s $ kubectl get pods virt-launcher-vmi-istio-ncx7r -o jsonpath='{.spec.containers[*].name}' compute volumecontainerdisk istio-proxy $ istioctl proxy-status NAME CDS LDS EDS RDS ISTIOD VERSION ... virt-launcher-vmi-istio-ncx7r.default SYNCED SYNCED SYNCED SYNCED istiod-7c4d8c7757-hshj5 1.10.0","title":"Verification"},{"location":"virtual_machines/istio_service_mesh/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"virtual_machines/istio_service_mesh/#istio-sidecar-is-not-deployed","text":"$ kubectl get pods NAME READY STATUS RESTARTS AGE virt-launcher-vmi-istio-jnw6p 2/2 Running 0 37s $ kubectl get pods virt-launcher-vmi-istio-jnw6p -o jsonpath='{.spec.containers[*].name}' compute volumecontainerdisk Resolution: Make sure the istio-injection=enabled is added to the target namespace. If the issue persists, consult relevant part of Istio documentation .","title":"Istio sidecar is not deployed"},{"location":"virtual_machines/istio_service_mesh/#istio-sidecar-is-not-ready","text":"$ kubectl get pods NAME READY STATUS RESTARTS AGE virt-launcher-vmi-istio-lg5gp 2/3 Running 0 90s $ kubectl describe pod virt-launcher-vmi-istio-lg5gp ... Warning Unhealthy 2d8h (x3 over 2d8h) kubelet Readiness probe failed: Get \"http://10.244.186.222:15021/healthz/ready\": dial tcp 10.244.186.222:15021: connect: no route to host Warning Unhealthy 2d8h (x4 over 2d8h) kubelet Readiness probe failed: Get \"http://10.244.186.222:15021/healthz/ready\": context deadline exceeded (Client.Timeout exceeded while awaiting headers) Resolution: Make sure the sidecar.istio.io/inject: \"true\" annotation is defined in the created VMI and that masquerade binding is used for pod network interface.","title":"Istio sidecar is not ready"},{"location":"virtual_machines/istio_service_mesh/#virt-launcher-pod-for-vmi-is-stuck-at-initialization-phase","text":"$ kubectl get pods NAME READY STATUS RESTARTS AGE virt-launcher-vmi-istio-44mws 0/3 Init:0/3 0 29s $ kubectl describe pod virt-launcher-vmi-istio-44mws ... Multus: [default/virt-launcher-vmi-istio-44mws]: error loading k8s delegates k8s args: TryLoadPodDelegates: error in getting k8s network for pod: GetNetworkDelegates: failed getting the delegate: getKubernetesDelegate: cannot find a network-attachment-definition (istio-cni) in namespace (default): network-attachment-definitions.k8s.cni.cncf.io \"istio-cni\" not found Resolution: Make sure the istio-cni NetworkAttachmentDefinition (provided in the Prerequisites section) is created in the target namespace.","title":"Virt-launcher pod for VMI is stuck at initialization phase"},{"location":"virtual_machines/lifecycle/","text":"Lifecycle \u00b6 Every VirtualMachineInstance represents a single virtual machine instance . In general, the management of VirtualMachineInstances is kept similar to how Pods are managed: Every VM that is defined in the cluster is expected to be running, just like Pods. Deleting a VirtualMachineInstance is equivalent to shutting it down, this is also equivalent to how Pods behave. Launching a virtual machine \u00b6 In order to start a VirtualMachineInstance, you just need to create a VirtualMachineInstance object using kubectl : $ kubectl create -f vmi.yaml Listing virtual machines \u00b6 VirtualMachineInstances can be listed by querying for VirtualMachineInstance objects: $ kubectl get vmis Retrieving a virtual machine definition \u00b6 A single VirtualMachineInstance definition can be retrieved by getting the specific VirtualMachineInstance object: $ kubectl get vmis testvmi Stopping a virtual machine \u00b6 To stop the VirtualMachineInstance, you just need to delete the corresponding VirtualMachineInstance object using kubectl . $ kubectl delete -f vmi.yaml # OR $ kubectl delete vmis testvmi Note: Stopping a VirtualMachineInstance implies that it will be deleted from the cluster. You will not be able to start this VirtualMachineInstance object again. Pausing and unpausing a virtual machine \u00b6 Note: Pausing in this context refers to libvirt's virDomainSuspend command: \"The process is frozen without further access to CPU resources and I/O but the memory used by the domain at the hypervisor level will stay allocated\" To pause a virtual machine, you need the virtctl command line tool. Its pause command works on either VirtualMachine s or VirtualMachinesInstance s: $ virtctl pause vm testvm # OR $ virtctl pause vmi testvm Paused VMIs have a Paused condition in their status: $ kubectl get vmi testvm -o=jsonpath='{.status.conditions[?(@.type==\"Paused\")].message}' VMI was paused by user Unpausing works similar to pausing: $ virtctl unpause vm testvm # OR $ virtctl unpause vmi testvm Renaming a Virtual Machine \u00b6 Note: Renaming a Virtual Machine is only possible when a Virtual Machine is stopped, or has a 'Halted' run strategy. $ virtctl rename vm_name new_vm_name","title":"Lifecycle"},{"location":"virtual_machines/lifecycle/#lifecycle","text":"Every VirtualMachineInstance represents a single virtual machine instance . In general, the management of VirtualMachineInstances is kept similar to how Pods are managed: Every VM that is defined in the cluster is expected to be running, just like Pods. Deleting a VirtualMachineInstance is equivalent to shutting it down, this is also equivalent to how Pods behave.","title":"Lifecycle"},{"location":"virtual_machines/lifecycle/#launching-a-virtual-machine","text":"In order to start a VirtualMachineInstance, you just need to create a VirtualMachineInstance object using kubectl : $ kubectl create -f vmi.yaml","title":"Launching a virtual machine"},{"location":"virtual_machines/lifecycle/#listing-virtual-machines","text":"VirtualMachineInstances can be listed by querying for VirtualMachineInstance objects: $ kubectl get vmis","title":"Listing virtual machines"},{"location":"virtual_machines/lifecycle/#retrieving-a-virtual-machine-definition","text":"A single VirtualMachineInstance definition can be retrieved by getting the specific VirtualMachineInstance object: $ kubectl get vmis testvmi","title":"Retrieving a virtual machine definition"},{"location":"virtual_machines/lifecycle/#stopping-a-virtual-machine","text":"To stop the VirtualMachineInstance, you just need to delete the corresponding VirtualMachineInstance object using kubectl . $ kubectl delete -f vmi.yaml # OR $ kubectl delete vmis testvmi Note: Stopping a VirtualMachineInstance implies that it will be deleted from the cluster. You will not be able to start this VirtualMachineInstance object again.","title":"Stopping a virtual machine"},{"location":"virtual_machines/lifecycle/#pausing-and-unpausing-a-virtual-machine","text":"Note: Pausing in this context refers to libvirt's virDomainSuspend command: \"The process is frozen without further access to CPU resources and I/O but the memory used by the domain at the hypervisor level will stay allocated\" To pause a virtual machine, you need the virtctl command line tool. Its pause command works on either VirtualMachine s or VirtualMachinesInstance s: $ virtctl pause vm testvm # OR $ virtctl pause vmi testvm Paused VMIs have a Paused condition in their status: $ kubectl get vmi testvm -o=jsonpath='{.status.conditions[?(@.type==\"Paused\")].message}' VMI was paused by user Unpausing works similar to pausing: $ virtctl unpause vm testvm # OR $ virtctl unpause vmi testvm","title":"Pausing and unpausing a virtual machine"},{"location":"virtual_machines/lifecycle/#renaming-a-virtual-machine","text":"Note: Renaming a Virtual Machine is only possible when a Virtual Machine is stopped, or has a 'Halted' run strategy. $ virtctl rename vm_name new_vm_name","title":"Renaming a Virtual Machine"},{"location":"virtual_machines/liveness_and_readiness_probes/","text":"Liveness and Readiness Probes \u00b6 It is possible to configure Liveness and Readiness Probes in a similar fashion like it is possible to configure Liveness and Readiness Probes on Containers . Liveness Probes will effectively stop the VirtualMachineInstance if they fail, which will allow higher level controllers, like VirtualMachine or VirtualMachineInstanceReplicaSet to spawn new instances, which will hopefully be responsive again. Readiness Probes are an indicator for Services and Endpoints if the VirtualMachineInstance is ready to receive traffic from Services. If Readiness Probes fail, the VirtualMachineInstance will be removed from the Endpoints which back services until the probe recovers. Watchdogs focus on ensuring that an Operating System is still responsive. They complement the probes which are more workload centric. Watchdogs require kernel support from the guest and additional tooling like the commonly used watchdog binary. Define a HTTP Liveness Probe \u00b6 The following VirtualMachineInstance configures a HTTP Liveness Probe via spec.livenessProbe.httpGet , which will query port 1500 of the VirtualMachineInstance, after an initial delay of 120 seconds. The VirtualMachineInstance itself installs and runs a minimal HTTP server on port 1500 via cloud-init. apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance metadata: labels: special: vmi-fedora name: vmi-fedora spec: domain: devices: disks: - disk: bus: virtio name: containerdisk - disk: bus: virtio name: cloudinitdisk resources: requests: memory: 1024M livenessProbe: initialDelaySeconds: 120 periodSeconds: 20 httpGet: port: 1500 timeoutSeconds: 10 terminationGracePeriodSeconds: 0 volumes: - name: containerdisk registryDisk: image: registry:5000/kubevirt/fedora-cloud-registry-disk-demo:devel - cloudInitNoCloud: userData: |- #cloud-config password: fedora chpasswd: { expire: False } bootcmd: - setenforce 0 - dnf install -y nmap-ncat - systemd-run --unit=httpserver nc -klp 1500 -e '/usr/bin/echo -e HTTP/1.1 200 OK\\\\nContent-Length: 12\\\\n\\\\nHello World!' name: cloudinitdisk Define a TCP Liveness Probe \u00b6 The following VirtualMachineInstance configures a TCP Liveness Probe via spec.livenessProbe.tcpSocket , which will query port 1500 of the VirtualMachineInstance, after an initial delay of 120 seconds. The VirtualMachineInstance itself installs and runs a minimal HTTP server on port 1500 via cloud-init. apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance metadata: labels: special: vmi-fedora name: vmi-fedora spec: domain: devices: disks: - disk: bus: virtio name: containerdisk - disk: bus: virtio name: cloudinitdisk resources: requests: memory: 1024M livenessProbe: initialDelaySeconds: 120 periodSeconds: 20 tcpSocket: port: 1500 timeoutSeconds: 10 terminationGracePeriodSeconds: 0 volumes: - name: containerdisk registryDisk: image: registry:5000/kubevirt/fedora-cloud-registry-disk-demo:devel - cloudInitNoCloud: userData: |- #cloud-config password: fedora chpasswd: { expire: False } bootcmd: - setenforce 0 - dnf install -y nmap-ncat - systemd-run --unit=httpserver nc -klp 1500 -e '/usr/bin/echo -e HTTP/1.1 200 OK\\\\nContent-Length: 12\\\\n\\\\nHello World!' name: cloudinitdisk Define Readiness Probes \u00b6 Readiness Probes are configured in a similar way like liveness probes. Instead of spec.livenessProbe , spec.readinessProbe needs to be filled: apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance metadata: labels: special: vmi-fedora name: vmi-fedora spec: domain: devices: disks: - disk: bus: virtio name: containerdisk - disk: bus: virtio name: cloudinitdisk resources: requests: memory: 1024M readinessProbe: httpGet: port: 1500 initialDelaySeconds: 120 periodSeconds: 20 timeoutSeconds: 10 failureThreshold: 3 successThreshold: 3 terminationGracePeriodSeconds: 0 volumes: - name: containerdisk registryDisk: image: registry:5000/kubevirt/fedora-cloud-registry-disk-demo:devel - cloudInitNoCloud: userData: |- #cloud-config password: fedora chpasswd: { expire: False } bootcmd: - setenforce 0 - dnf install -y nmap-ncat - systemd-run --unit=httpserver nc -klp 1500 -e '/usr/bin/echo -e HTTP/1.1 200 OK\\\\n\\\\nHello World!' name: cloudinitdisk Note that in the case of Readiness Probes, it is also possible to set a failureThreshold and a successThreashold to only flip between ready and non-ready state if the probe succeeded or failed multiple times. Dual-stack considerations \u00b6 Some context is needed to understand the limitations imposed by a dual-stack network configuration on readiness - or liveness - probes. Users must be fully aware that a dual-stack configuration is currently only available when using a masquerade binding type. Furthermore, it must be recalled that accessing a VM using masquerade binding type is performed via the pod IP address; in dual-stack mode, both IPv4 and IPv6 addresses can be used to reach the VM. Dual-stack networking configurations have a limitation when using HTTP / TCP probes - you cannot probe the VMI by its IPv6 address . The reason for this is the host field for both the HTTP and TCP probe actions default to the pod's IP address, which is currently always the IPv4 address. Since the pod's IP address is not known before creating the VMI, it is not possible to pre-provision the probe's host field. Defining a Watchdog \u00b6 A watchdog is a more VM centric approach where the responsiveness of the Operating System is focused on. One can configure the i6300esb watchdog device: --- apiVersion: kubevirt.io/v1 kind: VirtualMachineInstance metadata: labels: special: vmi-with-watchdog name: vmi-with-watchdog spec: domain: devices: watchdog: name: mywatchdog i6300esb: action: \"poweroff\" disks: - disk: bus: virtio name: containerdisk machine: type: \"\" resources: requests: memory: 512M terminationGracePeriodSeconds: 0 volumes: - containerDisk: image: quay.io/kubevirt/alpine-container-disk-demo name: containerdisk The example above configures it with the poweroff action. It defines what will happen if the OS can't respond anymore. Other possible actions are reset and shutdown . The Alpine VM in this example will have the device exposed as /dev/watchdog . This device can then be used by the watchdog binary. For example, if root executes this command inside the VM: watchdog -t 2000ms -T 4000ms /dev/watchdog the watchdog will send a heartbeat every two seconds to /dev/watchdog and after four seconds without a heartbeat the defined action will be executed. In this case a hard poweroff .","title":"Liveness and Readiness Probes"},{"location":"virtual_machines/liveness_and_readiness_probes/#liveness-and-readiness-probes","text":"It is possible to configure Liveness and Readiness Probes in a similar fashion like it is possible to configure Liveness and Readiness Probes on Containers . Liveness Probes will effectively stop the VirtualMachineInstance if they fail, which will allow higher level controllers, like VirtualMachine or VirtualMachineInstanceReplicaSet to spawn new instances, which will hopefully be responsive again. Readiness Probes are an indicator for Services and Endpoints if the VirtualMachineInstance is ready to receive traffic from Services. If Readiness Probes fail, the VirtualMachineInstance will be removed from the Endpoints which back services until the probe recovers. Watchdogs focus on ensuring that an Operating System is still responsive. They complement the probes which are more workload centric. Watchdogs require kernel support from the guest and additional tooling like the commonly used watchdog binary.","title":"Liveness and Readiness Probes"},{"location":"virtual_machines/liveness_and_readiness_probes/#define-a-http-liveness-probe","text":"The following VirtualMachineInstance configures a HTTP Liveness Probe via spec.livenessProbe.httpGet , which will query port 1500 of the VirtualMachineInstance, after an initial delay of 120 seconds. The VirtualMachineInstance itself installs and runs a minimal HTTP server on port 1500 via cloud-init. apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance metadata: labels: special: vmi-fedora name: vmi-fedora spec: domain: devices: disks: - disk: bus: virtio name: containerdisk - disk: bus: virtio name: cloudinitdisk resources: requests: memory: 1024M livenessProbe: initialDelaySeconds: 120 periodSeconds: 20 httpGet: port: 1500 timeoutSeconds: 10 terminationGracePeriodSeconds: 0 volumes: - name: containerdisk registryDisk: image: registry:5000/kubevirt/fedora-cloud-registry-disk-demo:devel - cloudInitNoCloud: userData: |- #cloud-config password: fedora chpasswd: { expire: False } bootcmd: - setenforce 0 - dnf install -y nmap-ncat - systemd-run --unit=httpserver nc -klp 1500 -e '/usr/bin/echo -e HTTP/1.1 200 OK\\\\nContent-Length: 12\\\\n\\\\nHello World!' name: cloudinitdisk","title":"Define a HTTP Liveness Probe"},{"location":"virtual_machines/liveness_and_readiness_probes/#define-a-tcp-liveness-probe","text":"The following VirtualMachineInstance configures a TCP Liveness Probe via spec.livenessProbe.tcpSocket , which will query port 1500 of the VirtualMachineInstance, after an initial delay of 120 seconds. The VirtualMachineInstance itself installs and runs a minimal HTTP server on port 1500 via cloud-init. apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance metadata: labels: special: vmi-fedora name: vmi-fedora spec: domain: devices: disks: - disk: bus: virtio name: containerdisk - disk: bus: virtio name: cloudinitdisk resources: requests: memory: 1024M livenessProbe: initialDelaySeconds: 120 periodSeconds: 20 tcpSocket: port: 1500 timeoutSeconds: 10 terminationGracePeriodSeconds: 0 volumes: - name: containerdisk registryDisk: image: registry:5000/kubevirt/fedora-cloud-registry-disk-demo:devel - cloudInitNoCloud: userData: |- #cloud-config password: fedora chpasswd: { expire: False } bootcmd: - setenforce 0 - dnf install -y nmap-ncat - systemd-run --unit=httpserver nc -klp 1500 -e '/usr/bin/echo -e HTTP/1.1 200 OK\\\\nContent-Length: 12\\\\n\\\\nHello World!' name: cloudinitdisk","title":"Define a TCP Liveness Probe"},{"location":"virtual_machines/liveness_and_readiness_probes/#define-readiness-probes","text":"Readiness Probes are configured in a similar way like liveness probes. Instead of spec.livenessProbe , spec.readinessProbe needs to be filled: apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance metadata: labels: special: vmi-fedora name: vmi-fedora spec: domain: devices: disks: - disk: bus: virtio name: containerdisk - disk: bus: virtio name: cloudinitdisk resources: requests: memory: 1024M readinessProbe: httpGet: port: 1500 initialDelaySeconds: 120 periodSeconds: 20 timeoutSeconds: 10 failureThreshold: 3 successThreshold: 3 terminationGracePeriodSeconds: 0 volumes: - name: containerdisk registryDisk: image: registry:5000/kubevirt/fedora-cloud-registry-disk-demo:devel - cloudInitNoCloud: userData: |- #cloud-config password: fedora chpasswd: { expire: False } bootcmd: - setenforce 0 - dnf install -y nmap-ncat - systemd-run --unit=httpserver nc -klp 1500 -e '/usr/bin/echo -e HTTP/1.1 200 OK\\\\n\\\\nHello World!' name: cloudinitdisk Note that in the case of Readiness Probes, it is also possible to set a failureThreshold and a successThreashold to only flip between ready and non-ready state if the probe succeeded or failed multiple times.","title":"Define Readiness Probes"},{"location":"virtual_machines/liveness_and_readiness_probes/#dual-stack-considerations","text":"Some context is needed to understand the limitations imposed by a dual-stack network configuration on readiness - or liveness - probes. Users must be fully aware that a dual-stack configuration is currently only available when using a masquerade binding type. Furthermore, it must be recalled that accessing a VM using masquerade binding type is performed via the pod IP address; in dual-stack mode, both IPv4 and IPv6 addresses can be used to reach the VM. Dual-stack networking configurations have a limitation when using HTTP / TCP probes - you cannot probe the VMI by its IPv6 address . The reason for this is the host field for both the HTTP and TCP probe actions default to the pod's IP address, which is currently always the IPv4 address. Since the pod's IP address is not known before creating the VMI, it is not possible to pre-provision the probe's host field.","title":"Dual-stack considerations"},{"location":"virtual_machines/liveness_and_readiness_probes/#defining-a-watchdog","text":"A watchdog is a more VM centric approach where the responsiveness of the Operating System is focused on. One can configure the i6300esb watchdog device: --- apiVersion: kubevirt.io/v1 kind: VirtualMachineInstance metadata: labels: special: vmi-with-watchdog name: vmi-with-watchdog spec: domain: devices: watchdog: name: mywatchdog i6300esb: action: \"poweroff\" disks: - disk: bus: virtio name: containerdisk machine: type: \"\" resources: requests: memory: 512M terminationGracePeriodSeconds: 0 volumes: - containerDisk: image: quay.io/kubevirt/alpine-container-disk-demo name: containerdisk The example above configures it with the poweroff action. It defines what will happen if the OS can't respond anymore. Other possible actions are reset and shutdown . The Alpine VM in this example will have the device exposed as /dev/watchdog . This device can then be used by the watchdog binary. For example, if root executes this command inside the VM: watchdog -t 2000ms -T 4000ms /dev/watchdog the watchdog will send a heartbeat every two seconds to /dev/watchdog and after four seconds without a heartbeat the defined action will be executed. In this case a hard poweroff .","title":"Defining a Watchdog"},{"location":"virtual_machines/networkpolicy/","text":"NetworkPolicy \u00b6 Before creating NetworkPolicy objects, make sure you are using a networking solution which supports NetworkPolicy. Network isolation is controlled entirely by NetworkPolicy objects. By default, all vmis in a namespace are accessible from other vmis and network endpoints. To isolate one or more vmis in a project, you can create NetworkPolicy objects in that namespace to indicate the allowed incoming connections. Note: vmis and pods are treated equally by network policies, since labels are passed through to the pods which contain the running vmi. With other words, labels on vmis can be matched by spec.podSelector on the policy. Create NetworkPolicy to Deny All Traffic \u00b6 To make a project \"deny by default\" add a NetworkPolicy object that matches all vmis but accepts no traffic. kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: name: deny-by-default spec: podSelector: {} ingress: [] Create NetworkPolicy to only Accept connections from vmis within namespaces To make vmis accept connections from other vmis in the same namespace, but reject all other connections from vmis in other namespaces: .... kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: name: allow-same-namespace spec: podSelector: {} ingress: - from: - podSelector: {} .... Create NetworkPolicy to only allow HTTP and HTTPS traffic To enable only HTTP and HTTPS access to the vmis, add a NetworkPolicy object similar to: kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: name: allow-http-https spec: podSelector: {} ingress: - ports: - protocol: TCP port: 8080 - protocol: TCP port: 8443 Create NetworkPolicy to deny traffic by labels \u00b6 To make one specific vmi with a label type: test to reject all traffic from other vmis, create: kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: name: deny-by-label spec: podSelector: matchLabels: type: test ingress: [] Kubernetes NetworkPolicy Documentation can be found here: Kubernetes NetworkPolicy","title":"NetworkPolicy"},{"location":"virtual_machines/networkpolicy/#networkpolicy","text":"Before creating NetworkPolicy objects, make sure you are using a networking solution which supports NetworkPolicy. Network isolation is controlled entirely by NetworkPolicy objects. By default, all vmis in a namespace are accessible from other vmis and network endpoints. To isolate one or more vmis in a project, you can create NetworkPolicy objects in that namespace to indicate the allowed incoming connections. Note: vmis and pods are treated equally by network policies, since labels are passed through to the pods which contain the running vmi. With other words, labels on vmis can be matched by spec.podSelector on the policy.","title":"NetworkPolicy"},{"location":"virtual_machines/networkpolicy/#create-networkpolicy-to-deny-all-traffic","text":"To make a project \"deny by default\" add a NetworkPolicy object that matches all vmis but accepts no traffic. kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: name: deny-by-default spec: podSelector: {} ingress: [] Create NetworkPolicy to only Accept connections from vmis within namespaces To make vmis accept connections from other vmis in the same namespace, but reject all other connections from vmis in other namespaces: .... kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: name: allow-same-namespace spec: podSelector: {} ingress: - from: - podSelector: {} .... Create NetworkPolicy to only allow HTTP and HTTPS traffic To enable only HTTP and HTTPS access to the vmis, add a NetworkPolicy object similar to: kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: name: allow-http-https spec: podSelector: {} ingress: - ports: - protocol: TCP port: 8080 - protocol: TCP port: 8443","title":"Create NetworkPolicy to Deny All Traffic"},{"location":"virtual_machines/networkpolicy/#create-networkpolicy-to-deny-traffic-by-labels","text":"To make one specific vmi with a label type: test to reject all traffic from other vmis, create: kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: name: deny-by-label spec: podSelector: matchLabels: type: test ingress: [] Kubernetes NetworkPolicy Documentation can be found here: Kubernetes NetworkPolicy","title":"Create NetworkPolicy to deny traffic by labels"},{"location":"virtual_machines/numa/","text":"NUMA \u00b6 FEATURE STATE: KubeVirt v0.43 NUMA support in KubeVirt is at this stage limited to a small set of special use-cases and will improve over time together with improvements made to Kubernetes. In general, the goal is to map the host NUMA topology as efficiently as possible to the Virtual Machine topology to improve the performance. The following NUMA mapping strategies can be used: GuestMappingPassthrough Preconditions \u00b6 In order to use current NUMA support, the following preconditions must be met: Dedicated CPU Resources must be configured. Hugepages need to be allocatable on target nodes. The NUMA feature gate must be enabled. GuestMappingPassthrough \u00b6 GuestMappingPassthrough will pass through the node numa topology to the guest. The topology is based on the dedicated CPUs which the VMI got assigned from the kubelet via the CPU Manager. It can be requested by setting spec.domain.cpu.guestMappingPassthrough on the VMI. Since KubeVirt does not know upfront which exclusive CPUs the VMI will get from the kubelet, there are some limitations: Guests may see different NUMA topologies when being rescheduled. The resulting NUMA topology may be asymmetrical. The VMI may fail to start on the node if not enough hugepages are available on the assigned NUMA nodes. While this NUMA modelling strategy has its limitations, aligning the guest's NUMA architecture with the node's can be critical for high-performance applications. An example VMI may look like this: apiVersion: kubevirt.io/v1 kind: VirtualMachineInstance metadata: name: numavm spec: domain: cpu: cores: 4 dedicatedCpuPlacement: true numa: guestMappingPassthrough: { } devices: disks: - disk: bus: virtio name: containerdisk - disk: bus: virtio name: cloudinitdisk resources: requests: memory: 64Mi memory: hugepages: pageSize: 2Mi volumes: - containerDisk: image: quay.io/kubevirt/cirros-container-disk-demo name: containerdisk - cloudInitNoCloud: userData: | #!/bin/sh echo 'printed from cloud-init userdata' name: cloudinitdisk","title":"NUMA"},{"location":"virtual_machines/numa/#numa","text":"FEATURE STATE: KubeVirt v0.43 NUMA support in KubeVirt is at this stage limited to a small set of special use-cases and will improve over time together with improvements made to Kubernetes. In general, the goal is to map the host NUMA topology as efficiently as possible to the Virtual Machine topology to improve the performance. The following NUMA mapping strategies can be used: GuestMappingPassthrough","title":"NUMA"},{"location":"virtual_machines/numa/#preconditions","text":"In order to use current NUMA support, the following preconditions must be met: Dedicated CPU Resources must be configured. Hugepages need to be allocatable on target nodes. The NUMA feature gate must be enabled.","title":"Preconditions"},{"location":"virtual_machines/numa/#guestmappingpassthrough","text":"GuestMappingPassthrough will pass through the node numa topology to the guest. The topology is based on the dedicated CPUs which the VMI got assigned from the kubelet via the CPU Manager. It can be requested by setting spec.domain.cpu.guestMappingPassthrough on the VMI. Since KubeVirt does not know upfront which exclusive CPUs the VMI will get from the kubelet, there are some limitations: Guests may see different NUMA topologies when being rescheduled. The resulting NUMA topology may be asymmetrical. The VMI may fail to start on the node if not enough hugepages are available on the assigned NUMA nodes. While this NUMA modelling strategy has its limitations, aligning the guest's NUMA architecture with the node's can be critical for high-performance applications. An example VMI may look like this: apiVersion: kubevirt.io/v1 kind: VirtualMachineInstance metadata: name: numavm spec: domain: cpu: cores: 4 dedicatedCpuPlacement: true numa: guestMappingPassthrough: { } devices: disks: - disk: bus: virtio name: containerdisk - disk: bus: virtio name: cloudinitdisk resources: requests: memory: 64Mi memory: hugepages: pageSize: 2Mi volumes: - containerDisk: image: quay.io/kubevirt/cirros-container-disk-demo name: containerdisk - cloudInitNoCloud: userData: | #!/bin/sh echo 'printed from cloud-init userdata' name: cloudinitdisk","title":"GuestMappingPassthrough"},{"location":"virtual_machines/presets/","text":"Presets \u00b6 VirtualMachineInstancePresets are an extension to general VirtualMachineInstance configuration behaving much like PodPresets from Kubernetes. When a VirtualMachineInstance is created, any applicable VirtualMachineInstancePresets will be applied to the existing spec for the VirtualMachineInstance . This allows for re-use of common settings that should apply to multiple VirtualMachineInstances . Create a VirtualMachineInstancePreset \u00b6 You can describe a VirtualMachineInstancePreset in a YAML file. For example, the vmi-preset.yaml file below describes a VirtualMachineInstancePreset that requests a VirtualMachineInstance be created with a resource request for 64M of RAM. apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstancePreset metadata: name: small-qemu spec: selector: matchLabels: kubevirt.io/size: small domain: resources: requests: memory: 64M Create a VirtualMachineInstancePreset based on that YAML file: kubectl create -f vmipreset.yaml Required Fields \u00b6 As with most Kubernetes resources, a VirtualMachineInstancePreset requires apiVersion , kind and metadata fields. Additionally VirtualMachineInstancePresets also need a spec section. While not technically required to satisfy syntax, it is strongly recommended to include a Selector in the spec section, otherwise a VirtualMachineInstancePreset will match all VirtualMachineInstances in a namespace. VirtualMachine Selector \u00b6 KubeVirt uses Kubernetes Labels and Selectors to determine which VirtualMachineInstancePresets apply to a given VirtualMachineInstance , similarly to how PodPresets work in Kubernetes. If a setting from a VirtualMachineInstancePreset is applied to a VirtualMachineInstance , the VirtualMachineInstance will be marked with an Annotation upon completion. Any domain structure can be listed in the spec of a VirtualMachineInstancePreset , e.g. Clock, Features, Memory, CPU, or Devices such as network interfaces. All elements of the spec section of a VirtualMachineInstancePreset will be applied to the VirtualMachineInstance . Once a VirtualMachineInstancePreset is successfully applied to a VirtualMachineInstance , the VirtualMachineInstance will be marked with an annotation to indicate that it was applied. If a conflict occurs while a VirtualMachineInstancePreset is being applied, that portion of the VirtualMachineInstancePreset will be skipped. Any valid Label can be matched against, but it is suggested that a general rule of thumb is to use os/shortname, e.g. kubevirt.io/os: rhel7 . Updating a VirtualMachineInstancePreset \u00b6 If a VirtualMachineInstancePreset is modified, changes will not be applied to existing VirtualMachineInstances . This applies to both the Selector indicating which VirtualMachineInstances should be matched, and also the Domain section which lists the settings that should be applied to a VirtualMachine . Overrides \u00b6 VirtualMachineInstancePresets use a similar conflict resolution strategy to Kubernetes PodPresets . If a portion of the domain spec is present in both a VirtualMachineInstance and a VirtualMachineInstancePreset and both resources have the identical information, then creation of the VirtualMachineInstance will continue normally. If however there is a difference between the resources, an Event will be created indicating which DomainSpec element of which VirtualMachineInstancePreset was overridden. For example: If both the VirtualMachineInstance and VirtualMachineInstancePreset define a CPU , but use a different number of Cores , KubeVirt will note the difference. If any settings from the VirtualMachineInstancePreset were successfully applied, the VirtualMachineInstance will be annotated. In the event that there is a difference between the Domains of a VirtualMachineInstance and VirtualMachineInstancePreset , KubeVirt will create an Event . kubectl get events can be used to show all Events . For example: $ kubectl get events .... Events: FirstSeen LastSeen Count From SubobjectPath Reason Message 2m 2m 1 myvmi.1515bbb8d397f258 VirtualMachineInstance Warning Conflict virtualmachineinstance-preset-controller Unable to apply VirtualMachineInstancePreset 'example-preset': spec.cpu: &{6} != &{4} Usage \u00b6 VirtualMachineInstancePresets are namespaced resources, so should be created in the same namespace as the VirtualMachineInstances that will use them: kubectl create -f <preset>.yaml [--namespace <namespace>] KubeVirt will determine which VirtualMachineInstancePresets apply to a Particular VirtualMachineInstance by matching Labels . For example: apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstancePreset metadata: name: example-preset selector: matchLabels: kubevirt.io/os: win10 ... would match any VirtualMachineInstance in the same namespace with a Label of flavor: foo . For example: apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance version: v1 metadata: name: myvmi labels: kubevirt.io/os: win10 ... Conflicts \u00b6 When multiple VirtualMachineInstancePresets match a particular VirtualMachineInstance , if they specify the same settings within a Domain, those settings must match. If two VirtualMachineInstancePresets have conflicting settings (e.g. for the number of CPU cores requested), an error will occur, and the VirtualMachineInstance will enter the Failed state, and a Warning event will be emitted explaining which settings of which VirtualMachineInstancePresets were problematic. Matching Multiple VirtualMachineInstances \u00b6 The main use case for VirtualMachineInstancePresets is to create re-usable settings that can be applied across various machines. Multiple methods are available to match the labels of a VirtualMachineInstance using selectors. matchLabels: Each VirtualMachineInstance can use a specific label shared by all instances. * matchExpressions: Logical operators for sets can be used to match multiple labels. Using matchLabels, the label used in the VirtualMachineInstancePreset must match one of the labels of the VirtualMachineInstance : selector: matchLabels: kubevirt.io/memory: large would match metadata: labels: kubevirt.io/memory: large kubevirt.io/os: win10 or metadata: labels: kubevirt.io/memory: large kubevirt.io/os: fedora27 Using matchExpressions allows for matching multiple labels of VirtualMachineInstances without needing to explicity list a label. selector: matchExpressions: - {key: kubevirt.io/os, operator: In, values: [fedora27, fedora26]} would match both: metadata: labels: kubevirt.io/os: fedora26 metadata: labels: kubevirt.io/os: fedora27 The Kubernetes documentation has a detailed explanation. Examples are provided below. Exclusions \u00b6 Since VirtualMachineInstancePresets use Selectors that indicate which VirtualMachineInstances their settings should apply to, there needs to exist a mechanism by which VirtualMachineInstances can opt out of VirtualMachineInstancePresets altogether. This is done using an annotation: kind: VirtualMachineInstance version: v1 metadata: name: myvmi annotations: virtualmachineinstancepresets.admission.kubevirt.io/exclude: \"true\" ... Examples \u00b6 Simple VirtualMachineInstancePreset Example \u00b6 apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstancePreset version: v1alpha3 metadata: name: example-preset spec: selector: matchLabels: kubevirt.io/os: win10 domain: features: acpi: {} apic: {} hyperv: relaxed: {} vapic: {} spinlocks: spinlocks: 8191 --- apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance version: v1 metadata: name: myvmi labels: kubevirt.io/os: win10 spec: domain: firmware: uuid: c8f99fc8-20f5-46c4-85e5-2b841c547cef Once the VirtualMachineInstancePreset is applied to the VirtualMachineInstance , the resulting resource would look like this: apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance metadata: annotations: presets.virtualmachineinstances.kubevirt.io/presets-applied: kubevirt.io/v1alpha3 virtualmachineinstancepreset.kubevirt.io/example-preset: kubevirt.io/v1alpha3 labels: kubevirt.io/os: win10 kubevirt.io/nodeName: master name: myvmi namespace: default spec: domain: devices: {} features: acpi: enabled: true apic: enabled: true hyperv: relaxed: enabled: true spinlocks: enabled: true spinlocks: 8191 vapic: enabled: true firmware: uuid: c8f99fc8-20f5-46c4-85e5-2b841c547cef machine: type: q35 resources: requests: memory: 8Mi Conflict Example \u00b6 This is an example of a merge conflict. In this case both the VirtualMachineInstance and VirtualMachineInstancePreset request different number of CPU's. apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstancePreset version: v1alpha3 metadata: name: example-preset spec: selector: matchLabels: kubevirt.io/flavor: default-features domain: cpu: cores: 4 --- apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance version: v1 metadata: name: myvmi labels: kubevirt.io/flavor: default-features spec: domain: cpu: cores: 6 In this case the VirtualMachineInstance Spec will remain unmodified. Use kubectl get events to show events. apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance metadata: annotations: presets.virtualmachineinstances.kubevirt.io/presets-applied: kubevirt.io/v1alpha3 generation: 0 labels: kubevirt.io/flavor: default-features name: myvmi namespace: default spec: domain: cpu: cores: 6 devices: {} machine: type: \"\" resources: {} status: {} Calling kubectl get events would have a line like: 2m 2m 1 myvmi.1515bbb8d397f258 VirtualMachineInstance Warning Conflict virtualmachineinstance-preset-controller Unable to apply VirtualMachineInstancePreset example-preset: spec.cpu: &{6} != &{4} Matching Multiple VirtualMachineInstances Using MatchLabels \u00b6 These VirtualMachineInstances have multiple labels, one that is unique and one that is shared. Note: This example breaks from the convention of using os-shortname as a Label for demonstration purposes. apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstancePreset metadata: name: twelve-cores spec: selector: matchLabels: kubevirt.io/cpu: dodecacore domain: cpu: cores: 12 --- apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance metadata: name: windows-10 labels: kubevirt.io/os: win10 kubevirt.io/cpu: dodecacore spec: --- apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance metadata: name: windows-7 labels: kubevirt.io/os: win7 kubevirt.io/cpu: dodecacore spec: terminationGracePeriodSeconds: 0 Adding this VirtualMachineInstancePreset and these VirtualMachineInstances will result in: apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance metadata: annotations: presets.virtualmachineinstances.kubevirt.io/presets-applied: kubevirt.io/v1alpha3 virtualmachineinstancepreset.kubevirt.io/twelve-cores: kubevirt.io/v1alpha3 labels: kubevirt.io/cpu: dodecacore kubevirt.io/os: win10 name: windows-10 spec: domain: cpu: cores: 12 devices: {} resources: requests: memory: 4Gi --- apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance metadata: annotations: presets.virtualmachineinstances.kubevirt.io/presets-applied: kubevirt.io/v1alpha3 virtualmachineinstancepreset.kubevirt.io/twelve-cores: kubevirt.io/v1alpha3 labels: kubevirt.io/cpu: dodecacore kubevirt.io/os: win7 name: windows-7 spec: domain: cpu: cores: 12 devices: {} resources: requests: memory: 4Gi terminationGracePeriodSeconds: 0 Matching Multiple VirtualMachineInstances Using MatchExpressions \u00b6 This VirtualMachineInstancePreset has a matchExpression that will match two labels: kubevirt.io/os: win10 and kubevirt.io/os: win7 . apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstancePreset metadata: name: windows-vmis spec: selector: matchExpressions: - {key: kubevirt.io/os, operator: In, values: [win10, win7]} domain: resources: requests: memory: 128M --- apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance metadata: name: smallvmi labels: kubevirt.io/os: win10 spec: terminationGracePeriodSeconds: 60 --- apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance metadata: name: largevmi labels: kubevirt.io/os: win7 spec: terminationGracePeriodSeconds: 120 Applying the preset to both VM's will result in: apiVersion: v1 items: - apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance metadata: annotations: presets.virtualmachineinstances.kubevirt.io/presets-applied: kubevirt.io/v1alpha3 virtualmachineinstancepreset.kubevirt.io/windows-vmis: kubevirt.io/v1alpha3 labels: kubevirt.io/os: win7 name: largevmi spec: domain: resources: requests: memory: 128M terminationGracePeriodSeconds: 120 - apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance metadata: annotations: presets.virtualmachineinstances.kubevirt.io/presets-applied: kubevirt.io/v1alpha3 virtualmachineinstancepreset.kubevirt.io/windows-vmis: kubevirt.io/v1alpha3 labels: kubevirt.io/os: win10 name: smallvmi spec: domain: resources: requests: memory: 128M terminationGracePeriodSeconds: 60","title":"Presets"},{"location":"virtual_machines/presets/#presets","text":"VirtualMachineInstancePresets are an extension to general VirtualMachineInstance configuration behaving much like PodPresets from Kubernetes. When a VirtualMachineInstance is created, any applicable VirtualMachineInstancePresets will be applied to the existing spec for the VirtualMachineInstance . This allows for re-use of common settings that should apply to multiple VirtualMachineInstances .","title":"Presets"},{"location":"virtual_machines/presets/#create-a-virtualmachineinstancepreset","text":"You can describe a VirtualMachineInstancePreset in a YAML file. For example, the vmi-preset.yaml file below describes a VirtualMachineInstancePreset that requests a VirtualMachineInstance be created with a resource request for 64M of RAM. apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstancePreset metadata: name: small-qemu spec: selector: matchLabels: kubevirt.io/size: small domain: resources: requests: memory: 64M Create a VirtualMachineInstancePreset based on that YAML file: kubectl create -f vmipreset.yaml","title":"Create a VirtualMachineInstancePreset"},{"location":"virtual_machines/presets/#required-fields","text":"As with most Kubernetes resources, a VirtualMachineInstancePreset requires apiVersion , kind and metadata fields. Additionally VirtualMachineInstancePresets also need a spec section. While not technically required to satisfy syntax, it is strongly recommended to include a Selector in the spec section, otherwise a VirtualMachineInstancePreset will match all VirtualMachineInstances in a namespace.","title":"Required Fields"},{"location":"virtual_machines/presets/#virtualmachine-selector","text":"KubeVirt uses Kubernetes Labels and Selectors to determine which VirtualMachineInstancePresets apply to a given VirtualMachineInstance , similarly to how PodPresets work in Kubernetes. If a setting from a VirtualMachineInstancePreset is applied to a VirtualMachineInstance , the VirtualMachineInstance will be marked with an Annotation upon completion. Any domain structure can be listed in the spec of a VirtualMachineInstancePreset , e.g. Clock, Features, Memory, CPU, or Devices such as network interfaces. All elements of the spec section of a VirtualMachineInstancePreset will be applied to the VirtualMachineInstance . Once a VirtualMachineInstancePreset is successfully applied to a VirtualMachineInstance , the VirtualMachineInstance will be marked with an annotation to indicate that it was applied. If a conflict occurs while a VirtualMachineInstancePreset is being applied, that portion of the VirtualMachineInstancePreset will be skipped. Any valid Label can be matched against, but it is suggested that a general rule of thumb is to use os/shortname, e.g. kubevirt.io/os: rhel7 .","title":"VirtualMachine Selector"},{"location":"virtual_machines/presets/#updating-a-virtualmachineinstancepreset","text":"If a VirtualMachineInstancePreset is modified, changes will not be applied to existing VirtualMachineInstances . This applies to both the Selector indicating which VirtualMachineInstances should be matched, and also the Domain section which lists the settings that should be applied to a VirtualMachine .","title":"Updating a VirtualMachineInstancePreset"},{"location":"virtual_machines/presets/#overrides","text":"VirtualMachineInstancePresets use a similar conflict resolution strategy to Kubernetes PodPresets . If a portion of the domain spec is present in both a VirtualMachineInstance and a VirtualMachineInstancePreset and both resources have the identical information, then creation of the VirtualMachineInstance will continue normally. If however there is a difference between the resources, an Event will be created indicating which DomainSpec element of which VirtualMachineInstancePreset was overridden. For example: If both the VirtualMachineInstance and VirtualMachineInstancePreset define a CPU , but use a different number of Cores , KubeVirt will note the difference. If any settings from the VirtualMachineInstancePreset were successfully applied, the VirtualMachineInstance will be annotated. In the event that there is a difference between the Domains of a VirtualMachineInstance and VirtualMachineInstancePreset , KubeVirt will create an Event . kubectl get events can be used to show all Events . For example: $ kubectl get events .... Events: FirstSeen LastSeen Count From SubobjectPath Reason Message 2m 2m 1 myvmi.1515bbb8d397f258 VirtualMachineInstance Warning Conflict virtualmachineinstance-preset-controller Unable to apply VirtualMachineInstancePreset 'example-preset': spec.cpu: &{6} != &{4}","title":"Overrides"},{"location":"virtual_machines/presets/#usage","text":"VirtualMachineInstancePresets are namespaced resources, so should be created in the same namespace as the VirtualMachineInstances that will use them: kubectl create -f <preset>.yaml [--namespace <namespace>] KubeVirt will determine which VirtualMachineInstancePresets apply to a Particular VirtualMachineInstance by matching Labels . For example: apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstancePreset metadata: name: example-preset selector: matchLabels: kubevirt.io/os: win10 ... would match any VirtualMachineInstance in the same namespace with a Label of flavor: foo . For example: apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance version: v1 metadata: name: myvmi labels: kubevirt.io/os: win10 ...","title":"Usage"},{"location":"virtual_machines/presets/#conflicts","text":"When multiple VirtualMachineInstancePresets match a particular VirtualMachineInstance , if they specify the same settings within a Domain, those settings must match. If two VirtualMachineInstancePresets have conflicting settings (e.g. for the number of CPU cores requested), an error will occur, and the VirtualMachineInstance will enter the Failed state, and a Warning event will be emitted explaining which settings of which VirtualMachineInstancePresets were problematic.","title":"Conflicts"},{"location":"virtual_machines/presets/#matching-multiple-virtualmachineinstances","text":"The main use case for VirtualMachineInstancePresets is to create re-usable settings that can be applied across various machines. Multiple methods are available to match the labels of a VirtualMachineInstance using selectors. matchLabels: Each VirtualMachineInstance can use a specific label shared by all instances. * matchExpressions: Logical operators for sets can be used to match multiple labels. Using matchLabels, the label used in the VirtualMachineInstancePreset must match one of the labels of the VirtualMachineInstance : selector: matchLabels: kubevirt.io/memory: large would match metadata: labels: kubevirt.io/memory: large kubevirt.io/os: win10 or metadata: labels: kubevirt.io/memory: large kubevirt.io/os: fedora27 Using matchExpressions allows for matching multiple labels of VirtualMachineInstances without needing to explicity list a label. selector: matchExpressions: - {key: kubevirt.io/os, operator: In, values: [fedora27, fedora26]} would match both: metadata: labels: kubevirt.io/os: fedora26 metadata: labels: kubevirt.io/os: fedora27 The Kubernetes documentation has a detailed explanation. Examples are provided below.","title":"Matching Multiple VirtualMachineInstances"},{"location":"virtual_machines/presets/#exclusions","text":"Since VirtualMachineInstancePresets use Selectors that indicate which VirtualMachineInstances their settings should apply to, there needs to exist a mechanism by which VirtualMachineInstances can opt out of VirtualMachineInstancePresets altogether. This is done using an annotation: kind: VirtualMachineInstance version: v1 metadata: name: myvmi annotations: virtualmachineinstancepresets.admission.kubevirt.io/exclude: \"true\" ...","title":"Exclusions"},{"location":"virtual_machines/presets/#examples","text":"","title":"Examples"},{"location":"virtual_machines/presets/#simple-virtualmachineinstancepreset-example","text":"apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstancePreset version: v1alpha3 metadata: name: example-preset spec: selector: matchLabels: kubevirt.io/os: win10 domain: features: acpi: {} apic: {} hyperv: relaxed: {} vapic: {} spinlocks: spinlocks: 8191 --- apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance version: v1 metadata: name: myvmi labels: kubevirt.io/os: win10 spec: domain: firmware: uuid: c8f99fc8-20f5-46c4-85e5-2b841c547cef Once the VirtualMachineInstancePreset is applied to the VirtualMachineInstance , the resulting resource would look like this: apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance metadata: annotations: presets.virtualmachineinstances.kubevirt.io/presets-applied: kubevirt.io/v1alpha3 virtualmachineinstancepreset.kubevirt.io/example-preset: kubevirt.io/v1alpha3 labels: kubevirt.io/os: win10 kubevirt.io/nodeName: master name: myvmi namespace: default spec: domain: devices: {} features: acpi: enabled: true apic: enabled: true hyperv: relaxed: enabled: true spinlocks: enabled: true spinlocks: 8191 vapic: enabled: true firmware: uuid: c8f99fc8-20f5-46c4-85e5-2b841c547cef machine: type: q35 resources: requests: memory: 8Mi","title":"Simple VirtualMachineInstancePreset Example"},{"location":"virtual_machines/presets/#conflict-example","text":"This is an example of a merge conflict. In this case both the VirtualMachineInstance and VirtualMachineInstancePreset request different number of CPU's. apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstancePreset version: v1alpha3 metadata: name: example-preset spec: selector: matchLabels: kubevirt.io/flavor: default-features domain: cpu: cores: 4 --- apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance version: v1 metadata: name: myvmi labels: kubevirt.io/flavor: default-features spec: domain: cpu: cores: 6 In this case the VirtualMachineInstance Spec will remain unmodified. Use kubectl get events to show events. apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance metadata: annotations: presets.virtualmachineinstances.kubevirt.io/presets-applied: kubevirt.io/v1alpha3 generation: 0 labels: kubevirt.io/flavor: default-features name: myvmi namespace: default spec: domain: cpu: cores: 6 devices: {} machine: type: \"\" resources: {} status: {} Calling kubectl get events would have a line like: 2m 2m 1 myvmi.1515bbb8d397f258 VirtualMachineInstance Warning Conflict virtualmachineinstance-preset-controller Unable to apply VirtualMachineInstancePreset example-preset: spec.cpu: &{6} != &{4}","title":"Conflict Example"},{"location":"virtual_machines/presets/#matching-multiple-virtualmachineinstances-using-matchlabels","text":"These VirtualMachineInstances have multiple labels, one that is unique and one that is shared. Note: This example breaks from the convention of using os-shortname as a Label for demonstration purposes. apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstancePreset metadata: name: twelve-cores spec: selector: matchLabels: kubevirt.io/cpu: dodecacore domain: cpu: cores: 12 --- apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance metadata: name: windows-10 labels: kubevirt.io/os: win10 kubevirt.io/cpu: dodecacore spec: --- apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance metadata: name: windows-7 labels: kubevirt.io/os: win7 kubevirt.io/cpu: dodecacore spec: terminationGracePeriodSeconds: 0 Adding this VirtualMachineInstancePreset and these VirtualMachineInstances will result in: apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance metadata: annotations: presets.virtualmachineinstances.kubevirt.io/presets-applied: kubevirt.io/v1alpha3 virtualmachineinstancepreset.kubevirt.io/twelve-cores: kubevirt.io/v1alpha3 labels: kubevirt.io/cpu: dodecacore kubevirt.io/os: win10 name: windows-10 spec: domain: cpu: cores: 12 devices: {} resources: requests: memory: 4Gi --- apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance metadata: annotations: presets.virtualmachineinstances.kubevirt.io/presets-applied: kubevirt.io/v1alpha3 virtualmachineinstancepreset.kubevirt.io/twelve-cores: kubevirt.io/v1alpha3 labels: kubevirt.io/cpu: dodecacore kubevirt.io/os: win7 name: windows-7 spec: domain: cpu: cores: 12 devices: {} resources: requests: memory: 4Gi terminationGracePeriodSeconds: 0","title":"Matching Multiple VirtualMachineInstances Using MatchLabels"},{"location":"virtual_machines/presets/#matching-multiple-virtualmachineinstances-using-matchexpressions","text":"This VirtualMachineInstancePreset has a matchExpression that will match two labels: kubevirt.io/os: win10 and kubevirt.io/os: win7 . apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstancePreset metadata: name: windows-vmis spec: selector: matchExpressions: - {key: kubevirt.io/os, operator: In, values: [win10, win7]} domain: resources: requests: memory: 128M --- apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance metadata: name: smallvmi labels: kubevirt.io/os: win10 spec: terminationGracePeriodSeconds: 60 --- apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance metadata: name: largevmi labels: kubevirt.io/os: win7 spec: terminationGracePeriodSeconds: 120 Applying the preset to both VM's will result in: apiVersion: v1 items: - apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance metadata: annotations: presets.virtualmachineinstances.kubevirt.io/presets-applied: kubevirt.io/v1alpha3 virtualmachineinstancepreset.kubevirt.io/windows-vmis: kubevirt.io/v1alpha3 labels: kubevirt.io/os: win7 name: largevmi spec: domain: resources: requests: memory: 128M terminationGracePeriodSeconds: 120 - apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance metadata: annotations: presets.virtualmachineinstances.kubevirt.io/presets-applied: kubevirt.io/v1alpha3 virtualmachineinstancepreset.kubevirt.io/windows-vmis: kubevirt.io/v1alpha3 labels: kubevirt.io/os: win10 name: smallvmi spec: domain: resources: requests: memory: 128M terminationGracePeriodSeconds: 60","title":"Matching Multiple VirtualMachineInstances Using MatchExpressions"},{"location":"virtual_machines/replicaset/","text":"VirtualMachineInstanceReplicaSet \u00b6 A VirtualMachineInstanceReplicaSet tries to ensures that a specified number of VirtualMachineInstance replicas are running at any time. In other words, a VirtualMachineInstanceReplicaSet makes sure that a VirtualMachineInstance or a homogeneous set of VirtualMachineInstances is always up and ready. It is very similar to a Kubernetes ReplicaSet . No state is kept and no guarantees about the maximum number of VirtualMachineInstance replicas which are up are given. For example, the VirtualMachineInstanceReplicaSet may decide to create new replicas if possibly still running VMs are entering an unknown state. Using VirtualMachineInstanceReplicaSet \u00b6 The VirtualMachineInstanceReplicaSet allows us to specify a VirtualMachineInstanceTemplate in spec.template . It consists of ObjectMetadata in spec.template.metadata , and a VirtualMachineInstanceSpec in spec.template.spec . The specification of the virtual machine is equal to the specification of the virtual machine in the VirtualMachineInstance workload. spec.replicas can be used to specify how many replicas are wanted. If unspecified, the default value is 1. This value can be updated anytime. The controller will react to the changes. spec.selector is used by the controller to keep track of managed virtual machines. The selector specified there must be able to match the virtual machine labels as specified in spec.template.metadata.labels . If the selector does not match these labels, or they are empty, the controller will simply do nothing except from logging an error. The user is responsible for not creating other virtual machines or VirtualMachineInstanceReplicaSets which conflict with the selector and the template labels. Exposing a VirtualMachineInstanceReplicaSet as a Service \u00b6 A VirtualMachineInstanceReplicaSet could be exposed as a service. When this is done, one of the VirtualMachineInstances replicas will be picked for the actual delivery of the service. For example, exposing SSH port (22) as a ClusterIP service using virtctl on a VirtualMachineInstanceReplicaSet: $ virtctl expose vmirs vmi-ephemeral --name vmiservice --port 27017 --target-port 22 All service exposure options that apply to a VirtualMachineInstance apply to a VirtualMachineInstanceReplicaSet. See Exposing VirtualMachineInstance for more details. When to use a VirtualMachineInstanceReplicaSet \u00b6 Note: The base assumption is that referenced disks are read-only or that the VMIs are writing internally to a tmpfs. The most obvious volume sources for VirtualMachineInstanceReplicaSets which KubeVirt supports are referenced below. If other types are used data corruption is possible. Using VirtualMachineInstanceReplicaSet is the right choice when one wants many identical VMs and does not care about maintaining any disk state after the VMs are terminated. Volume types which work well in combination with a VirtualMachineInstanceReplicaSet are: cloudInitNoCloud ephemeral containerDisk emptyDisk configMap secret any other type, if the VMI writes internally to a tmpfs Fast starting ephemeral Virtual Machines \u00b6 This use-case involves small and fast booting VMs with little provisioning performed during initialization. In this scenario, migrations are not important. Redistributing VM workloads between Nodes can be achieved simply by deleting managed VirtualMachineInstances which are running on an overloaded Node. The eviction of such a VirtualMachineInstance can happen by directly deleting the VirtualMachineInstance instance (KubeVirt aware workload redistribution) or by deleting the corresponding Pod where the Virtual Machine runs in (Only Kubernetes aware workload redistribution). Slow starting ephemeral Virtual Machines \u00b6 In this use-case one has big and slow booting VMs, and complex or resource intensive provisioning is done during boot. More specifically, the timespan between the creation of a new VM and it entering the ready state is long. In this scenario, one still does not care about the state, but since re-provisioning VMs is expensive, migrations are important. Workload redistribution between Nodes can be achieved by migrating VirtualMachineInstances to different Nodes. A workload redistributor needs to be aware of KubeVirt and create migrations, instead of evicting VirtualMachineInstances by deletion. Note: The simplest form of having a migratable ephemeral VirtualMachineInstance, will be to use local storage based on ContainerDisks in combination with a file based backing store. However, migratable backing store support has not officially landed yet in KubeVirt and is untested. Example \u00b6 apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstanceReplicaSet metadata: name: testreplicaset spec: replicas: 3 selector: matchLabels: myvmi: myvmi template: metadata: name: test labels: myvmi: myvmi spec: domain: devices: disks: - disk: name: containerdisk resources: requests: memory: 64M volumes: - name: containerdisk containerDisk: image: kubevirt/cirros-container-disk-demo:latest Saving this manifest into testreplicaset.yaml and submitting it to Kubernetes will create three virtual machines based on the template. $ kubectl create -f testreplicaset.yaml virtualmachineinstancereplicaset \"testreplicaset\" created $ kubectl describe vmirs testreplicaset Name: testreplicaset Namespace: default Labels: <none> Annotations: <none> API Version: kubevirt.io/v1alpha3 Kind: VirtualMachineInstanceReplicaSet Metadata: Cluster Name: Creation Timestamp: 2018-01-03T12:42:30Z Generation: 0 Resource Version: 6380 Self Link: /apis/kubevirt.io/v1alpha3/namespaces/default/virtualmachineinstancereplicasets/testreplicaset UID: 903a9ea0-f083-11e7-9094-525400ee45b0 Spec: Replicas: 3 Selector: Match Labels: Myvmi: myvmi Template: Metadata: Creation Timestamp: <nil> Labels: Myvmi: myvmi Name: test Spec: Domain: Devices: Disks: Disk: Name: containerdisk Volume Name: containerdisk Resources: Requests: Memory: 64M Volumes: Name: containerdisk Container Disk: Image: kubevirt/cirros-container-disk-demo:latest Status: Conditions: <nil> Ready Replicas: 2 Replicas: 3 Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal SuccessfulCreate 13s virtualmachineinstancereplicaset-controller Created virtual machine: testh8998 Normal SuccessfulCreate 13s virtualmachineinstancereplicaset-controller Created virtual machine: testf474w Normal SuccessfulCreate 13s virtualmachineinstancereplicaset-controller Created virtual machine: test5lvkd Replicas is 3 and Ready Replicas is 2 . This means that at the moment when showing the status, three Virtual Machines were already created, but only two are running and ready. Scaling via the Scale Subresource \u00b6 Note: This requires the CustomResourceSubresources feature gate to be enabled for clusters prior to 1.11. The VirtualMachineInstanceReplicaSet supports the scale subresource. As a consequence it is possible to scale it via kubectl : $ kubectl scale vmirs myvmirs --replicas 5 Using the Horizontal Pod Autoscaler \u00b6 Note: This requires at cluster newer or equal to 1.11. The HorizontalPodAutoscaler (HPA) can be used with a VirtualMachineInstanceReplicaSet . Simply reference it in the spec of the autoscaler: apiVersion: autoscaling/v1 kind: HorizontalPodAutoscaler metadata: name: myhpa spec: scaleTargetRef: kind: VirtualMachineInstanceReplicaSet name: vmi-replicaset-cirros apiVersion: kubevirt.io/v1alpha3 minReplicas: 3 maxReplicas: 10 targetCPUUtilizationPercentage: 50 or use kubectl autoscale to define the HPA via the commandline: $ kubectl autoscale vmirs vmi-replicaset-cirros --min=3 --max=10","title":"VirtualMachineInstanceReplicaSet"},{"location":"virtual_machines/replicaset/#virtualmachineinstancereplicaset","text":"A VirtualMachineInstanceReplicaSet tries to ensures that a specified number of VirtualMachineInstance replicas are running at any time. In other words, a VirtualMachineInstanceReplicaSet makes sure that a VirtualMachineInstance or a homogeneous set of VirtualMachineInstances is always up and ready. It is very similar to a Kubernetes ReplicaSet . No state is kept and no guarantees about the maximum number of VirtualMachineInstance replicas which are up are given. For example, the VirtualMachineInstanceReplicaSet may decide to create new replicas if possibly still running VMs are entering an unknown state.","title":"VirtualMachineInstanceReplicaSet"},{"location":"virtual_machines/replicaset/#using-virtualmachineinstancereplicaset","text":"The VirtualMachineInstanceReplicaSet allows us to specify a VirtualMachineInstanceTemplate in spec.template . It consists of ObjectMetadata in spec.template.metadata , and a VirtualMachineInstanceSpec in spec.template.spec . The specification of the virtual machine is equal to the specification of the virtual machine in the VirtualMachineInstance workload. spec.replicas can be used to specify how many replicas are wanted. If unspecified, the default value is 1. This value can be updated anytime. The controller will react to the changes. spec.selector is used by the controller to keep track of managed virtual machines. The selector specified there must be able to match the virtual machine labels as specified in spec.template.metadata.labels . If the selector does not match these labels, or they are empty, the controller will simply do nothing except from logging an error. The user is responsible for not creating other virtual machines or VirtualMachineInstanceReplicaSets which conflict with the selector and the template labels.","title":"Using VirtualMachineInstanceReplicaSet"},{"location":"virtual_machines/replicaset/#exposing-a-virtualmachineinstancereplicaset-as-a-service","text":"A VirtualMachineInstanceReplicaSet could be exposed as a service. When this is done, one of the VirtualMachineInstances replicas will be picked for the actual delivery of the service. For example, exposing SSH port (22) as a ClusterIP service using virtctl on a VirtualMachineInstanceReplicaSet: $ virtctl expose vmirs vmi-ephemeral --name vmiservice --port 27017 --target-port 22 All service exposure options that apply to a VirtualMachineInstance apply to a VirtualMachineInstanceReplicaSet. See Exposing VirtualMachineInstance for more details.","title":"Exposing a VirtualMachineInstanceReplicaSet as a Service"},{"location":"virtual_machines/replicaset/#when-to-use-a-virtualmachineinstancereplicaset","text":"Note: The base assumption is that referenced disks are read-only or that the VMIs are writing internally to a tmpfs. The most obvious volume sources for VirtualMachineInstanceReplicaSets which KubeVirt supports are referenced below. If other types are used data corruption is possible. Using VirtualMachineInstanceReplicaSet is the right choice when one wants many identical VMs and does not care about maintaining any disk state after the VMs are terminated. Volume types which work well in combination with a VirtualMachineInstanceReplicaSet are: cloudInitNoCloud ephemeral containerDisk emptyDisk configMap secret any other type, if the VMI writes internally to a tmpfs","title":"When to use a VirtualMachineInstanceReplicaSet"},{"location":"virtual_machines/replicaset/#fast-starting-ephemeral-virtual-machines","text":"This use-case involves small and fast booting VMs with little provisioning performed during initialization. In this scenario, migrations are not important. Redistributing VM workloads between Nodes can be achieved simply by deleting managed VirtualMachineInstances which are running on an overloaded Node. The eviction of such a VirtualMachineInstance can happen by directly deleting the VirtualMachineInstance instance (KubeVirt aware workload redistribution) or by deleting the corresponding Pod where the Virtual Machine runs in (Only Kubernetes aware workload redistribution).","title":"Fast starting ephemeral Virtual Machines"},{"location":"virtual_machines/replicaset/#slow-starting-ephemeral-virtual-machines","text":"In this use-case one has big and slow booting VMs, and complex or resource intensive provisioning is done during boot. More specifically, the timespan between the creation of a new VM and it entering the ready state is long. In this scenario, one still does not care about the state, but since re-provisioning VMs is expensive, migrations are important. Workload redistribution between Nodes can be achieved by migrating VirtualMachineInstances to different Nodes. A workload redistributor needs to be aware of KubeVirt and create migrations, instead of evicting VirtualMachineInstances by deletion. Note: The simplest form of having a migratable ephemeral VirtualMachineInstance, will be to use local storage based on ContainerDisks in combination with a file based backing store. However, migratable backing store support has not officially landed yet in KubeVirt and is untested.","title":"Slow starting ephemeral Virtual Machines"},{"location":"virtual_machines/replicaset/#example","text":"apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstanceReplicaSet metadata: name: testreplicaset spec: replicas: 3 selector: matchLabels: myvmi: myvmi template: metadata: name: test labels: myvmi: myvmi spec: domain: devices: disks: - disk: name: containerdisk resources: requests: memory: 64M volumes: - name: containerdisk containerDisk: image: kubevirt/cirros-container-disk-demo:latest Saving this manifest into testreplicaset.yaml and submitting it to Kubernetes will create three virtual machines based on the template. $ kubectl create -f testreplicaset.yaml virtualmachineinstancereplicaset \"testreplicaset\" created $ kubectl describe vmirs testreplicaset Name: testreplicaset Namespace: default Labels: <none> Annotations: <none> API Version: kubevirt.io/v1alpha3 Kind: VirtualMachineInstanceReplicaSet Metadata: Cluster Name: Creation Timestamp: 2018-01-03T12:42:30Z Generation: 0 Resource Version: 6380 Self Link: /apis/kubevirt.io/v1alpha3/namespaces/default/virtualmachineinstancereplicasets/testreplicaset UID: 903a9ea0-f083-11e7-9094-525400ee45b0 Spec: Replicas: 3 Selector: Match Labels: Myvmi: myvmi Template: Metadata: Creation Timestamp: <nil> Labels: Myvmi: myvmi Name: test Spec: Domain: Devices: Disks: Disk: Name: containerdisk Volume Name: containerdisk Resources: Requests: Memory: 64M Volumes: Name: containerdisk Container Disk: Image: kubevirt/cirros-container-disk-demo:latest Status: Conditions: <nil> Ready Replicas: 2 Replicas: 3 Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal SuccessfulCreate 13s virtualmachineinstancereplicaset-controller Created virtual machine: testh8998 Normal SuccessfulCreate 13s virtualmachineinstancereplicaset-controller Created virtual machine: testf474w Normal SuccessfulCreate 13s virtualmachineinstancereplicaset-controller Created virtual machine: test5lvkd Replicas is 3 and Ready Replicas is 2 . This means that at the moment when showing the status, three Virtual Machines were already created, but only two are running and ready.","title":"Example"},{"location":"virtual_machines/replicaset/#scaling-via-the-scale-subresource","text":"Note: This requires the CustomResourceSubresources feature gate to be enabled for clusters prior to 1.11. The VirtualMachineInstanceReplicaSet supports the scale subresource. As a consequence it is possible to scale it via kubectl : $ kubectl scale vmirs myvmirs --replicas 5","title":"Scaling via the Scale Subresource"},{"location":"virtual_machines/replicaset/#using-the-horizontal-pod-autoscaler","text":"Note: This requires at cluster newer or equal to 1.11. The HorizontalPodAutoscaler (HPA) can be used with a VirtualMachineInstanceReplicaSet . Simply reference it in the spec of the autoscaler: apiVersion: autoscaling/v1 kind: HorizontalPodAutoscaler metadata: name: myhpa spec: scaleTargetRef: kind: VirtualMachineInstanceReplicaSet name: vmi-replicaset-cirros apiVersion: kubevirt.io/v1alpha3 minReplicas: 3 maxReplicas: 10 targetCPUUtilizationPercentage: 50 or use kubectl autoscale to define the HPA via the commandline: $ kubectl autoscale vmirs vmi-replicaset-cirros --min=3 --max=10","title":"Using the Horizontal Pod Autoscaler"},{"location":"virtual_machines/run_strategies/","text":"Run Strategies \u00b6 Overview \u00b6 VirtualMachines have a Running setting that determines whether or not there should be a guest running or not. Because KubeVirt will always immediately restart a VirtualMachineInstance for VirtualMachines with spec.running: true , a simple boolean is not always enough to fully describe desired behavior. For instance, there are cases when a user would like the ability to shut down a guest from inside the virtual machine. With spec.running: true , KubeVirt would immediately restart the VirtualMachineInstance. RunStrategy \u00b6 To allow for greater variation of user states, the RunStrategy field has been introduced. This is mutually exclusive with Running as they have somewhat overlapping conditions. There are currently four RunStrategies defined: Always: A VirtualMachineInstance will always be present. If the VirtualMachineInstance crashed, a new one will be spawned. This is the same behavior as spec.running: true . RerunOnFailure: A VirtualMachineInstance will be respawned if the previous instance failed in an error state. It will not be re-created if the guest stopped successfully (e.g. shut down from inside guest). Manual: The presence of a VirtualMachineInstance or lack thereof is controlled exclusively by the start/stop/restart VirtualMachine subresource endpoints. Halted: No VirtualMachineInstance will be present. If a guest is already running, it will be stopped. This is the same behavior as spec.running: false . Note : RunStrategy and Running are mutually exclusive, because they can be contradictory. The API server will reject VirtualMachine resources that define both. Virtctl \u00b6 The start , stop and restart methods of virtctl will invoke their respective subresources of VirtualMachines. This can have an effect on the runStrategy of the VirtualMachine as below: RunStrategy start stop restart Always - Halted Always RerunOnFailure - Halted RerunOnFailure Manual Manual Manual Manual Halted Always - - Table entries marked with - don't make sense, so won't have an effect on RunStrategy. RunStrategy Examples \u00b6 Always \u00b6 An example usage of the Always RunStrategy. apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachine metadata: labels: kubevirt.io/vm: vm-cirros name: vm-cirros spec: runStrategy: Always template: metadata: labels: kubevirt.io/vm: vm-cirros spec: domain: devices: disks: - disk: bus: virtio name: containerdisk terminationGracePeriodSeconds: 0 volumes: - containerDisk: image: kubevirt/cirros-container-disk-demo:latest name: containerdisk","title":"Run Strategies"},{"location":"virtual_machines/run_strategies/#run-strategies","text":"","title":"Run Strategies"},{"location":"virtual_machines/run_strategies/#overview","text":"VirtualMachines have a Running setting that determines whether or not there should be a guest running or not. Because KubeVirt will always immediately restart a VirtualMachineInstance for VirtualMachines with spec.running: true , a simple boolean is not always enough to fully describe desired behavior. For instance, there are cases when a user would like the ability to shut down a guest from inside the virtual machine. With spec.running: true , KubeVirt would immediately restart the VirtualMachineInstance.","title":"Overview"},{"location":"virtual_machines/run_strategies/#runstrategy","text":"To allow for greater variation of user states, the RunStrategy field has been introduced. This is mutually exclusive with Running as they have somewhat overlapping conditions. There are currently four RunStrategies defined: Always: A VirtualMachineInstance will always be present. If the VirtualMachineInstance crashed, a new one will be spawned. This is the same behavior as spec.running: true . RerunOnFailure: A VirtualMachineInstance will be respawned if the previous instance failed in an error state. It will not be re-created if the guest stopped successfully (e.g. shut down from inside guest). Manual: The presence of a VirtualMachineInstance or lack thereof is controlled exclusively by the start/stop/restart VirtualMachine subresource endpoints. Halted: No VirtualMachineInstance will be present. If a guest is already running, it will be stopped. This is the same behavior as spec.running: false . Note : RunStrategy and Running are mutually exclusive, because they can be contradictory. The API server will reject VirtualMachine resources that define both.","title":"RunStrategy"},{"location":"virtual_machines/run_strategies/#virtctl","text":"The start , stop and restart methods of virtctl will invoke their respective subresources of VirtualMachines. This can have an effect on the runStrategy of the VirtualMachine as below: RunStrategy start stop restart Always - Halted Always RerunOnFailure - Halted RerunOnFailure Manual Manual Manual Manual Halted Always - - Table entries marked with - don't make sense, so won't have an effect on RunStrategy.","title":"Virtctl"},{"location":"virtual_machines/run_strategies/#runstrategy-examples","text":"","title":"RunStrategy Examples"},{"location":"virtual_machines/run_strategies/#always","text":"An example usage of the Always RunStrategy. apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachine metadata: labels: kubevirt.io/vm: vm-cirros name: vm-cirros spec: runStrategy: Always template: metadata: labels: kubevirt.io/vm: vm-cirros spec: domain: devices: disks: - disk: bus: virtio name: containerdisk terminationGracePeriodSeconds: 0 volumes: - containerDisk: image: kubevirt/cirros-container-disk-demo:latest name: containerdisk","title":"Always"},{"location":"virtual_machines/service_objects/","text":"Service objects \u00b6 Once the VirtualMachineInstance is started, in order to connect to a VirtualMachineInstance, you can create a Service object for a VirtualMachineInstance. Currently, three types of service are supported: ClusterIP , NodePort and LoadBalancer . The default type is ClusterIP . Note : Labels on a VirtualMachineInstance are passed through to the pod, so simply add your labels for service creation to the VirtualMachineInstance. From there on it works like exposing any other k8s resource, by referencing these labels in a service. Expose VirtualMachineInstance as a ClusterIP Service \u00b6 Give a VirtualMachineInstance with the label special: key : apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance metadata: name: vmi-ephemeral labels: special: key spec: domain: devices: disks: - disk: bus: virtio name: containerdisk resources: requests: memory: 64M volumes: - name: containerdisk containerDisk: image: kubevirt/cirros-registry-disk-demo:latest we can expose its SSH port (22) by creating a ClusterIP service: apiVersion: v1 kind: Service metadata: name: vmiservice spec: ports: - port: 27017 protocol: TCP targetPort: 22 selector: special: key type: ClusterIP You just need to create this ClusterIP service by using kubectl : $ kubectl create -f vmiservice.yaml Alternatively, the VirtualMachineInstance could be exposed using the virtctl command: $ virtctl expose virtualmachineinstance vmi-ephemeral --name vmiservice --port 27017 --target-port 22 Notes: * If --target-port is not set, it will be take the same value as --port * The cluster IP is usually allocated automatically, but it may also be forced into a value using the --cluster-ip flag (assuming value is in the valid range and not taken) Query the service object: $ kubectl get service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE vmiservice ClusterIP 172.30.3.149 <none> 27017/TCP 2m You can connect to the VirtualMachineInstance by service IP and service port inside the cluster network: $ ssh cirros@172.30.3.149 -p 27017 Expose VirtualMachineInstance as a NodePort Service \u00b6 Expose the SSH port (22) of a VirtualMachineInstance running on KubeVirt by creating a NodePort service: apiVersion: v1 kind: Service metadata: name: nodeport spec: externalTrafficPolicy: Cluster ports: - name: nodeport nodePort: 30000 port: 27017 protocol: TCP targetPort: 22 selector: special: key type: NodePort You just need to create this NodePort service by using kubectl : $ kubectl -f nodeport.yaml Alternatively, the VirtualMachineInstance could be exposed using the virtctl command: $ virtctl expose virtualmachineinstance vmi-ephemeral --name nodeport --type NodePort --port 27017 --target-port 22 --node-port 30000 Notes: * If --node-port is not set, its value will be allocated dynamically (in the range above 30000) * If the --node-port value is set, it must be unique across all services The service can be listed by querying for the service objects: $ kubectl get service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE nodeport NodePort 172.30.232.73 <none> 27017:30000/TCP 5m Connect to the VirtualMachineInstance by using a node IP and node port outside the cluster network: $ ssh cirros@$NODE_IP -p 30000 Expose VirtualMachineInstance as a LoadBalancer Service \u00b6 Expose the RDP port (3389) of a VirtualMachineInstance running on KubeVirt by creating LoadBalancer service. Here is an example: apiVersion: v1 kind: Service metadata: name: lbsvc spec: externalTrafficPolicy: Cluster ports: - port: 27017 protocol: TCP targetPort: 3389 selector: special: key type: LoadBalancer You could create this LoadBalancer service by using kubectl : $ kubectl -f lbsvc.yaml Alternatively, the VirtualMachineInstance could be exposed using the virtctl command: $ virtctl expose virtualmachineinstance vmi-ephemeral --name lbsvc --type LoadBalancer --port 27017 --target-port 3389 Note that the external IP of the service could be forced to a value using the --external-ip flag (no validation is performed on this value). The service can be listed by querying for the service objects: $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE lbsvc LoadBalancer 172.30.27.5 172.29.10.235,172.29.10.235 27017:31829/TCP 5s Use vinagre client to connect your VirtualMachineInstance by using the public IP and port. Note that here the external port here (31829) was dynamically allocated.","title":"Service objects"},{"location":"virtual_machines/service_objects/#service-objects","text":"Once the VirtualMachineInstance is started, in order to connect to a VirtualMachineInstance, you can create a Service object for a VirtualMachineInstance. Currently, three types of service are supported: ClusterIP , NodePort and LoadBalancer . The default type is ClusterIP . Note : Labels on a VirtualMachineInstance are passed through to the pod, so simply add your labels for service creation to the VirtualMachineInstance. From there on it works like exposing any other k8s resource, by referencing these labels in a service.","title":"Service objects"},{"location":"virtual_machines/service_objects/#expose-virtualmachineinstance-as-a-clusterip-service","text":"Give a VirtualMachineInstance with the label special: key : apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance metadata: name: vmi-ephemeral labels: special: key spec: domain: devices: disks: - disk: bus: virtio name: containerdisk resources: requests: memory: 64M volumes: - name: containerdisk containerDisk: image: kubevirt/cirros-registry-disk-demo:latest we can expose its SSH port (22) by creating a ClusterIP service: apiVersion: v1 kind: Service metadata: name: vmiservice spec: ports: - port: 27017 protocol: TCP targetPort: 22 selector: special: key type: ClusterIP You just need to create this ClusterIP service by using kubectl : $ kubectl create -f vmiservice.yaml Alternatively, the VirtualMachineInstance could be exposed using the virtctl command: $ virtctl expose virtualmachineinstance vmi-ephemeral --name vmiservice --port 27017 --target-port 22 Notes: * If --target-port is not set, it will be take the same value as --port * The cluster IP is usually allocated automatically, but it may also be forced into a value using the --cluster-ip flag (assuming value is in the valid range and not taken) Query the service object: $ kubectl get service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE vmiservice ClusterIP 172.30.3.149 <none> 27017/TCP 2m You can connect to the VirtualMachineInstance by service IP and service port inside the cluster network: $ ssh cirros@172.30.3.149 -p 27017","title":"Expose VirtualMachineInstance as a ClusterIP Service"},{"location":"virtual_machines/service_objects/#expose-virtualmachineinstance-as-a-nodeport-service","text":"Expose the SSH port (22) of a VirtualMachineInstance running on KubeVirt by creating a NodePort service: apiVersion: v1 kind: Service metadata: name: nodeport spec: externalTrafficPolicy: Cluster ports: - name: nodeport nodePort: 30000 port: 27017 protocol: TCP targetPort: 22 selector: special: key type: NodePort You just need to create this NodePort service by using kubectl : $ kubectl -f nodeport.yaml Alternatively, the VirtualMachineInstance could be exposed using the virtctl command: $ virtctl expose virtualmachineinstance vmi-ephemeral --name nodeport --type NodePort --port 27017 --target-port 22 --node-port 30000 Notes: * If --node-port is not set, its value will be allocated dynamically (in the range above 30000) * If the --node-port value is set, it must be unique across all services The service can be listed by querying for the service objects: $ kubectl get service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE nodeport NodePort 172.30.232.73 <none> 27017:30000/TCP 5m Connect to the VirtualMachineInstance by using a node IP and node port outside the cluster network: $ ssh cirros@$NODE_IP -p 30000","title":"Expose VirtualMachineInstance as a NodePort Service"},{"location":"virtual_machines/service_objects/#expose-virtualmachineinstance-as-a-loadbalancer-service","text":"Expose the RDP port (3389) of a VirtualMachineInstance running on KubeVirt by creating LoadBalancer service. Here is an example: apiVersion: v1 kind: Service metadata: name: lbsvc spec: externalTrafficPolicy: Cluster ports: - port: 27017 protocol: TCP targetPort: 3389 selector: special: key type: LoadBalancer You could create this LoadBalancer service by using kubectl : $ kubectl -f lbsvc.yaml Alternatively, the VirtualMachineInstance could be exposed using the virtctl command: $ virtctl expose virtualmachineinstance vmi-ephemeral --name lbsvc --type LoadBalancer --port 27017 --target-port 3389 Note that the external IP of the service could be forced to a value using the --external-ip flag (no validation is performed on this value). The service can be listed by querying for the service objects: $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE lbsvc LoadBalancer 172.30.27.5 172.29.10.235,172.29.10.235 27017:31829/TCP 5s Use vinagre client to connect your VirtualMachineInstance by using the public IP and port. Note that here the external port here (31829) was dynamically allocated.","title":"Expose VirtualMachineInstance as a LoadBalancer Service"},{"location":"virtual_machines/startup_scripts/","text":"Startup Scripts \u00b6 KubeVirt supports the ability to assign a startup script to a VirtualMachineInstance instance which is executed automatically when the VM initializes. These scripts are commonly used to automate injection of users and SSH keys into VMs in order to provide remote access to the machine. For example, a startup script can be used to inject credentials into a VM that allows an Ansible job running on a remote host to access and provision the VM. Startup scripts are not limited to any specific use case though. They can be used to run any arbitrary script in a VM on boot. Cloud-init \u00b6 cloud-init is a widely adopted project used for early initialization of a VM. Used by cloud providers such as AWS and GCP, cloud-init has established itself as the defacto method of providing startup scripts to VMs. Cloud-init documentation can be found here: Cloud-init Documentation . KubeVirt supports cloud-init's \"NoCloud\" and \"ConfigDrive\" datasources which involve injecting startup scripts into a VM instance through the use of an ephemeral disk. VMs with the cloud-init package installed will detect the ephemeral disk and execute custom userdata scripts at boot. Sysprep \u00b6 Sysprep is an automation tool for Windows that automates Windows installation, setup, and custom software provisioning. The general flow is: Seal the vm image with the Sysprep tool, for example by running: %WINDIR%\\system32\\sysprep\\sysprep.exe /generalize /shutdown /oobe /mode:vm Note We need to make sure the base vm does not restart, which can be done by setting the vm run strategy as RerunOnFailure . VM runStrategy: spec: runStrategy: RerunOnFailure More information can be found here: Sysprep Process Overview Sysprep (Generalize) a Windows installation Note It is important that there is no answer file detected when the Sysprep Tool is triggered, because Windows Setup searches for answer files at the beginning of each configuration pass and caches it. If that happens, when the OS will start - it will just use the cached answer file, ignoring the one we provide through the Sysprep API. More information can be found here . Providing an Answer file named autounattend.xml in an attached media. The answer file can be provided in a ConfigMap or a Secret with the key autounattend.xml More information can be found here: Answer files (unattend.xml) Note There are also many easy to find online tools available for creating an answer file. Cloud-init Examples \u00b6 User Data \u00b6 KubeVirt supports the cloud-init NoCloud and ConfigDrive data sources which involve injecting startup scripts through the use of a disk attached to the VM. In order to assign a custom userdata script to a VirtualMachineInstance using this method, users must define a disk and a volume for the NoCloud or ConfigDrive datasource in the VirtualMachineInstance's spec. Data Sources \u00b6 Under most circumstances users should stick to the NoCloud data source as it is the simplest cloud-init data source. Only if NoCloud is not supported by the cloud-init implementation (e.g. coreos-cloudinit ) users should switch the data source to ConfigDrive. Switching the cloud-init data source to ConfigDrive is as easy as changing the volume type in the VirtualMachineInstance's spec from cloudInitNoCloud to cloudInitConfigDrive . NoCloud data source: volumes: - name: cloudinitvolume cloudInitNoCloud: userData: \"#cloud-config\" ConfigDrive data source: volumes: - name: cloudinitvolume cloudInitConfigDrive: userData: \"#cloud-config\" See the examples below for more complete cloud-init examples. Cloud-init user-data as clear text \u00b6 In the example below, a SSH key is stored in the cloudInitNoCloud Volume's userData field as clean text. There is a corresponding disks entry that references the cloud-init volume and assigns it to the VM's device. # Create a VM manifest with the startup script # a cloudInitNoCloud volume's userData field. cat << END > my-vmi.yaml apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance metadata: name: myvmi spec: terminationGracePeriodSeconds: 5 domain: resources: requests: memory: 64M devices: disks: - name: containerdisk disk: bus: virtio - name: cloudinitdisk disk: bus: virtio volumes: - name: containerdisk containerDisk: image: kubevirt/cirros-container-disk-demo:latest - name: cloudinitdisk cloudInitNoCloud: userData: | ssh_authorized_keys: - ssh-rsa AAAAB3NzaK8L93bWxnyp test@test.com END # Post the Virtual Machine spec to KubeVirt. kubectl create -f my-vmi.yaml Cloud-init user-data as base64 string \u00b6 In the example below, a simple bash script is base64 encoded and stored in the cloudInitNoCloud Volume's userDataBase64 field. There is a corresponding disks entry that references the cloud-init volume and assigns it to the VM's device. Users also have the option of storing the startup script in a Kubernetes Secret and referencing the Secret in the VM's spec. Examples further down in the document illustrate how that is done. # Create a simple startup script cat << END > startup-script.sh #!/bin/bash echo \"Hi from startup script!\" END # Create a VM manifest with the startup script base64 encoded into # a cloudInitNoCloud volume's userDataBase64 field. cat << END > my-vmi.yaml apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance metadata: name: myvmi spec: terminationGracePeriodSeconds: 5 domain: resources: requests: memory: 64M devices: disks: - name: containerdisk disk: bus: virtio - name: cloudinitdisk disk: bus: virtio volumes: - name: containerdisk containerDisk: image: kubevirt/cirros-container-disk-demo:latest - name: cloudinitdisk cloudInitNoCloud: userDataBase64: $(cat startup-script.sh | base64 -w0) END # Post the Virtual Machine spec to KubeVirt. kubectl create -f my-vmi.yaml Cloud-init UserData as k8s Secret \u00b6 Users who wish to not store the cloud-init userdata directly in the VirtualMachineInstance spec have the option to store the userdata into a Kubernetes Secret and reference that Secret in the spec. Multiple VirtualMachineInstance specs can reference the same Kubernetes Secret containing cloud-init userdata. Below is an example of how to create a Kubernetes Secret containing a startup script and reference that Secret in the VM's spec. # Create a simple startup script cat << END > startup-script.sh #!/bin/bash echo \"Hi from startup script!\" END # Store the startup script in a Kubernetes Secret kubectl create secret generic my-vmi-secret --from-file=userdata=startup-script.sh # Create a VM manifest and reference the Secret's name in the cloudInitNoCloud # Volume's secretRef field cat << END > my-vmi.yaml apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance metadata: name: myvmi spec: terminationGracePeriodSeconds: 5 domain: resources: requests: memory: 64M devices: disks: - name: containerdisk disk: bus: virtio - name: cloudinitdisk disk: bus: virtio volumes: - name: containerdisk containerDisk: image: kubevirt/cirros-registry-disk-demo:latest - name: cloudinitdisk cloudInitNoCloud: secretRef: name: my-vmi-secret END # Post the VM kubectl create -f my-vmi.yaml Injecting SSH keys with Cloud-init's Cloud-config \u00b6 In the examples so far, the cloud-init userdata script has been a bash script. Cloud-init has it's own configuration that can handle some common tasks such as user creation and SSH key injection. More cloud-config examples can be found here: Cloud-init Examples Below is an example of using cloud-config to inject an SSH key for the default user (fedora in this case) of a Fedora Atomic disk image. # Create the cloud-init cloud-config userdata. cat << END > startup-script #cloud-config password: atomic chpasswd: { expire: False } ssh_pwauth: False ssh_authorized_keys: - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC6zdgFiLr1uAK7PdcchDd+LseA5fEOcxCCt7TLlr7Mx6h8jUg+G+8L9JBNZuDzTZSF0dR7qwzdBBQjorAnZTmY3BhsKcFr8Gt4KMGrS6r3DNmGruP8GORvegdWZuXgASKVpXeI7nCIjRJwAaK1x+eGHwAWO9Z8ohcboHbLyffOoSZDSIuk2kRIc47+ENRjg0T6x2VRsqX27g6j4DfPKQZGk0zvXkZaYtr1e2tZgqTBWqZUloMJK8miQq6MktCKAS4VtPk0k7teQX57OGwD6D7uo4b+Cl8aYAAwhn0hc0C2USfbuVHgq88ESo2/+NwV4SQcl3sxCW21yGIjAGt4Hy7J fedora@localhost.localdomain END # Create the VM spec cat << END > my-vmi.yaml apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance metadata: name: sshvmi spec: terminationGracePeriodSeconds: 0 domain: resources: requests: memory: 1024M devices: disks: - name: containerdisk disk: dev: vda - name: cloudinitdisk disk: dev: vdb volumes: - name: containerdisk containerDisk: image: kubevirt/fedora-atomic-registry-disk-demo:latest - name: cloudinitdisk cloudInitNoCloud: userDataBase64: $(cat startup-script | base64 -w0) END # Post the VirtualMachineInstance spec to KubeVirt. kubectl create -f my-vmi.yaml # Connect to VM with passwordless SSH key ssh -i <insert private key here> fedora@<insert ip here> Inject SSH key using a Custom Shell Script \u00b6 Depending on the boot image in use, users may have a mixed experience using cloud-init's cloud-config to create users and inject SSH keys. Below is an example of creating a user and injecting SSH keys for that user using a script instead of cloud-config. cat << END > startup-script.sh #!/bin/bash export NEW_USER=\"foo\" export SSH_PUB_KEY=\"ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC6zdgFiLr1uAK7PdcchDd+LseA5fEOcxCCt7TLlr7Mx6h8jUg+G+8L9JBNZuDzTZSF0dR7qwzdBBQjorAnZTmY3BhsKcFr8Gt4KMGrS6r3DNmGruP8GORvegdWZuXgASKVpXeI7nCIjRJwAaK1x+eGHwAWO9Z8ohcboHbLyffOoSZDSIuk2kRIc47+ENRjg0T6x2VRsqX27g6j4DfPKQZGk0zvXkZaYtr1e2tZgqTBWqZUloMJK8miQq6MktCKAS4VtPk0k7teQX57OGwD6D7uo4b+Cl8aYAAwhn0hc0C2USfbuVHgq88ESo2/+NwV4SQcl3sxCW21yGIjAGt4Hy7J $NEW_USER@localhost.localdomain\" sudo adduser -U -m $NEW_USER echo \"$NEW_USER:atomic\" | chpasswd sudo mkdir /home/$NEW_USER/.ssh sudo echo \"$SSH_PUB_KEY\" > /home/$NEW_USER/.ssh/authorized_keys sudo chown -R ${NEW_USER}: /home/$NEW_USER/.ssh END # Create the VM spec cat << END > my-vmi.yaml apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance metadata: name: sshvmi spec: terminationGracePeriodSeconds: 0 domain: resources: requests: memory: 1024M devices: disks: - name: containerdisk disk: dev: vda - name: cloudinitdisk disk: dev: vdb volumes: - name: containerdisk containerDisk: image: kubevirt/fedora-atomic-registry-disk-demo:latest - name: cloudinitdisk cloudInitNoCloud: userDataBase64: $(cat startup-script.sh | base64 -w0) END # Post the VirtualMachineInstance spec to KubeVirt. kubectl create -f my-vmi.yaml # Connect to VM with passwordless SSH key ssh -i <insert private key here> foo@<insert ip here> Network Config \u00b6 A cloud-init network version 1 configuration can be set to configure the network at boot. Cloud-init user-data must be set for cloud-init to parse network-config even if it is just the user-data config header: #cloud-config Cloud-init network-config as clear text \u00b6 In the example below, a simple cloud-init network-config is stored in the cloudInitNoCloud Volume's networkData field as clean text. There is a corresponding disks entry that references the cloud-init volume and assigns it to the VM's device. # Create a VM manifest with the network-config in # a cloudInitNoCloud volume's networkData field. cat << END > my-vmi.yaml apiVersion: kubevirt.io/v1alpha2 kind: VirtualMachineInstance metadata: name: myvmi spec: terminationGracePeriodSeconds: 5 domain: resources: requests: memory: 64M devices: disks: - name: containerdisk volumeName: registryvolume disk: bus: virtio - name: cloudinitdisk volumeName: cloudinitvolume disk: bus: virtio volumes: - name: registryvolume containerDisk: image: kubevirt/cirros-container-disk-demo:latest - name: cloudinitvolume cloudInitNoCloud: userData: \"#cloud-config\" networkData: | network: version: 1 config: - type: physical name: eth0 subnets: - type: dhcp END # Post the Virtual Machine spec to KubeVirt. kubectl create -f my-vmi.yaml Cloud-init network-config as base64 string \u00b6 In the example below, a simple network-config is base64 encoded and stored in the cloudInitNoCloud Volume's networkDataBase64 field. There is a corresponding disks entry that references the cloud-init volume and assigns it to the VM's device. Users also have the option of storing the network-config in a Kubernetes Secret and referencing the Secret in the VM's spec. Examples further down in the document illustrate how that is done. # Create a simple network-config cat << END > network-config network: version: 1 config: - type: physical name: eth0 subnets: - type: dhcp END # Create a VM manifest with the networkData base64 encoded into # a cloudInitNoCloud volume's networkDataBase64 field. cat << END > my-vmi.yaml apiVersion: kubevirt.io/v1alpha2 kind: VirtualMachineInstance metadata: name: myvmi spec: terminationGracePeriodSeconds: 5 domain: resources: requests: memory: 64M devices: disks: - name: containerdisk volumeName: registryvolume disk: bus: virtio - name: cloudinitdisk volumeName: cloudinitvolume disk: bus: virtio volumes: - name: registryvolume containerDisk: image: kubevirt/cirros-container-disk-demo:latest - name: cloudinitvolume cloudInitNoCloud: userData: \"#cloud-config\" networkDataBase64: $(cat network-config | base64 -w0) END # Post the Virtual Machine spec to KubeVirt. kubectl create -f my-vmi.yaml Cloud-init network-config as k8s Secret \u00b6 Users who wish to not store the cloud-init network-config directly in the VirtualMachineInstance spec have the option to store the network-config into a Kubernetes Secret and reference that Secret in the spec. Multiple VirtualMachineInstance specs can reference the same Kubernetes Secret containing cloud-init network-config. Below is an example of how to create a Kubernetes Secret containing a network-config and reference that Secret in the VM's spec. # Create a simple network-config cat << END > network-config network: version: 1 config: - type: physical name: eth0 subnets: - type: dhcp END # Store the network-config in a Kubernetes Secret kubectl create secret generic my-vmi-secret --from-file=networkdata=network-config # Create a VM manifest and reference the Secret's name in the cloudInitNoCloud # Volume's secretRef field cat << END > my-vmi.yaml apiVersion: kubevirt.io/v1alpha2 kind: VirtualMachineInstance metadata: name: myvmi spec: terminationGracePeriodSeconds: 5 domain: resources: requests: memory: 64M devices: disks: - name: containerdisk volumeName: registryvolume disk: bus: virtio - name: cloudinitdisk volumeName: cloudinitvolume disk: bus: virtio volumes: - name: registryvolume containerDisk: image: kubevirt/cirros-registry-disk-demo:latest - name: cloudinitvolume cloudInitNoCloud: userData: \"#cloud-config\" networkDataSecretRef: name: my-vmi-secret END # Post the VM kubectl create -f my-vmi.yaml Debugging \u00b6 Depending on the operating system distribution in use, cloud-init output is often printed to the console output on boot up. When developing userdata scripts, users can connect to the VM's console during boot up to debug. Example of connecting to console using virtctl: virtctl console <name of vmi> Device Role Tagging \u00b6 KubeVirt provides a mechanism for users to tag devices such as Network Interfaces with a specific role. The tag will be matched to the hardware address of the device and this mapping exposed to the guest OS via cloud-init. This additional metadata will help the guest OS users with multiple networks interfaces to identify the devices that may have a specific role, such as a network device dedicated to a specific service or a disk intended to be used by a specific application (database, webcache, etc.) This functionality already exists in platforms such as OpenStack. KubeVirt will provide the data in a similar format, known to users and services like cloud-init. For example: kind: VirtualMachineInstance spec: domain: devices: interfaces: - masquerade: {} name: default - bridge: {} name: ptp tag: ptp - name: sriov-net sriov: {} tag: nfvfunc networks: - name: default pod: {} - multus: networkName: ptp-conf name: ptp networkName: sriov/sriov-network name: sriov-net The metadata will be available in the guests config drive `openstack/latest/meta_data.json` { \"devices\": [ { \"type\": \"nic\", \"bus\": \"pci\", \"address\": \"0000:00:02.0\", \"mac\": \"01:22:22:42:22:21\", \"tags\": [\"ptp\"] }, { \"type\": \"nic\", \"bus\": \"pci\", \"address\": \"0000:81:10.1\", \"mac\": \"01:22:22:42:22:22\", \"tags\": [\"nfvfunc\"] }, ] } Sysprep Examples \u00b6 Sysprep in a ConfigMap \u00b6 The answer file can be provided in a ConfigMap: apiVersion: v1 kind: ConfigMap metadata: name: sysprep-config data: autounattend.xml: | <?xml version=\"1.0\" encoding=\"utf-8\"?> <unattend xmlns=\"urn:schemas-microsoft-com:unattend\"> ... </unattend> And attached to the VM like so: kind: VirtualMachine metadata: name: windows-with-sysprep spec: running: false template: metadata: labels: kubevirt.io/domain: windows-with-sysprep spec: domain: cpu: cores: 3 devices: disks: - bootOrder: 1 disk: bus: virtio name: harddrive - name: sysprep cdrom: bus: sata machine: type: q35 resources: requests: memory: 6G volumes: - name: harddrive persistentVolumeClaim: claimName: windows_pvc - name: sysprep sysprep: configMap: name: sysprep-config Sysprep in a Secret \u00b6 The answer file can be provided in a Secret: apiVersion: v1 kind: Secret metadata: name: sysprep-config stringData: data: autounattend.xml: | <?xml version=\"1.0\" encoding=\"utf-8\"?> <unattend xmlns=\"urn:schemas-microsoft-com:unattend\"> ... </unattend> And attached to the VM like so: kind: VirtualMachine metadata: name: windows-with-sysprep spec: running: false template: metadata: labels: kubevirt.io/domain: windows-with-sysprep spec: domain: cpu: cores: 3 devices: disks: - bootOrder: 1 disk: bus: virtio name: harddrive - name: sysprep cdrom: bus: sata machine: type: q35 resources: requests: memory: 6G volumes: - name: harddrive persistentVolumeClaim: claimName: windows_pvc - name: sysprep sysprep: secret: name: sysprep-secret Base Sysprep VM \u00b6 In the example below, a configMap with autounattend.xml file is used to modify the Windows iso image which is downloaded from Microsoft and creates a base installed Windows machine with virtio drivers installed and all the commands executed in post-install.ps1 For the below manifests to work it needs to have win10-iso DataVolume. apiVersion: v1 kind: ConfigMap metadata: name: win10-template-configmap data: autounattend.xml: |- <?xml version=\"1.0\" encoding=\"utf-8\"?> <unattend xmlns=\"urn:schemas-microsoft-com:unattend\"> <settings pass=\"windowsPE\"> <component xmlns:wcm=\"http://schemas.microsoft.com/WMIConfig/2002/State\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" name=\"Microsoft-Windows-International-Core-WinPE\" processorArchitecture=\"amd64\" publicKeyToken=\"31bf3856ad364e35\" language=\"neutral\" versionScope=\"nonSxS\"> <SetupUILanguage> <UILanguage>en-US</UILanguage> </SetupUILanguage> <InputLocale>0409:00000409</InputLocale> <SystemLocale>en-US</SystemLocale> <UILanguage>en-US</UILanguage> <UILanguageFallback>en-US</UILanguageFallback> <UserLocale>en-US</UserLocale> </component> <component xmlns:wcm=\"http://schemas.microsoft.com/WMIConfig/2002/State\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" name=\"Microsoft-Windows-PnpCustomizationsWinPE\" processorArchitecture=\"amd64\" publicKeyToken=\"31bf3856ad364e35\" language=\"neutral\" versionScope=\"nonSxS\"> <DriverPaths> <PathAndCredentials wcm:keyValue=\"4b29ba63\" wcm:action=\"add\"> <Path>E:\\amd64\\2k19</Path> </PathAndCredentials> <PathAndCredentials wcm:keyValue=\"25fe51ea\" wcm:action=\"add\"> <Path>E:\\NetKVM\\2k19\\amd64</Path> </PathAndCredentials> </DriverPaths> </component> <component xmlns:wcm=\"http://schemas.microsoft.com/WMIConfig/2002/State\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" name=\"Microsoft-Windows-Setup\" processorArchitecture=\"amd64\" publicKeyToken=\"31bf3856ad364e35\" language=\"neutral\" versionScope=\"nonSxS\"> <DiskConfiguration> <Disk wcm:action=\"add\"> <CreatePartitions> <CreatePartition wcm:action=\"add\"> <Order>1</Order> <Type>Primary</Type> <Size>100</Size> </CreatePartition> <CreatePartition wcm:action=\"add\"> <Extend>true</Extend> <Order>2</Order> <Type>Primary</Type> </CreatePartition> </CreatePartitions> <ModifyPartitions> <ModifyPartition wcm:action=\"add\"> <Format>NTFS</Format> <Label>System Reserved</Label> <Order>1</Order> <PartitionID>1</PartitionID> <TypeID>0x27</TypeID> </ModifyPartition> <ModifyPartition wcm:action=\"add\"> <Format>NTFS</Format> <Label>OS</Label> <Letter>C</Letter> <Order>2</Order> <PartitionID>2</PartitionID> </ModifyPartition> </ModifyPartitions> <DiskID>0</DiskID> <WillWipeDisk>true</WillWipeDisk> </Disk> </DiskConfiguration> <ImageInstall> <OSImage> <InstallFrom> <MetaData wcm:action=\"add\"> <Key>/Image/Description</Key> <Value>Windows 10 Pro</Value> </MetaData> </InstallFrom> <InstallTo> <DiskID>0</DiskID> <PartitionID>2</PartitionID> </InstallTo> </OSImage> </ImageInstall> <UserData> <AcceptEula>true</AcceptEula> <FullName/> <Organization/> <ProductKey> <Key/> </ProductKey> </UserData> </component> </settings> <settings pass=\"offlineServicing\"> <component xmlns:wcm=\"http://schemas.microsoft.com/WMIConfig/2002/State\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" name=\"Microsoft-Windows-LUA-Settings\" processorArchitecture=\"amd64\" publicKeyToken=\"31bf3856ad364e35\" language=\"neutral\" versionScope=\"nonSxS\"> <EnableLUA>false</EnableLUA> </component> </settings> <settings pass=\"specialize\"> <component xmlns:wcm=\"http://schemas.microsoft.com/WMIConfig/2002/State\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" name=\"Microsoft-Windows-International-Core\" processorArchitecture=\"amd64\" publicKeyToken=\"31bf3856ad364e35\" language=\"neutral\" versionScope=\"nonSxS\"> <InputLocale>0409:00000409</InputLocale> <SystemLocale>en-US</SystemLocale> <UILanguage>en-US</UILanguage> <UILanguageFallback>en-US</UILanguageFallback> <UserLocale>en-US</UserLocale> </component> <component xmlns:wcm=\"http://schemas.microsoft.com/WMIConfig/2002/State\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" name=\"Microsoft-Windows-Security-SPP-UX\" processorArchitecture=\"amd64\" publicKeyToken=\"31bf3856ad364e35\" language=\"neutral\" versionScope=\"nonSxS\"> <SkipAutoActivation>true</SkipAutoActivation> </component> <component xmlns:wcm=\"http://schemas.microsoft.com/WMIConfig/2002/State\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" name=\"Microsoft-Windows-SQMApi\" processorArchitecture=\"amd64\" publicKeyToken=\"31bf3856ad364e35\" language=\"neutral\" versionScope=\"nonSxS\"> <CEIPEnabled>0</CEIPEnabled> </component> </settings> <settings pass=\"oobeSystem\"> <component xmlns:wcm=\"http://schemas.microsoft.com/WMIConfig/2002/State\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" name=\"Microsoft-Windows-Shell-Setup\" processorArchitecture=\"amd64\" publicKeyToken=\"31bf3856ad364e35\" language=\"neutral\" versionScope=\"nonSxS\"> <OOBE> <HideEULAPage>true</HideEULAPage> <HideOEMRegistrationScreen>true</HideOEMRegistrationScreen> <HideOnlineAccountScreens>true</HideOnlineAccountScreens> <HideWirelessSetupInOOBE>true</HideWirelessSetupInOOBE> <NetworkLocation>Work</NetworkLocation> <SkipUserOOBE>true</SkipUserOOBE> <SkipMachineOOBE>true</SkipMachineOOBE> <ProtectYourPC>3</ProtectYourPC> </OOBE> <AutoLogon> <Password> <Value>123456</Value> <PlainText>true</PlainText> </Password> <Enabled>true</Enabled> <Username>Administrator</Username> </AutoLogon> <UserAccounts> <AdministratorPassword> <Value>123456</Value> <PlainText>true</PlainText> </AdministratorPassword> </UserAccounts> <RegisteredOrganization/> <RegisteredOwner/> <TimeZone>Eastern Standard Time</TimeZone> <FirstLogonCommands> <SynchronousCommand wcm:action=\"add\"> <CommandLine>powershell -ExecutionPolicy Bypass -NoExit -NoProfile f:\\post-install.ps1</CommandLine> <RequiresUserInput>false</RequiresUserInput> <Order>1</Order> <Description>Post Installation Script</Description> </SynchronousCommand> </FirstLogonCommands> </component> </settings> </unattend> post-install.ps1: |- # Remove AutoLogin # https://docs.microsoft.com/en-us/windows-hardware/customize/desktop/unattend/microsoft-windows-shell-setup-autologon-logoncount#logoncount-known-issue reg add \"HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\\Winlogon\" /v AutoAdminLogon /t REG_SZ /d 0 /f # install Qemu Tools (Drivers) Start-Process msiexec -Wait -ArgumentList '/i e:\\virtio-win-gt-x64.msi /qn /passive /norestart' # install Guest Agent Start-Process msiexec -Wait -ArgumentList '/i e:\\guest-agent\\qemu-ga-x86_64.msi /qn /passive /norestart' # Rename cached unattend.xml to avoid it is picked up by sysprep mv C:\\Windows\\Panther\\unattend.xml C:\\Windows\\Panther\\unattend.install.xml # Eject CD, to avoid that the autounattend.xml on the CD is picked up by sysprep (new-object -COM Shell.Application).NameSpace(17).ParseName('F:').InvokeVerb('Eject') # Run Sysprep and Shutdown C:\\Windows\\System32\\Sysprep\\sysprep.exe /generalize /oobe /shutdown /mode:vm --- apiVersion: kubevirt.io/v1 kind: VirtualMachine metadata: annotations: name.os.template.kubevirt.io/win10: Microsoft Windows 10 vm.kubevirt.io/validations: | [ { \"name\": \"minimal-required-memory\", \"path\": \"jsonpath::.spec.domain.resources.requests.memory\", \"rule\": \"integer\", \"message\": \"This VM requires more memory.\", \"min\": 2147483648 }, { \"name\": \"windows-virtio-bus\", \"path\": \"jsonpath::.spec.domain.devices.disks[*].disk.bus\", \"valid\": \"jsonpath::.spec.domain.devices.disks[*].disk.bus\", \"rule\": \"enum\", \"message\": \"virto disk bus type has better performance, install virtio drivers in VM and change bus type\", \"values\": [\"virtio\"], \"justWarning\": true }, { \"name\": \"windows-disk-bus\", \"path\": \"jsonpath::.spec.domain.devices.disks[*].disk.bus\", \"valid\": \"jsonpath::.spec.domain.devices.disks[*].disk.bus\", \"rule\": \"enum\", \"message\": \"disk bus has to be either virtio or sata or scsi\", \"values\": [\"virtio\", \"sata\", \"scsi\"] }, { \"name\": \"windows-cd-bus\", \"path\": \"jsonpath::.spec.domain.devices.disks[*].cdrom.bus\", \"valid\": \"jsonpath::.spec.domain.devices.disks[*].cdrom.bus\", \"rule\": \"enum\", \"message\": \"cd bus has to be sata\", \"values\": [\"sata\"] } ] name: win10-template namespace: default labels: app: win10-template flavor.template.kubevirt.io/medium: 'true' os.template.kubevirt.io/win10: 'true' vm.kubevirt.io/template: windows10-desktop-medium vm.kubevirt.io/template.namespace: openshift vm.kubevirt.io/template.revision: '1' vm.kubevirt.io/template.version: v0.14.0 workload.template.kubevirt.io/desktop: 'true' spec: runStrategy: RerunOnFailure dataVolumeTemplates: - metadata: name: win10-template-windows-iso spec: pvc: accessModes: - ReadWriteOnce resources: requests: storage: 20Gi source: pvc: name: windows10-iso namespace: default - metadata: name: win10-template spec: pvc: accessModes: - ReadWriteOnce resources: requests: storage: 25Gi volumeMode: Filesystem source: blank: {} template: metadata: annotations: vm.kubevirt.io/flavor: medium vm.kubevirt.io/os: windows10 vm.kubevirt.io/workload: desktop labels: flavor.template.kubevirt.io/medium: 'true' kubevirt.io/domain: win10-template kubevirt.io/size: medium os.template.kubevirt.io/win10: 'true' vm.kubevirt.io/name: win10-template workload.template.kubevirt.io/desktop: 'true' spec: domain: clock: timer: hpet: present: false hyperv: {} pit: tickPolicy: delay rtc: tickPolicy: catchup utc: {} cpu: cores: 1 sockets: 1 threads: 1 devices: disks: - bootOrder: 1 disk: bus: virtio name: win10-template - bootOrder: 2 cdrom: bus: sata name: windows-iso - cdrom: bus: sata name: windows-guest-tools - name: sysprep cdrom: bus: sata inputs: - bus: usb name: tablet type: tablet interfaces: - masquerade: {} model: virtio name: default features: acpi: {} apic: {} hyperv: reenlightenment: {} ipi: {} synic: {} synictimer: direct: {} spinlocks: spinlocks: 8191 reset: {} relaxed: {} vpindex: {} runtime: {} tlbflush: {} frequencies: {} vapic: {} machine: type: pc-q35-rhel8.4.0 resources: requests: memory: 4Gi hostname: win10-template networks: - name: default pod: {} volumes: - dataVolume: name: win10-iso name: windows-iso - dataVolume: name: win10-template name: win10-template - containerDisk: image: kubevirt/virtio-container-disk name: windows-guest-tools - name: sysprep sysprep: configMap: name: win10-template-configmap Launching a VM from template \u00b6 From the above example after the sysprep command is executed in the post-install.ps1 and the vm is in shutdown state, A new VM can be launched from the base win10-template with additional changes mentioned from the below unattend.xml in sysprep-config . The new VM can take upto 5 minutes to be in running state since Windows goes through oobe setup in the background with the customizations specified in the below unattend.xml file. apiVersion: v1 kind: ConfigMap metadata: name: sysprep-config data: autounattend.xml: |- <?xml version=\"1.0\" encoding=\"utf-8\"?> <!-- responsible for installing windows, ignored on sysprepped images --> unattend.xml: |- <?xml version=\"1.0\" encoding=\"utf-8\"?> <unattend xmlns=\"urn:schemas-microsoft-com:unattend\"> <settings pass=\"oobeSystem\"> <component xmlns:wcm=\"http://schemas.microsoft.com/WMIConfig/2002/State\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" name=\"Microsoft-Windows-Shell-Setup\" processorArchitecture=\"amd64\" publicKeyToken=\"31bf3856ad364e35\" language=\"neutral\" versionScope=\"nonSxS\"> <OOBE> <HideEULAPage>true</HideEULAPage> <HideOEMRegistrationScreen>true</HideOEMRegistrationScreen> <HideOnlineAccountScreens>true</HideOnlineAccountScreens> <HideWirelessSetupInOOBE>true</HideWirelessSetupInOOBE> <NetworkLocation>Work</NetworkLocation> <SkipUserOOBE>true</SkipUserOOBE> <SkipMachineOOBE>true</SkipMachineOOBE> <ProtectYourPC>3</ProtectYourPC> </OOBE> <AutoLogon> <Password> <Value>123456</Value> <PlainText>true</PlainText> </Password> <Enabled>true</Enabled> <Username>Administrator</Username> </AutoLogon> <UserAccounts> <AdministratorPassword> <Value>123456</Value> <PlainText>true</PlainText> </AdministratorPassword> </UserAccounts> <RegisteredOrganization>Kuebvirt</RegisteredOrganization> <RegisteredOwner>Kubevirt</RegisteredOwner> <TimeZone>Eastern Standard Time</TimeZone> <FirstLogonCommands> <SynchronousCommand wcm:action=\"add\"> <CommandLine>powershell -ExecutionPolicy Bypass -NoExit -WindowStyle Hidden -NoProfile d:\\customize.ps1</CommandLine> <RequiresUserInput>false</RequiresUserInput> <Order>1</Order> <Description>Customize Script</Description> </SynchronousCommand> </FirstLogonCommands> </component> </settings> </unattend> customize.ps1: |- # Enable RDP Set-ItemProperty -Path 'HKLM:\\System\\CurrentControlSet\\Control\\Terminal Server' -name \"fDenyTSConnections\" -value 0 Enable-NetFirewallRule -DisplayGroup \"Remote Desktop\" # https://docs.microsoft.com/en-us/windows-server/administration/openssh/openssh_install_firstuse # Install the OpenSSH Server Add-WindowsCapability -Online -Name OpenSSH.Server~~~~0.0.1.0 # Start the sshd service Start-Service sshd Set-Service -Name sshd -StartupType 'Automatic' # https://docs.microsoft.com/en-us/windows-server/administration/openssh/openssh_server_configuration # use powershell as default shell for ssh New-ItemProperty -Path \"HKLM:\\SOFTWARE\\OpenSSH\" -Name DefaultShell -Value \"C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\powershell.exe\" -PropertyType String -Force # Add ssh authorized_key for administrator # https://docs.microsoft.com/en-us/windows-server/administration/openssh/openssh_keymanagement $MyDir = $MyInvocation.MyCommand.Path | Split-Path -Parent $PublicKey = Get-Content -Path $MyDir\\id_rsa.pub $authrized_keys_path = $env:ProgramData + \"\\ssh\\administrators_authorized_keys\" Add-Content -Path $authrized_keys_path -Value $PublicKey icacls.exe $authrized_keys_path /inheritance:r /grant \"Administrators:F\" /grant \"SYSTEM:F\" # install application via exe file installer from url function Install-Exe { $dlurl = $args[0] $installerPath = Join-Path $env:TEMP (Split-Path $dlurl -Leaf) Invoke-WebRequest -UseBasicParsing $dlurl -OutFile $installerPath Start-Process -FilePath $installerPath -Args \"/S\" -Verb RunAs -Wait Remove-Item $installerPath } # Wait for networking before running a task at startup do { $ping = test-connection -comp kubevirt.io -count 1 -Quiet } until ($ping) # Installing the Latest Notepad++ with PowerShell $BaseUri = \"https://notepad-plus-plus.org\" $BasePage = Invoke-WebRequest -Uri $BaseUri -UseBasicParsing $ChildPath = $BasePage.Links | Where-Object { $_.outerHTML -like '*Current Version*' } | Select-Object -ExpandProperty href $DownloadPageUri = $BaseUri + $ChildPath $DownloadPage = Invoke-WebRequest -Uri $DownloadPageUri -UseBasicParsing $DownloadUrl = $DownloadPage.Links | Where-Object { $_.outerHTML -like '*npp.*.Installer.x64.exe\"*' } | Select-Object -ExpandProperty href Install-Exe $DownloadUrl id_rsa.pub: |- ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC6zdgFiLr1uAK7PdcchDd+LseA5fEOcxCCt7TLlr7Mx6h8jUg+G+8L9JBNZuDzTZSF0dR7qwzdBBQjorAnZTmY3BhsKcFr8Gt4KMGrS6r3DNmGruP8GORvegdWZuXgASKVpXeI7nCIjRJwAaK1x+eGHwAWO9Z8ohcboHbLyffOoSZDSIuk2kRIc47+ENRjg0T6x2VRsqX27g6j4DfPKQZGk0zvXkZaYtr1e2tZgqTBWqZUloMJK8miQq6MktCKAS4VtPk0k7teQX57OGwD6D7uo4b+Cl8aYAAwhn0hc0C2USfbuVHgq88ESo2/+NwV4SQcl3sxCW21yGIjAGt4Hy7J fedora@localhost.localdomain","title":"Startup Scripts"},{"location":"virtual_machines/startup_scripts/#startup-scripts","text":"KubeVirt supports the ability to assign a startup script to a VirtualMachineInstance instance which is executed automatically when the VM initializes. These scripts are commonly used to automate injection of users and SSH keys into VMs in order to provide remote access to the machine. For example, a startup script can be used to inject credentials into a VM that allows an Ansible job running on a remote host to access and provision the VM. Startup scripts are not limited to any specific use case though. They can be used to run any arbitrary script in a VM on boot.","title":"Startup Scripts"},{"location":"virtual_machines/startup_scripts/#cloud-init","text":"cloud-init is a widely adopted project used for early initialization of a VM. Used by cloud providers such as AWS and GCP, cloud-init has established itself as the defacto method of providing startup scripts to VMs. Cloud-init documentation can be found here: Cloud-init Documentation . KubeVirt supports cloud-init's \"NoCloud\" and \"ConfigDrive\" datasources which involve injecting startup scripts into a VM instance through the use of an ephemeral disk. VMs with the cloud-init package installed will detect the ephemeral disk and execute custom userdata scripts at boot.","title":"Cloud-init"},{"location":"virtual_machines/startup_scripts/#sysprep","text":"Sysprep is an automation tool for Windows that automates Windows installation, setup, and custom software provisioning. The general flow is: Seal the vm image with the Sysprep tool, for example by running: %WINDIR%\\system32\\sysprep\\sysprep.exe /generalize /shutdown /oobe /mode:vm Note We need to make sure the base vm does not restart, which can be done by setting the vm run strategy as RerunOnFailure . VM runStrategy: spec: runStrategy: RerunOnFailure More information can be found here: Sysprep Process Overview Sysprep (Generalize) a Windows installation Note It is important that there is no answer file detected when the Sysprep Tool is triggered, because Windows Setup searches for answer files at the beginning of each configuration pass and caches it. If that happens, when the OS will start - it will just use the cached answer file, ignoring the one we provide through the Sysprep API. More information can be found here . Providing an Answer file named autounattend.xml in an attached media. The answer file can be provided in a ConfigMap or a Secret with the key autounattend.xml More information can be found here: Answer files (unattend.xml) Note There are also many easy to find online tools available for creating an answer file.","title":"Sysprep"},{"location":"virtual_machines/startup_scripts/#cloud-init-examples","text":"","title":"Cloud-init Examples"},{"location":"virtual_machines/startup_scripts/#user-data","text":"KubeVirt supports the cloud-init NoCloud and ConfigDrive data sources which involve injecting startup scripts through the use of a disk attached to the VM. In order to assign a custom userdata script to a VirtualMachineInstance using this method, users must define a disk and a volume for the NoCloud or ConfigDrive datasource in the VirtualMachineInstance's spec.","title":"User Data"},{"location":"virtual_machines/startup_scripts/#data-sources","text":"Under most circumstances users should stick to the NoCloud data source as it is the simplest cloud-init data source. Only if NoCloud is not supported by the cloud-init implementation (e.g. coreos-cloudinit ) users should switch the data source to ConfigDrive. Switching the cloud-init data source to ConfigDrive is as easy as changing the volume type in the VirtualMachineInstance's spec from cloudInitNoCloud to cloudInitConfigDrive . NoCloud data source: volumes: - name: cloudinitvolume cloudInitNoCloud: userData: \"#cloud-config\" ConfigDrive data source: volumes: - name: cloudinitvolume cloudInitConfigDrive: userData: \"#cloud-config\" See the examples below for more complete cloud-init examples.","title":"Data Sources"},{"location":"virtual_machines/startup_scripts/#cloud-init-user-data-as-clear-text","text":"In the example below, a SSH key is stored in the cloudInitNoCloud Volume's userData field as clean text. There is a corresponding disks entry that references the cloud-init volume and assigns it to the VM's device. # Create a VM manifest with the startup script # a cloudInitNoCloud volume's userData field. cat << END > my-vmi.yaml apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance metadata: name: myvmi spec: terminationGracePeriodSeconds: 5 domain: resources: requests: memory: 64M devices: disks: - name: containerdisk disk: bus: virtio - name: cloudinitdisk disk: bus: virtio volumes: - name: containerdisk containerDisk: image: kubevirt/cirros-container-disk-demo:latest - name: cloudinitdisk cloudInitNoCloud: userData: | ssh_authorized_keys: - ssh-rsa AAAAB3NzaK8L93bWxnyp test@test.com END # Post the Virtual Machine spec to KubeVirt. kubectl create -f my-vmi.yaml","title":"Cloud-init user-data as clear text"},{"location":"virtual_machines/startup_scripts/#cloud-init-user-data-as-base64-string","text":"In the example below, a simple bash script is base64 encoded and stored in the cloudInitNoCloud Volume's userDataBase64 field. There is a corresponding disks entry that references the cloud-init volume and assigns it to the VM's device. Users also have the option of storing the startup script in a Kubernetes Secret and referencing the Secret in the VM's spec. Examples further down in the document illustrate how that is done. # Create a simple startup script cat << END > startup-script.sh #!/bin/bash echo \"Hi from startup script!\" END # Create a VM manifest with the startup script base64 encoded into # a cloudInitNoCloud volume's userDataBase64 field. cat << END > my-vmi.yaml apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance metadata: name: myvmi spec: terminationGracePeriodSeconds: 5 domain: resources: requests: memory: 64M devices: disks: - name: containerdisk disk: bus: virtio - name: cloudinitdisk disk: bus: virtio volumes: - name: containerdisk containerDisk: image: kubevirt/cirros-container-disk-demo:latest - name: cloudinitdisk cloudInitNoCloud: userDataBase64: $(cat startup-script.sh | base64 -w0) END # Post the Virtual Machine spec to KubeVirt. kubectl create -f my-vmi.yaml","title":"Cloud-init user-data as base64 string"},{"location":"virtual_machines/startup_scripts/#cloud-init-userdata-as-k8s-secret","text":"Users who wish to not store the cloud-init userdata directly in the VirtualMachineInstance spec have the option to store the userdata into a Kubernetes Secret and reference that Secret in the spec. Multiple VirtualMachineInstance specs can reference the same Kubernetes Secret containing cloud-init userdata. Below is an example of how to create a Kubernetes Secret containing a startup script and reference that Secret in the VM's spec. # Create a simple startup script cat << END > startup-script.sh #!/bin/bash echo \"Hi from startup script!\" END # Store the startup script in a Kubernetes Secret kubectl create secret generic my-vmi-secret --from-file=userdata=startup-script.sh # Create a VM manifest and reference the Secret's name in the cloudInitNoCloud # Volume's secretRef field cat << END > my-vmi.yaml apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance metadata: name: myvmi spec: terminationGracePeriodSeconds: 5 domain: resources: requests: memory: 64M devices: disks: - name: containerdisk disk: bus: virtio - name: cloudinitdisk disk: bus: virtio volumes: - name: containerdisk containerDisk: image: kubevirt/cirros-registry-disk-demo:latest - name: cloudinitdisk cloudInitNoCloud: secretRef: name: my-vmi-secret END # Post the VM kubectl create -f my-vmi.yaml","title":"Cloud-init UserData as k8s Secret"},{"location":"virtual_machines/startup_scripts/#injecting-ssh-keys-with-cloud-inits-cloud-config","text":"In the examples so far, the cloud-init userdata script has been a bash script. Cloud-init has it's own configuration that can handle some common tasks such as user creation and SSH key injection. More cloud-config examples can be found here: Cloud-init Examples Below is an example of using cloud-config to inject an SSH key for the default user (fedora in this case) of a Fedora Atomic disk image. # Create the cloud-init cloud-config userdata. cat << END > startup-script #cloud-config password: atomic chpasswd: { expire: False } ssh_pwauth: False ssh_authorized_keys: - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC6zdgFiLr1uAK7PdcchDd+LseA5fEOcxCCt7TLlr7Mx6h8jUg+G+8L9JBNZuDzTZSF0dR7qwzdBBQjorAnZTmY3BhsKcFr8Gt4KMGrS6r3DNmGruP8GORvegdWZuXgASKVpXeI7nCIjRJwAaK1x+eGHwAWO9Z8ohcboHbLyffOoSZDSIuk2kRIc47+ENRjg0T6x2VRsqX27g6j4DfPKQZGk0zvXkZaYtr1e2tZgqTBWqZUloMJK8miQq6MktCKAS4VtPk0k7teQX57OGwD6D7uo4b+Cl8aYAAwhn0hc0C2USfbuVHgq88ESo2/+NwV4SQcl3sxCW21yGIjAGt4Hy7J fedora@localhost.localdomain END # Create the VM spec cat << END > my-vmi.yaml apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance metadata: name: sshvmi spec: terminationGracePeriodSeconds: 0 domain: resources: requests: memory: 1024M devices: disks: - name: containerdisk disk: dev: vda - name: cloudinitdisk disk: dev: vdb volumes: - name: containerdisk containerDisk: image: kubevirt/fedora-atomic-registry-disk-demo:latest - name: cloudinitdisk cloudInitNoCloud: userDataBase64: $(cat startup-script | base64 -w0) END # Post the VirtualMachineInstance spec to KubeVirt. kubectl create -f my-vmi.yaml # Connect to VM with passwordless SSH key ssh -i <insert private key here> fedora@<insert ip here>","title":"Injecting SSH keys with Cloud-init's Cloud-config"},{"location":"virtual_machines/startup_scripts/#inject-ssh-key-using-a-custom-shell-script","text":"Depending on the boot image in use, users may have a mixed experience using cloud-init's cloud-config to create users and inject SSH keys. Below is an example of creating a user and injecting SSH keys for that user using a script instead of cloud-config. cat << END > startup-script.sh #!/bin/bash export NEW_USER=\"foo\" export SSH_PUB_KEY=\"ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC6zdgFiLr1uAK7PdcchDd+LseA5fEOcxCCt7TLlr7Mx6h8jUg+G+8L9JBNZuDzTZSF0dR7qwzdBBQjorAnZTmY3BhsKcFr8Gt4KMGrS6r3DNmGruP8GORvegdWZuXgASKVpXeI7nCIjRJwAaK1x+eGHwAWO9Z8ohcboHbLyffOoSZDSIuk2kRIc47+ENRjg0T6x2VRsqX27g6j4DfPKQZGk0zvXkZaYtr1e2tZgqTBWqZUloMJK8miQq6MktCKAS4VtPk0k7teQX57OGwD6D7uo4b+Cl8aYAAwhn0hc0C2USfbuVHgq88ESo2/+NwV4SQcl3sxCW21yGIjAGt4Hy7J $NEW_USER@localhost.localdomain\" sudo adduser -U -m $NEW_USER echo \"$NEW_USER:atomic\" | chpasswd sudo mkdir /home/$NEW_USER/.ssh sudo echo \"$SSH_PUB_KEY\" > /home/$NEW_USER/.ssh/authorized_keys sudo chown -R ${NEW_USER}: /home/$NEW_USER/.ssh END # Create the VM spec cat << END > my-vmi.yaml apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance metadata: name: sshvmi spec: terminationGracePeriodSeconds: 0 domain: resources: requests: memory: 1024M devices: disks: - name: containerdisk disk: dev: vda - name: cloudinitdisk disk: dev: vdb volumes: - name: containerdisk containerDisk: image: kubevirt/fedora-atomic-registry-disk-demo:latest - name: cloudinitdisk cloudInitNoCloud: userDataBase64: $(cat startup-script.sh | base64 -w0) END # Post the VirtualMachineInstance spec to KubeVirt. kubectl create -f my-vmi.yaml # Connect to VM with passwordless SSH key ssh -i <insert private key here> foo@<insert ip here>","title":"Inject SSH key using a Custom Shell Script"},{"location":"virtual_machines/startup_scripts/#network-config","text":"A cloud-init network version 1 configuration can be set to configure the network at boot. Cloud-init user-data must be set for cloud-init to parse network-config even if it is just the user-data config header: #cloud-config","title":"Network Config"},{"location":"virtual_machines/startup_scripts/#cloud-init-network-config-as-clear-text","text":"In the example below, a simple cloud-init network-config is stored in the cloudInitNoCloud Volume's networkData field as clean text. There is a corresponding disks entry that references the cloud-init volume and assigns it to the VM's device. # Create a VM manifest with the network-config in # a cloudInitNoCloud volume's networkData field. cat << END > my-vmi.yaml apiVersion: kubevirt.io/v1alpha2 kind: VirtualMachineInstance metadata: name: myvmi spec: terminationGracePeriodSeconds: 5 domain: resources: requests: memory: 64M devices: disks: - name: containerdisk volumeName: registryvolume disk: bus: virtio - name: cloudinitdisk volumeName: cloudinitvolume disk: bus: virtio volumes: - name: registryvolume containerDisk: image: kubevirt/cirros-container-disk-demo:latest - name: cloudinitvolume cloudInitNoCloud: userData: \"#cloud-config\" networkData: | network: version: 1 config: - type: physical name: eth0 subnets: - type: dhcp END # Post the Virtual Machine spec to KubeVirt. kubectl create -f my-vmi.yaml","title":"Cloud-init network-config as clear text"},{"location":"virtual_machines/startup_scripts/#cloud-init-network-config-as-base64-string","text":"In the example below, a simple network-config is base64 encoded and stored in the cloudInitNoCloud Volume's networkDataBase64 field. There is a corresponding disks entry that references the cloud-init volume and assigns it to the VM's device. Users also have the option of storing the network-config in a Kubernetes Secret and referencing the Secret in the VM's spec. Examples further down in the document illustrate how that is done. # Create a simple network-config cat << END > network-config network: version: 1 config: - type: physical name: eth0 subnets: - type: dhcp END # Create a VM manifest with the networkData base64 encoded into # a cloudInitNoCloud volume's networkDataBase64 field. cat << END > my-vmi.yaml apiVersion: kubevirt.io/v1alpha2 kind: VirtualMachineInstance metadata: name: myvmi spec: terminationGracePeriodSeconds: 5 domain: resources: requests: memory: 64M devices: disks: - name: containerdisk volumeName: registryvolume disk: bus: virtio - name: cloudinitdisk volumeName: cloudinitvolume disk: bus: virtio volumes: - name: registryvolume containerDisk: image: kubevirt/cirros-container-disk-demo:latest - name: cloudinitvolume cloudInitNoCloud: userData: \"#cloud-config\" networkDataBase64: $(cat network-config | base64 -w0) END # Post the Virtual Machine spec to KubeVirt. kubectl create -f my-vmi.yaml","title":"Cloud-init network-config as base64 string"},{"location":"virtual_machines/startup_scripts/#cloud-init-network-config-as-k8s-secret","text":"Users who wish to not store the cloud-init network-config directly in the VirtualMachineInstance spec have the option to store the network-config into a Kubernetes Secret and reference that Secret in the spec. Multiple VirtualMachineInstance specs can reference the same Kubernetes Secret containing cloud-init network-config. Below is an example of how to create a Kubernetes Secret containing a network-config and reference that Secret in the VM's spec. # Create a simple network-config cat << END > network-config network: version: 1 config: - type: physical name: eth0 subnets: - type: dhcp END # Store the network-config in a Kubernetes Secret kubectl create secret generic my-vmi-secret --from-file=networkdata=network-config # Create a VM manifest and reference the Secret's name in the cloudInitNoCloud # Volume's secretRef field cat << END > my-vmi.yaml apiVersion: kubevirt.io/v1alpha2 kind: VirtualMachineInstance metadata: name: myvmi spec: terminationGracePeriodSeconds: 5 domain: resources: requests: memory: 64M devices: disks: - name: containerdisk volumeName: registryvolume disk: bus: virtio - name: cloudinitdisk volumeName: cloudinitvolume disk: bus: virtio volumes: - name: registryvolume containerDisk: image: kubevirt/cirros-registry-disk-demo:latest - name: cloudinitvolume cloudInitNoCloud: userData: \"#cloud-config\" networkDataSecretRef: name: my-vmi-secret END # Post the VM kubectl create -f my-vmi.yaml","title":"Cloud-init network-config as k8s Secret"},{"location":"virtual_machines/startup_scripts/#debugging","text":"Depending on the operating system distribution in use, cloud-init output is often printed to the console output on boot up. When developing userdata scripts, users can connect to the VM's console during boot up to debug. Example of connecting to console using virtctl: virtctl console <name of vmi>","title":"Debugging"},{"location":"virtual_machines/startup_scripts/#device-role-tagging","text":"KubeVirt provides a mechanism for users to tag devices such as Network Interfaces with a specific role. The tag will be matched to the hardware address of the device and this mapping exposed to the guest OS via cloud-init. This additional metadata will help the guest OS users with multiple networks interfaces to identify the devices that may have a specific role, such as a network device dedicated to a specific service or a disk intended to be used by a specific application (database, webcache, etc.) This functionality already exists in platforms such as OpenStack. KubeVirt will provide the data in a similar format, known to users and services like cloud-init. For example: kind: VirtualMachineInstance spec: domain: devices: interfaces: - masquerade: {} name: default - bridge: {} name: ptp tag: ptp - name: sriov-net sriov: {} tag: nfvfunc networks: - name: default pod: {} - multus: networkName: ptp-conf name: ptp networkName: sriov/sriov-network name: sriov-net The metadata will be available in the guests config drive `openstack/latest/meta_data.json` { \"devices\": [ { \"type\": \"nic\", \"bus\": \"pci\", \"address\": \"0000:00:02.0\", \"mac\": \"01:22:22:42:22:21\", \"tags\": [\"ptp\"] }, { \"type\": \"nic\", \"bus\": \"pci\", \"address\": \"0000:81:10.1\", \"mac\": \"01:22:22:42:22:22\", \"tags\": [\"nfvfunc\"] }, ] }","title":"Device Role Tagging"},{"location":"virtual_machines/startup_scripts/#sysprep-examples","text":"","title":"Sysprep Examples"},{"location":"virtual_machines/startup_scripts/#sysprep-in-a-configmap","text":"The answer file can be provided in a ConfigMap: apiVersion: v1 kind: ConfigMap metadata: name: sysprep-config data: autounattend.xml: | <?xml version=\"1.0\" encoding=\"utf-8\"?> <unattend xmlns=\"urn:schemas-microsoft-com:unattend\"> ... </unattend> And attached to the VM like so: kind: VirtualMachine metadata: name: windows-with-sysprep spec: running: false template: metadata: labels: kubevirt.io/domain: windows-with-sysprep spec: domain: cpu: cores: 3 devices: disks: - bootOrder: 1 disk: bus: virtio name: harddrive - name: sysprep cdrom: bus: sata machine: type: q35 resources: requests: memory: 6G volumes: - name: harddrive persistentVolumeClaim: claimName: windows_pvc - name: sysprep sysprep: configMap: name: sysprep-config","title":"Sysprep in a ConfigMap"},{"location":"virtual_machines/startup_scripts/#sysprep-in-a-secret","text":"The answer file can be provided in a Secret: apiVersion: v1 kind: Secret metadata: name: sysprep-config stringData: data: autounattend.xml: | <?xml version=\"1.0\" encoding=\"utf-8\"?> <unattend xmlns=\"urn:schemas-microsoft-com:unattend\"> ... </unattend> And attached to the VM like so: kind: VirtualMachine metadata: name: windows-with-sysprep spec: running: false template: metadata: labels: kubevirt.io/domain: windows-with-sysprep spec: domain: cpu: cores: 3 devices: disks: - bootOrder: 1 disk: bus: virtio name: harddrive - name: sysprep cdrom: bus: sata machine: type: q35 resources: requests: memory: 6G volumes: - name: harddrive persistentVolumeClaim: claimName: windows_pvc - name: sysprep sysprep: secret: name: sysprep-secret","title":"Sysprep in a Secret"},{"location":"virtual_machines/startup_scripts/#base-sysprep-vm","text":"In the example below, a configMap with autounattend.xml file is used to modify the Windows iso image which is downloaded from Microsoft and creates a base installed Windows machine with virtio drivers installed and all the commands executed in post-install.ps1 For the below manifests to work it needs to have win10-iso DataVolume. apiVersion: v1 kind: ConfigMap metadata: name: win10-template-configmap data: autounattend.xml: |- <?xml version=\"1.0\" encoding=\"utf-8\"?> <unattend xmlns=\"urn:schemas-microsoft-com:unattend\"> <settings pass=\"windowsPE\"> <component xmlns:wcm=\"http://schemas.microsoft.com/WMIConfig/2002/State\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" name=\"Microsoft-Windows-International-Core-WinPE\" processorArchitecture=\"amd64\" publicKeyToken=\"31bf3856ad364e35\" language=\"neutral\" versionScope=\"nonSxS\"> <SetupUILanguage> <UILanguage>en-US</UILanguage> </SetupUILanguage> <InputLocale>0409:00000409</InputLocale> <SystemLocale>en-US</SystemLocale> <UILanguage>en-US</UILanguage> <UILanguageFallback>en-US</UILanguageFallback> <UserLocale>en-US</UserLocale> </component> <component xmlns:wcm=\"http://schemas.microsoft.com/WMIConfig/2002/State\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" name=\"Microsoft-Windows-PnpCustomizationsWinPE\" processorArchitecture=\"amd64\" publicKeyToken=\"31bf3856ad364e35\" language=\"neutral\" versionScope=\"nonSxS\"> <DriverPaths> <PathAndCredentials wcm:keyValue=\"4b29ba63\" wcm:action=\"add\"> <Path>E:\\amd64\\2k19</Path> </PathAndCredentials> <PathAndCredentials wcm:keyValue=\"25fe51ea\" wcm:action=\"add\"> <Path>E:\\NetKVM\\2k19\\amd64</Path> </PathAndCredentials> </DriverPaths> </component> <component xmlns:wcm=\"http://schemas.microsoft.com/WMIConfig/2002/State\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" name=\"Microsoft-Windows-Setup\" processorArchitecture=\"amd64\" publicKeyToken=\"31bf3856ad364e35\" language=\"neutral\" versionScope=\"nonSxS\"> <DiskConfiguration> <Disk wcm:action=\"add\"> <CreatePartitions> <CreatePartition wcm:action=\"add\"> <Order>1</Order> <Type>Primary</Type> <Size>100</Size> </CreatePartition> <CreatePartition wcm:action=\"add\"> <Extend>true</Extend> <Order>2</Order> <Type>Primary</Type> </CreatePartition> </CreatePartitions> <ModifyPartitions> <ModifyPartition wcm:action=\"add\"> <Format>NTFS</Format> <Label>System Reserved</Label> <Order>1</Order> <PartitionID>1</PartitionID> <TypeID>0x27</TypeID> </ModifyPartition> <ModifyPartition wcm:action=\"add\"> <Format>NTFS</Format> <Label>OS</Label> <Letter>C</Letter> <Order>2</Order> <PartitionID>2</PartitionID> </ModifyPartition> </ModifyPartitions> <DiskID>0</DiskID> <WillWipeDisk>true</WillWipeDisk> </Disk> </DiskConfiguration> <ImageInstall> <OSImage> <InstallFrom> <MetaData wcm:action=\"add\"> <Key>/Image/Description</Key> <Value>Windows 10 Pro</Value> </MetaData> </InstallFrom> <InstallTo> <DiskID>0</DiskID> <PartitionID>2</PartitionID> </InstallTo> </OSImage> </ImageInstall> <UserData> <AcceptEula>true</AcceptEula> <FullName/> <Organization/> <ProductKey> <Key/> </ProductKey> </UserData> </component> </settings> <settings pass=\"offlineServicing\"> <component xmlns:wcm=\"http://schemas.microsoft.com/WMIConfig/2002/State\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" name=\"Microsoft-Windows-LUA-Settings\" processorArchitecture=\"amd64\" publicKeyToken=\"31bf3856ad364e35\" language=\"neutral\" versionScope=\"nonSxS\"> <EnableLUA>false</EnableLUA> </component> </settings> <settings pass=\"specialize\"> <component xmlns:wcm=\"http://schemas.microsoft.com/WMIConfig/2002/State\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" name=\"Microsoft-Windows-International-Core\" processorArchitecture=\"amd64\" publicKeyToken=\"31bf3856ad364e35\" language=\"neutral\" versionScope=\"nonSxS\"> <InputLocale>0409:00000409</InputLocale> <SystemLocale>en-US</SystemLocale> <UILanguage>en-US</UILanguage> <UILanguageFallback>en-US</UILanguageFallback> <UserLocale>en-US</UserLocale> </component> <component xmlns:wcm=\"http://schemas.microsoft.com/WMIConfig/2002/State\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" name=\"Microsoft-Windows-Security-SPP-UX\" processorArchitecture=\"amd64\" publicKeyToken=\"31bf3856ad364e35\" language=\"neutral\" versionScope=\"nonSxS\"> <SkipAutoActivation>true</SkipAutoActivation> </component> <component xmlns:wcm=\"http://schemas.microsoft.com/WMIConfig/2002/State\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" name=\"Microsoft-Windows-SQMApi\" processorArchitecture=\"amd64\" publicKeyToken=\"31bf3856ad364e35\" language=\"neutral\" versionScope=\"nonSxS\"> <CEIPEnabled>0</CEIPEnabled> </component> </settings> <settings pass=\"oobeSystem\"> <component xmlns:wcm=\"http://schemas.microsoft.com/WMIConfig/2002/State\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" name=\"Microsoft-Windows-Shell-Setup\" processorArchitecture=\"amd64\" publicKeyToken=\"31bf3856ad364e35\" language=\"neutral\" versionScope=\"nonSxS\"> <OOBE> <HideEULAPage>true</HideEULAPage> <HideOEMRegistrationScreen>true</HideOEMRegistrationScreen> <HideOnlineAccountScreens>true</HideOnlineAccountScreens> <HideWirelessSetupInOOBE>true</HideWirelessSetupInOOBE> <NetworkLocation>Work</NetworkLocation> <SkipUserOOBE>true</SkipUserOOBE> <SkipMachineOOBE>true</SkipMachineOOBE> <ProtectYourPC>3</ProtectYourPC> </OOBE> <AutoLogon> <Password> <Value>123456</Value> <PlainText>true</PlainText> </Password> <Enabled>true</Enabled> <Username>Administrator</Username> </AutoLogon> <UserAccounts> <AdministratorPassword> <Value>123456</Value> <PlainText>true</PlainText> </AdministratorPassword> </UserAccounts> <RegisteredOrganization/> <RegisteredOwner/> <TimeZone>Eastern Standard Time</TimeZone> <FirstLogonCommands> <SynchronousCommand wcm:action=\"add\"> <CommandLine>powershell -ExecutionPolicy Bypass -NoExit -NoProfile f:\\post-install.ps1</CommandLine> <RequiresUserInput>false</RequiresUserInput> <Order>1</Order> <Description>Post Installation Script</Description> </SynchronousCommand> </FirstLogonCommands> </component> </settings> </unattend> post-install.ps1: |- # Remove AutoLogin # https://docs.microsoft.com/en-us/windows-hardware/customize/desktop/unattend/microsoft-windows-shell-setup-autologon-logoncount#logoncount-known-issue reg add \"HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\\Winlogon\" /v AutoAdminLogon /t REG_SZ /d 0 /f # install Qemu Tools (Drivers) Start-Process msiexec -Wait -ArgumentList '/i e:\\virtio-win-gt-x64.msi /qn /passive /norestart' # install Guest Agent Start-Process msiexec -Wait -ArgumentList '/i e:\\guest-agent\\qemu-ga-x86_64.msi /qn /passive /norestart' # Rename cached unattend.xml to avoid it is picked up by sysprep mv C:\\Windows\\Panther\\unattend.xml C:\\Windows\\Panther\\unattend.install.xml # Eject CD, to avoid that the autounattend.xml on the CD is picked up by sysprep (new-object -COM Shell.Application).NameSpace(17).ParseName('F:').InvokeVerb('Eject') # Run Sysprep and Shutdown C:\\Windows\\System32\\Sysprep\\sysprep.exe /generalize /oobe /shutdown /mode:vm --- apiVersion: kubevirt.io/v1 kind: VirtualMachine metadata: annotations: name.os.template.kubevirt.io/win10: Microsoft Windows 10 vm.kubevirt.io/validations: | [ { \"name\": \"minimal-required-memory\", \"path\": \"jsonpath::.spec.domain.resources.requests.memory\", \"rule\": \"integer\", \"message\": \"This VM requires more memory.\", \"min\": 2147483648 }, { \"name\": \"windows-virtio-bus\", \"path\": \"jsonpath::.spec.domain.devices.disks[*].disk.bus\", \"valid\": \"jsonpath::.spec.domain.devices.disks[*].disk.bus\", \"rule\": \"enum\", \"message\": \"virto disk bus type has better performance, install virtio drivers in VM and change bus type\", \"values\": [\"virtio\"], \"justWarning\": true }, { \"name\": \"windows-disk-bus\", \"path\": \"jsonpath::.spec.domain.devices.disks[*].disk.bus\", \"valid\": \"jsonpath::.spec.domain.devices.disks[*].disk.bus\", \"rule\": \"enum\", \"message\": \"disk bus has to be either virtio or sata or scsi\", \"values\": [\"virtio\", \"sata\", \"scsi\"] }, { \"name\": \"windows-cd-bus\", \"path\": \"jsonpath::.spec.domain.devices.disks[*].cdrom.bus\", \"valid\": \"jsonpath::.spec.domain.devices.disks[*].cdrom.bus\", \"rule\": \"enum\", \"message\": \"cd bus has to be sata\", \"values\": [\"sata\"] } ] name: win10-template namespace: default labels: app: win10-template flavor.template.kubevirt.io/medium: 'true' os.template.kubevirt.io/win10: 'true' vm.kubevirt.io/template: windows10-desktop-medium vm.kubevirt.io/template.namespace: openshift vm.kubevirt.io/template.revision: '1' vm.kubevirt.io/template.version: v0.14.0 workload.template.kubevirt.io/desktop: 'true' spec: runStrategy: RerunOnFailure dataVolumeTemplates: - metadata: name: win10-template-windows-iso spec: pvc: accessModes: - ReadWriteOnce resources: requests: storage: 20Gi source: pvc: name: windows10-iso namespace: default - metadata: name: win10-template spec: pvc: accessModes: - ReadWriteOnce resources: requests: storage: 25Gi volumeMode: Filesystem source: blank: {} template: metadata: annotations: vm.kubevirt.io/flavor: medium vm.kubevirt.io/os: windows10 vm.kubevirt.io/workload: desktop labels: flavor.template.kubevirt.io/medium: 'true' kubevirt.io/domain: win10-template kubevirt.io/size: medium os.template.kubevirt.io/win10: 'true' vm.kubevirt.io/name: win10-template workload.template.kubevirt.io/desktop: 'true' spec: domain: clock: timer: hpet: present: false hyperv: {} pit: tickPolicy: delay rtc: tickPolicy: catchup utc: {} cpu: cores: 1 sockets: 1 threads: 1 devices: disks: - bootOrder: 1 disk: bus: virtio name: win10-template - bootOrder: 2 cdrom: bus: sata name: windows-iso - cdrom: bus: sata name: windows-guest-tools - name: sysprep cdrom: bus: sata inputs: - bus: usb name: tablet type: tablet interfaces: - masquerade: {} model: virtio name: default features: acpi: {} apic: {} hyperv: reenlightenment: {} ipi: {} synic: {} synictimer: direct: {} spinlocks: spinlocks: 8191 reset: {} relaxed: {} vpindex: {} runtime: {} tlbflush: {} frequencies: {} vapic: {} machine: type: pc-q35-rhel8.4.0 resources: requests: memory: 4Gi hostname: win10-template networks: - name: default pod: {} volumes: - dataVolume: name: win10-iso name: windows-iso - dataVolume: name: win10-template name: win10-template - containerDisk: image: kubevirt/virtio-container-disk name: windows-guest-tools - name: sysprep sysprep: configMap: name: win10-template-configmap","title":"Base Sysprep VM"},{"location":"virtual_machines/startup_scripts/#launching-a-vm-from-template","text":"From the above example after the sysprep command is executed in the post-install.ps1 and the vm is in shutdown state, A new VM can be launched from the base win10-template with additional changes mentioned from the below unattend.xml in sysprep-config . The new VM can take upto 5 minutes to be in running state since Windows goes through oobe setup in the background with the customizations specified in the below unattend.xml file. apiVersion: v1 kind: ConfigMap metadata: name: sysprep-config data: autounattend.xml: |- <?xml version=\"1.0\" encoding=\"utf-8\"?> <!-- responsible for installing windows, ignored on sysprepped images --> unattend.xml: |- <?xml version=\"1.0\" encoding=\"utf-8\"?> <unattend xmlns=\"urn:schemas-microsoft-com:unattend\"> <settings pass=\"oobeSystem\"> <component xmlns:wcm=\"http://schemas.microsoft.com/WMIConfig/2002/State\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" name=\"Microsoft-Windows-Shell-Setup\" processorArchitecture=\"amd64\" publicKeyToken=\"31bf3856ad364e35\" language=\"neutral\" versionScope=\"nonSxS\"> <OOBE> <HideEULAPage>true</HideEULAPage> <HideOEMRegistrationScreen>true</HideOEMRegistrationScreen> <HideOnlineAccountScreens>true</HideOnlineAccountScreens> <HideWirelessSetupInOOBE>true</HideWirelessSetupInOOBE> <NetworkLocation>Work</NetworkLocation> <SkipUserOOBE>true</SkipUserOOBE> <SkipMachineOOBE>true</SkipMachineOOBE> <ProtectYourPC>3</ProtectYourPC> </OOBE> <AutoLogon> <Password> <Value>123456</Value> <PlainText>true</PlainText> </Password> <Enabled>true</Enabled> <Username>Administrator</Username> </AutoLogon> <UserAccounts> <AdministratorPassword> <Value>123456</Value> <PlainText>true</PlainText> </AdministratorPassword> </UserAccounts> <RegisteredOrganization>Kuebvirt</RegisteredOrganization> <RegisteredOwner>Kubevirt</RegisteredOwner> <TimeZone>Eastern Standard Time</TimeZone> <FirstLogonCommands> <SynchronousCommand wcm:action=\"add\"> <CommandLine>powershell -ExecutionPolicy Bypass -NoExit -WindowStyle Hidden -NoProfile d:\\customize.ps1</CommandLine> <RequiresUserInput>false</RequiresUserInput> <Order>1</Order> <Description>Customize Script</Description> </SynchronousCommand> </FirstLogonCommands> </component> </settings> </unattend> customize.ps1: |- # Enable RDP Set-ItemProperty -Path 'HKLM:\\System\\CurrentControlSet\\Control\\Terminal Server' -name \"fDenyTSConnections\" -value 0 Enable-NetFirewallRule -DisplayGroup \"Remote Desktop\" # https://docs.microsoft.com/en-us/windows-server/administration/openssh/openssh_install_firstuse # Install the OpenSSH Server Add-WindowsCapability -Online -Name OpenSSH.Server~~~~0.0.1.0 # Start the sshd service Start-Service sshd Set-Service -Name sshd -StartupType 'Automatic' # https://docs.microsoft.com/en-us/windows-server/administration/openssh/openssh_server_configuration # use powershell as default shell for ssh New-ItemProperty -Path \"HKLM:\\SOFTWARE\\OpenSSH\" -Name DefaultShell -Value \"C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\powershell.exe\" -PropertyType String -Force # Add ssh authorized_key for administrator # https://docs.microsoft.com/en-us/windows-server/administration/openssh/openssh_keymanagement $MyDir = $MyInvocation.MyCommand.Path | Split-Path -Parent $PublicKey = Get-Content -Path $MyDir\\id_rsa.pub $authrized_keys_path = $env:ProgramData + \"\\ssh\\administrators_authorized_keys\" Add-Content -Path $authrized_keys_path -Value $PublicKey icacls.exe $authrized_keys_path /inheritance:r /grant \"Administrators:F\" /grant \"SYSTEM:F\" # install application via exe file installer from url function Install-Exe { $dlurl = $args[0] $installerPath = Join-Path $env:TEMP (Split-Path $dlurl -Leaf) Invoke-WebRequest -UseBasicParsing $dlurl -OutFile $installerPath Start-Process -FilePath $installerPath -Args \"/S\" -Verb RunAs -Wait Remove-Item $installerPath } # Wait for networking before running a task at startup do { $ping = test-connection -comp kubevirt.io -count 1 -Quiet } until ($ping) # Installing the Latest Notepad++ with PowerShell $BaseUri = \"https://notepad-plus-plus.org\" $BasePage = Invoke-WebRequest -Uri $BaseUri -UseBasicParsing $ChildPath = $BasePage.Links | Where-Object { $_.outerHTML -like '*Current Version*' } | Select-Object -ExpandProperty href $DownloadPageUri = $BaseUri + $ChildPath $DownloadPage = Invoke-WebRequest -Uri $DownloadPageUri -UseBasicParsing $DownloadUrl = $DownloadPage.Links | Where-Object { $_.outerHTML -like '*npp.*.Installer.x64.exe\"*' } | Select-Object -ExpandProperty href Install-Exe $DownloadUrl id_rsa.pub: |- ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC6zdgFiLr1uAK7PdcchDd+LseA5fEOcxCCt7TLlr7Mx6h8jUg+G+8L9JBNZuDzTZSF0dR7qwzdBBQjorAnZTmY3BhsKcFr8Gt4KMGrS6r3DNmGruP8GORvegdWZuXgASKVpXeI7nCIjRJwAaK1x+eGHwAWO9Z8ohcboHbLyffOoSZDSIuk2kRIc47+ENRjg0T6x2VRsqX27g6j4DfPKQZGk0zvXkZaYtr1e2tZgqTBWqZUloMJK8miQq6MktCKAS4VtPk0k7teQX57OGwD6D7uo4b+Cl8aYAAwhn0hc0C2USfbuVHgq88ESo2/+NwV4SQcl3sxCW21yGIjAGt4Hy7J fedora@localhost.localdomain","title":"Launching a VM from template"},{"location":"virtual_machines/templates/","text":"Templates \u00b6 Note By deploying KubeVirt on top of OpenShift the user can benefit from the OpenShift Template functionality. Virtual machine templates \u00b6 What is a virtual machine template? \u00b6 The KubeVirt projects provides a set of templates to create VMs to handle common usage scenarios. These templates provide a combination of some key factors that could be further customized and processed to have a Virtual Machine object. The key factors which define a template are Workload Most Virtual Machine should be server or desktop to have maximum flexibility; the highperformance workload trades some of this flexibility to provide better performances. Guest Operating System (OS) This allow to ensure that the emulated hardware is compatible with the guest OS. Furthermore, it allows to maximize the stability of the VM, and allows performance optimizations. Size (flavor) Defines the amount of resources (CPU, memory) to allocate to the VM. More documentation is available in the common templates subproject Accessing the virtual machine templates \u00b6 If you installed KubeVirt using a supported method you should find the common templates preinstalled in the cluster. Should you want to upgrade the templates, or install them from scratch, you can use one of the supported releases To install the templates: $ export VERSION=$(curl -s https://api.github.com/repos/kubevirt/common-templates/releases | grep tag_name | grep -v -- '-rc' | head -1 | awk -F': ' '{print $2}' | sed 's/,//' | xargs) $ oc create -f https://github.com/kubevirt/common-templates/releases/download/$VERSION/common-templates-$VERSION.yaml Editable fields \u00b6 You can edit the fields of the templates which define the amount of resources which the VMs will receive. Each template can list a different set of fields that are to be considered editable. The fields are used as hints for the user interface, and also for other components in the cluster. The editable fields are taken from annotations in the template. Here is a snippet presenting a couple of most commonly found editable fields: metadata: annotations: template.kubevirt.io/editable: | /objects[0].spec.template.spec.domain.cpu.sockets /objects[0].spec.template.spec.domain.cpu.cores /objects[0].spec.template.spec.domain.cpu.threads /objects[0].spec.template.spec.domain.resources.requests.memory Each entry in the editable field list must be a jsonpath . The jsonpath root is the objects: element of the template. The actually editable field is the last entry (the \"leaf\") of the path. For example, the following minimal snippet highlights the fields which you can edit: objects: spec: template: spec: domain: cpu: sockets: VALUE # this is editable cores: VALUE # this is editable threads: VALUE # this is editable resources: requests: memory: VALUE # this is editable Relationship between templates and VMs \u00b6 Once processed the templates produce VM objects to be used in the cluster. The VMs produced from templates will have a vm.kubevirt.io/template label, whose value will be the name of the parent template, for example fedora-desktop-medium : metadata: labels: vm.kubevirt.io/template: fedora-desktop-medium In addition, these VMs can include an optional label vm.kubevirt.io/template-namespace , whose value will be the namespace of the parent template, for example: metadata: labels: vm.kubevirt.io/template-namespace: openshift If this label is not defined, the template is expected to belong to the same namespace as the VM. This make it possible to query for all the VMs built from any template. Example: oc process -o yaml -f dist/templates/rhel8-server-tiny.yaml NAME=rheltinyvm SRC_PVC_NAME=rhel SRC_PVC_NAMESPACE=kubevirt And the output: apiVersion: v1 items: - apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachine metadata: annotations: vm.kubevirt.io/flavor: tiny vm.kubevirt.io/os: rhel8 vm.kubevirt.io/validations: | [ { \"name\": \"minimal-required-memory\", \"path\": \"jsonpath::.spec.domain.resources.requests.memory\", \"rule\": \"integer\", \"message\": \"This VM requires more memory.\", \"min\": 1610612736 } ] vm.kubevirt.io/workload: server labels: app: rheltinyvm vm.kubevirt.io/template: rhel8-server-tiny vm.kubevirt.io/template.revision: \"45\" vm.kubevirt.io/template.version: 0.11.3 name: rheltinyvm spec: dataVolumeTemplates: - apiVersion: cdi.kubevirt.io/v1beta1 kind: DataVolume metadata: name: rheltinyvm spec: pvc: accessModes: - ReadWriteMany resources: requests: storage: 30Gi source: pvc: name: rhel namespace: kubevirt running: false template: metadata: labels: kubevirt.io/domain: rheltinyvm kubevirt.io/size: tiny spec: domain: cpu: cores: 1 sockets: 1 threads: 1 devices: disks: - disk: bus: virtio name: rheltinyvm - disk: bus: virtio name: cloudinitdisk interfaces: - masquerade: {} name: default networkInterfaceMultiqueue: true rng: {} resources: requests: memory: 1.5Gi networks: - name: default pod: {} terminationGracePeriodSeconds: 180 volumes: - dataVolume: name: rheltinyvm name: rheltinyvm - cloudInitNoCloud: userData: |- #cloud-config user: cloud-user password: lymp-fda4-m1cv chpasswd: { expire: False } name: cloudinitdisk kind: List metadata: {} You can add the VM from the template to the cluster in one go oc process rhel8-server-tiny NAME=rheltinyvm SRC_PVC_NAME=rhel SRC_PVC_NAMESPACE=kubevirt | oc apply -f - Please note that after the generation step VM and template objects have no relationship with each other besides the aforementioned label. Changes in templates do not automatically affect VMs or vice versa. common template customization \u00b6 The templates provided by the kubevirt project provide a set of conventions and annotations that augment the basic feature of the openshift templates . You can customize your kubevirt-provided templates editing these annotations, or you can add them to your existing templates to make them consumable by the kubevirt services. Here's a description of the kubevirt annotations. Unless otherwise specified, the following keys are meant to be top-level entries of the template metadata, like apiVersion: v1 kind: Template metadata: name: windows-10 annotations: openshift.io/display-name: \"Generic demo template\" All the following annotations are prefixed with defaults.template.kubevirt.io , which is omitted below for brevity. So the actual annotations you should use will look like apiVersion: v1 kind: Template metadata: name: windows-10 annotations: defaults.template.kubevirt.io/disk: default-disk defaults.template.kubevirt.io/volume: default-volume defaults.template.kubevirt.io/nic: default-nic defaults.template.kubevirt.io/network: default-network Unless otherwise specified, all annotations are meant to be safe defaults, both for performance and compatibility, and hints for the CNV-aware UI and tooling. disk \u00b6 See the section references below. Example: apiVersion: v1 kind: Template metadata: name: Linux annotations: defaults.template.kubevirt.io/disk: rhel-disk nic \u00b6 See the section references below. Example: apiVersion: v1 kind: Template metadata: name: Windows annotations: defaults.template.kubevirt.io/nic: my-nic volume \u00b6 See the section references below. Example: apiVersion: v1 kind: Template metadata: name: Linux annotations: defaults.template.kubevirt.io/volume: custom-volume network \u00b6 See the section references below. Example: apiVersion: v1 kind: Template metadata: name: Linux annotations: defaults.template.kubevirt.io/network: fast-net references \u00b6 The default values for network, nic, volume, disk are meant to be the name of a section later in the document that the UI will find and consume to find the default values for the corresponding types. For example, considering the annotation defaults.template.kubevirt.io/disk: my-disk : we assume that later in the document it exists an element called my-disk that the UI can use to find the data it needs. The names actually don't matter as long as they are legal for kubernetes and consistent with the content of the document. complete example \u00b6 demo-template.yaml apiversion: v1 items: - apiversion: kubevirt.io/v1alpha3 kind: virtualmachine metadata: labels: vm.kubevirt.io/template: rhel7-generic-tiny name: rheltinyvm osinfoname: rhel7.0 defaults.template.kubevirt.io/disk: rhel-default-disk defaults.template.kubevirt.io/nic: rhel-default-net spec: running: false template: spec: domain: cpu: sockets: 1 cores: 1 threads: 1 devices: rng: {} resources: requests: memory: 1g terminationgraceperiodseconds: 0 volumes: - containerDisk: image: registry:5000/kubevirt/cirros-container-disk-demo:devel name: rhel-default-disk networks: - genie: networkName: flannel name: rhel-default-net kind: list metadata: {} once processed becomes: demo-vm.yaml apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachine metadata: labels: vm.kubevirt.io/template: rhel7-generic-tiny name: rheltinyvm osinfoname: rhel7.0 spec: running: false template: spec: domain: cpu: sockets: 1 cores: 1 threads: 1 resources: requests: memory: 1g devices: rng: {} disks: - disk: name: rhel-default-disk interfaces: - bridge: {} name: rhel-default-nic terminationgraceperiodseconds: 0 volumes: - containerDisk: image: registry:5000/kubevirt/cirros-container-disk-demo:devel name: containerdisk networks: - genie: networkName: flannel name: rhel-default-nic Virtual machine creation \u00b6 Overview \u00b6 The KubeVirt projects provides a set of templates to create VMs to handle common usage scenarios. These templates provide a combination of some key factors that could be further customized and processed to have a Virtual Machine object. The key factors which define a template are - Workload Most Virtual Machine should be server or desktop to have maximum flexibility; the highperformance workload trades some of this flexibility to provide better performances. - Guest Operating System (OS) This allow to ensure that the emulated hardware is compatible with the guest OS. Furthermore, it allows to maximize the stability of the VM, and allows performance optimizations. - Size (flavor) Defines the amount of resources (CPU, memory) to allocate to the VM. Openshift Console \u00b6 VMs can be created through OpenShift Cluster Console UI . This UI supports creation VM using templates and templates features - flavors and workload profiles. To create VM from template, choose WorkLoads in the left panel >> choose Virtualization >> press to the \"Create Virtual Machine\" blue button >> choose \"Create from wizard\". Next, you have to see \"Create Virtual Machine\" window Common-templates \u00b6 There is the common-templates subproject. It provides official prepared and useful templates. You can also create templates by hand. You can find an example below, in the \"Example template\" section. Example template \u00b6 In order to create a virtual machine via OpenShift CLI, you need to provide a template defining the corresponding object and its metadata. NOTE Only VirtualMachine object is currently supported. Here is an example template that defines an instance of the VirtualMachine object: apiVersion: template.openshift.io/v1 kind: Template metadata: name: fedora-desktop-large annotations: openshift.io/display-name: \"Fedora 32+ VM\" description: >- Template for Fedora 32 VM or newer. A PVC with the Fedora disk image must be available. Recommended disk image: https://download.fedoraproject.org/pub/fedora/linux/releases/32/Cloud/x86_64/images/Fedora-Cloud-Base-32-1.6.x86_64.qcow2 tags: \"hidden,kubevirt,virtualmachine,fedora\" iconClass: \"icon-fedora\" openshift.io/provider-display-name: \"KubeVirt\" openshift.io/documentation-url: \"https://github.com/kubevirt/common-templates\" openshift.io/support-url: \"https://github.com/kubevirt/common-templates/issues\" template.openshift.io/bindable: \"false\" template.kubevirt.io/version: v1alpha1 defaults.template.kubevirt.io/disk: rootdisk template.kubevirt.io/editable: | /objects[0].spec.template.spec.domain.cpu.sockets /objects[0].spec.template.spec.domain.cpu.cores /objects[0].spec.template.spec.domain.cpu.threads /objects[0].spec.template.spec.domain.resources.requests.memory /objects[0].spec.template.spec.domain.devices.disks /objects[0].spec.template.spec.volumes /objects[0].spec.template.spec.networks name.os.template.kubevirt.io/fedora32: Fedora 32 or higher name.os.template.kubevirt.io/fedora33: Fedora 32 or higher name.os.template.kubevirt.io/silverblue32: Fedora 32 or higher name.os.template.kubevirt.io/silverblue33: Fedora 32 or higher labels: os.template.kubevirt.io/fedora32: \"true\" os.template.kubevirt.io/fedora33: \"true\" os.template.kubevirt.io/silverblue32: \"true\" os.template.kubevirt.io/silverblue33: \"true\" workload.template.kubevirt.io/desktop: \"true\" flavor.template.kubevirt.io/large: \"true\" template.kubevirt.io/type: \"base\" template.kubevirt.io/version: \"0.11.3\" objects: - apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachine metadata: name: ${NAME} labels: vm.kubevirt.io/template: fedora-desktop-large vm.kubevirt.io/template.version: \"0.11.3\" vm.kubevirt.io/template.revision: \"45\" app: ${NAME} annotations: vm.kubevirt.io/os: \"fedora\" vm.kubevirt.io/workload: \"desktop\" vm.kubevirt.io/flavor: \"large\" vm.kubevirt.io/validations: | [ { \"name\": \"minimal-required-memory\", \"path\": \"jsonpath::.spec.domain.resources.requests.memory\", \"rule\": \"integer\", \"message\": \"This VM requires more memory.\", \"min\": 1073741824 } ] spec: dataVolumeTemplates: - apiVersion: cdi.kubevirt.io/v1beta1 kind: DataVolume metadata: name: ${NAME} spec: pvc: accessModes: - ReadWriteMany resources: requests: storage: 30Gi source: pvc: name: ${SRC_PVC_NAME} namespace: ${SRC_PVC_NAMESPACE} running: false template: metadata: labels: kubevirt.io/domain: ${NAME} kubevirt.io/size: large spec: domain: cpu: sockets: 2 cores: 1 threads: 1 resources: requests: memory: 8Gi devices: rng: {} networkInterfaceMultiqueue: true inputs: - type: tablet bus: virtio name: tablet disks: - disk: bus: virtio name: ${NAME} - disk: bus: virtio name: cloudinitdisk interfaces: - masquerade: {} name: default terminationGracePeriodSeconds: 180 networks: - name: default pod: {} volumes: - dataVolume: name: ${NAME} name: ${NAME} - cloudInitNoCloud: userData: |- #cloud-config user: fedora password: ${CLOUD_USER_PASSWORD} chpasswd: { expire: False } name: cloudinitdisk parameters: - description: VM name from: 'fedora-[a-z0-9]{16}' generate: expression name: NAME - name: SRC_PVC_NAME description: Name of the PVC to clone value: 'fedora' - name: SRC_PVC_NAMESPACE description: Namespace of the source PVC value: kubevirt-os-images - description: Randomized password for the cloud-init user fedora from: '[a-z0-9]{4}-[a-z0-9]{4}-[a-z0-9]{4}' generate: expression name: CLOUD_USER_PASSWORD Note that the template above defines free parameters ( NAME , SRC_PVC_NAME , SRC_PVC_NAMESPACE , CLOUD_USER_PASSWORD ) and the NAME parameter does not have specified default value. An OpenShift template has to be converted into the JSON file via oc process command, that also allows you to set the template parameters. A complete example can be found in the KubeVirt repository . !> You need to be logged in by oc login command. $ oc process -f cluster/vmi-template-fedora.yaml\\ -p NAME=testvmi \\ -p SRC_PVC_NAME=fedora \\ -p SRC_PVC_NAMESPACE=kubevirt \\ { \"kind\": \"List\", \"apiVersion\": \"v1\", \"metadata\": {}, \"items\": [ { The JSON file is usually applied directly by piping the processed output to oc create command. $ oc process -f cluster/examples/vm-template-fedora.yaml \\ -p NAME=testvm \\ -p SRC_PVC_NAME=fedora \\ -p SRC_PVC_NAMESPACE=kubevirt \\ | oc create -f - virtualmachine.kubevirt.io/testvm created The command above results in creating a Kubernetes object according to the specification given by the template \\(in this example it is an instance of the VirtualMachine object\\). It's possible to get list of available parameters using the following command: $ oc process -f dist/templates/fedora-desktop-large.yaml --parameters NAME DESCRIPTION GENERATOR VALUE NAME VM name expression fedora-[a-z0-9]{16} SRC_PVC_NAME Name of the PVC to clone fedora SRC_PVC_NAMESPACE Namespace of the source PVC kubevirt-os-images CLOUD_USER_PASSWORD Randomized password for the cloud-init user fedora expression [a-z0-9]{4}-[a-z0-9]{4}-[a-z0-9]{4} Starting virtual machine from the created object \u00b6 The created object is now a regular VirtualMachine object and from now it can be controlled by accessing Kubernetes API resources. The preferred way how to do this from within the OpenShift environment is to use oc patch command. $ oc patch virtualmachine testvm --type merge -p '{\"spec\":{\"running\":true}}' virtualmachine.kubevirt.io/testvm patched Do not forget about virtctl tool. Using it in the real cases instead of using kubernetes API can be more convenient. Example: $ virtctl start testvm VM testvm was scheduled to start As soon as VM starts, Kubernetes creates new type of object - VirtualMachineInstance. It has similar name to VirtualMachine. Example (not full output, it's too big): $ kubectl describe vm testvm name: testvm Namespace: myproject Labels: kubevirt-vm=vm-testvm kubevirt.io/os=fedora33 Annotations: <none> API Version: kubevirt.io/v1alpha3 Kind: VirtualMachine Cloud-init script and parameters \u00b6 Kubevirt VM templates, just like kubevirt VM/VMI yaml configs, supports cloud-init scripts Hack - use pre-downloaded image \u00b6 Kubevirt VM templates, just like kubevirt VM/VMI yaml configs, can use pre-downloaded VM image, which can be a useful feature especially in the debug/development/testing cases. No special parameters required in the VM template or VM/VMI yaml config. The main idea is to create Kubernetes PersistentVolume and PersistentVolumeClaim corresponding to existing image in the file system. Example: --- kind: PersistentVolume apiVersion: v1 metadata: name: mypv labels: type: local spec: storageClassName: manual capacity: storage: 10G accessModes: - ReadWriteOnce hostPath: path: \"/mnt/sda1/images/testvm\" --- kind: PersistentVolumeClaim apiVersion: v1 metadata: name: mypvc spec: storageClassName: manual accessModes: - ReadWriteOnce resources: requests: storage: 10G Using DataVolumes \u00b6 Kubevirt VM templates are using dataVolumeTemplates. Before using dataVolumes, CDI has to be installed in cluster. After that, source Datavolume can be created. --- apiVersion: cdi.kubevirt.io/v1beta1 kind: DataVolume metadata: name: fedora-datavolume-original namespace: kubevirt spec: source: registry: url: \"image_url\" pvc: accessModes: - ReadWriteOnce resources: requests: storage: 30Gi After import is completed, VM can be created: $ oc process -f cluster/examples/vm-template-fedora.yaml \\ -p NAME=testvmi \\ -p SRC_PVC_NAME=fedora-datavolume-original \\ -p SRC_PVC_NAMESPACE=kubevirt \\ | oc create -f - virtualmachine.kubevirt.io/testvm created Additional information \u00b6 You can follow Virtual Machine Lifecycle Guide for further reference.","title":"Templates"},{"location":"virtual_machines/templates/#templates","text":"Note By deploying KubeVirt on top of OpenShift the user can benefit from the OpenShift Template functionality.","title":"Templates"},{"location":"virtual_machines/templates/#virtual-machine-templates","text":"","title":"Virtual machine templates"},{"location":"virtual_machines/templates/#what-is-a-virtual-machine-template","text":"The KubeVirt projects provides a set of templates to create VMs to handle common usage scenarios. These templates provide a combination of some key factors that could be further customized and processed to have a Virtual Machine object. The key factors which define a template are Workload Most Virtual Machine should be server or desktop to have maximum flexibility; the highperformance workload trades some of this flexibility to provide better performances. Guest Operating System (OS) This allow to ensure that the emulated hardware is compatible with the guest OS. Furthermore, it allows to maximize the stability of the VM, and allows performance optimizations. Size (flavor) Defines the amount of resources (CPU, memory) to allocate to the VM. More documentation is available in the common templates subproject","title":"What is a virtual machine template?"},{"location":"virtual_machines/templates/#accessing-the-virtual-machine-templates","text":"If you installed KubeVirt using a supported method you should find the common templates preinstalled in the cluster. Should you want to upgrade the templates, or install them from scratch, you can use one of the supported releases To install the templates: $ export VERSION=$(curl -s https://api.github.com/repos/kubevirt/common-templates/releases | grep tag_name | grep -v -- '-rc' | head -1 | awk -F': ' '{print $2}' | sed 's/,//' | xargs) $ oc create -f https://github.com/kubevirt/common-templates/releases/download/$VERSION/common-templates-$VERSION.yaml","title":"Accessing the virtual machine templates"},{"location":"virtual_machines/templates/#editable-fields","text":"You can edit the fields of the templates which define the amount of resources which the VMs will receive. Each template can list a different set of fields that are to be considered editable. The fields are used as hints for the user interface, and also for other components in the cluster. The editable fields are taken from annotations in the template. Here is a snippet presenting a couple of most commonly found editable fields: metadata: annotations: template.kubevirt.io/editable: | /objects[0].spec.template.spec.domain.cpu.sockets /objects[0].spec.template.spec.domain.cpu.cores /objects[0].spec.template.spec.domain.cpu.threads /objects[0].spec.template.spec.domain.resources.requests.memory Each entry in the editable field list must be a jsonpath . The jsonpath root is the objects: element of the template. The actually editable field is the last entry (the \"leaf\") of the path. For example, the following minimal snippet highlights the fields which you can edit: objects: spec: template: spec: domain: cpu: sockets: VALUE # this is editable cores: VALUE # this is editable threads: VALUE # this is editable resources: requests: memory: VALUE # this is editable","title":"Editable fields"},{"location":"virtual_machines/templates/#relationship-between-templates-and-vms","text":"Once processed the templates produce VM objects to be used in the cluster. The VMs produced from templates will have a vm.kubevirt.io/template label, whose value will be the name of the parent template, for example fedora-desktop-medium : metadata: labels: vm.kubevirt.io/template: fedora-desktop-medium In addition, these VMs can include an optional label vm.kubevirt.io/template-namespace , whose value will be the namespace of the parent template, for example: metadata: labels: vm.kubevirt.io/template-namespace: openshift If this label is not defined, the template is expected to belong to the same namespace as the VM. This make it possible to query for all the VMs built from any template. Example: oc process -o yaml -f dist/templates/rhel8-server-tiny.yaml NAME=rheltinyvm SRC_PVC_NAME=rhel SRC_PVC_NAMESPACE=kubevirt And the output: apiVersion: v1 items: - apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachine metadata: annotations: vm.kubevirt.io/flavor: tiny vm.kubevirt.io/os: rhel8 vm.kubevirt.io/validations: | [ { \"name\": \"minimal-required-memory\", \"path\": \"jsonpath::.spec.domain.resources.requests.memory\", \"rule\": \"integer\", \"message\": \"This VM requires more memory.\", \"min\": 1610612736 } ] vm.kubevirt.io/workload: server labels: app: rheltinyvm vm.kubevirt.io/template: rhel8-server-tiny vm.kubevirt.io/template.revision: \"45\" vm.kubevirt.io/template.version: 0.11.3 name: rheltinyvm spec: dataVolumeTemplates: - apiVersion: cdi.kubevirt.io/v1beta1 kind: DataVolume metadata: name: rheltinyvm spec: pvc: accessModes: - ReadWriteMany resources: requests: storage: 30Gi source: pvc: name: rhel namespace: kubevirt running: false template: metadata: labels: kubevirt.io/domain: rheltinyvm kubevirt.io/size: tiny spec: domain: cpu: cores: 1 sockets: 1 threads: 1 devices: disks: - disk: bus: virtio name: rheltinyvm - disk: bus: virtio name: cloudinitdisk interfaces: - masquerade: {} name: default networkInterfaceMultiqueue: true rng: {} resources: requests: memory: 1.5Gi networks: - name: default pod: {} terminationGracePeriodSeconds: 180 volumes: - dataVolume: name: rheltinyvm name: rheltinyvm - cloudInitNoCloud: userData: |- #cloud-config user: cloud-user password: lymp-fda4-m1cv chpasswd: { expire: False } name: cloudinitdisk kind: List metadata: {} You can add the VM from the template to the cluster in one go oc process rhel8-server-tiny NAME=rheltinyvm SRC_PVC_NAME=rhel SRC_PVC_NAMESPACE=kubevirt | oc apply -f - Please note that after the generation step VM and template objects have no relationship with each other besides the aforementioned label. Changes in templates do not automatically affect VMs or vice versa.","title":"Relationship between templates and VMs"},{"location":"virtual_machines/templates/#common-template-customization","text":"The templates provided by the kubevirt project provide a set of conventions and annotations that augment the basic feature of the openshift templates . You can customize your kubevirt-provided templates editing these annotations, or you can add them to your existing templates to make them consumable by the kubevirt services. Here's a description of the kubevirt annotations. Unless otherwise specified, the following keys are meant to be top-level entries of the template metadata, like apiVersion: v1 kind: Template metadata: name: windows-10 annotations: openshift.io/display-name: \"Generic demo template\" All the following annotations are prefixed with defaults.template.kubevirt.io , which is omitted below for brevity. So the actual annotations you should use will look like apiVersion: v1 kind: Template metadata: name: windows-10 annotations: defaults.template.kubevirt.io/disk: default-disk defaults.template.kubevirt.io/volume: default-volume defaults.template.kubevirt.io/nic: default-nic defaults.template.kubevirt.io/network: default-network Unless otherwise specified, all annotations are meant to be safe defaults, both for performance and compatibility, and hints for the CNV-aware UI and tooling.","title":"common template customization"},{"location":"virtual_machines/templates/#disk","text":"See the section references below. Example: apiVersion: v1 kind: Template metadata: name: Linux annotations: defaults.template.kubevirt.io/disk: rhel-disk","title":"disk"},{"location":"virtual_machines/templates/#nic","text":"See the section references below. Example: apiVersion: v1 kind: Template metadata: name: Windows annotations: defaults.template.kubevirt.io/nic: my-nic","title":"nic"},{"location":"virtual_machines/templates/#volume","text":"See the section references below. Example: apiVersion: v1 kind: Template metadata: name: Linux annotations: defaults.template.kubevirt.io/volume: custom-volume","title":"volume"},{"location":"virtual_machines/templates/#network","text":"See the section references below. Example: apiVersion: v1 kind: Template metadata: name: Linux annotations: defaults.template.kubevirt.io/network: fast-net","title":"network"},{"location":"virtual_machines/templates/#references","text":"The default values for network, nic, volume, disk are meant to be the name of a section later in the document that the UI will find and consume to find the default values for the corresponding types. For example, considering the annotation defaults.template.kubevirt.io/disk: my-disk : we assume that later in the document it exists an element called my-disk that the UI can use to find the data it needs. The names actually don't matter as long as they are legal for kubernetes and consistent with the content of the document.","title":"references"},{"location":"virtual_machines/templates/#complete-example","text":"demo-template.yaml apiversion: v1 items: - apiversion: kubevirt.io/v1alpha3 kind: virtualmachine metadata: labels: vm.kubevirt.io/template: rhel7-generic-tiny name: rheltinyvm osinfoname: rhel7.0 defaults.template.kubevirt.io/disk: rhel-default-disk defaults.template.kubevirt.io/nic: rhel-default-net spec: running: false template: spec: domain: cpu: sockets: 1 cores: 1 threads: 1 devices: rng: {} resources: requests: memory: 1g terminationgraceperiodseconds: 0 volumes: - containerDisk: image: registry:5000/kubevirt/cirros-container-disk-demo:devel name: rhel-default-disk networks: - genie: networkName: flannel name: rhel-default-net kind: list metadata: {} once processed becomes: demo-vm.yaml apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachine metadata: labels: vm.kubevirt.io/template: rhel7-generic-tiny name: rheltinyvm osinfoname: rhel7.0 spec: running: false template: spec: domain: cpu: sockets: 1 cores: 1 threads: 1 resources: requests: memory: 1g devices: rng: {} disks: - disk: name: rhel-default-disk interfaces: - bridge: {} name: rhel-default-nic terminationgraceperiodseconds: 0 volumes: - containerDisk: image: registry:5000/kubevirt/cirros-container-disk-demo:devel name: containerdisk networks: - genie: networkName: flannel name: rhel-default-nic","title":"complete example"},{"location":"virtual_machines/templates/#virtual-machine-creation","text":"","title":"Virtual machine creation"},{"location":"virtual_machines/templates/#overview","text":"The KubeVirt projects provides a set of templates to create VMs to handle common usage scenarios. These templates provide a combination of some key factors that could be further customized and processed to have a Virtual Machine object. The key factors which define a template are - Workload Most Virtual Machine should be server or desktop to have maximum flexibility; the highperformance workload trades some of this flexibility to provide better performances. - Guest Operating System (OS) This allow to ensure that the emulated hardware is compatible with the guest OS. Furthermore, it allows to maximize the stability of the VM, and allows performance optimizations. - Size (flavor) Defines the amount of resources (CPU, memory) to allocate to the VM.","title":"Overview"},{"location":"virtual_machines/templates/#openshift-console","text":"VMs can be created through OpenShift Cluster Console UI . This UI supports creation VM using templates and templates features - flavors and workload profiles. To create VM from template, choose WorkLoads in the left panel >> choose Virtualization >> press to the \"Create Virtual Machine\" blue button >> choose \"Create from wizard\". Next, you have to see \"Create Virtual Machine\" window","title":"Openshift Console"},{"location":"virtual_machines/templates/#common-templates","text":"There is the common-templates subproject. It provides official prepared and useful templates. You can also create templates by hand. You can find an example below, in the \"Example template\" section.","title":"Common-templates"},{"location":"virtual_machines/templates/#example-template","text":"In order to create a virtual machine via OpenShift CLI, you need to provide a template defining the corresponding object and its metadata. NOTE Only VirtualMachine object is currently supported. Here is an example template that defines an instance of the VirtualMachine object: apiVersion: template.openshift.io/v1 kind: Template metadata: name: fedora-desktop-large annotations: openshift.io/display-name: \"Fedora 32+ VM\" description: >- Template for Fedora 32 VM or newer. A PVC with the Fedora disk image must be available. Recommended disk image: https://download.fedoraproject.org/pub/fedora/linux/releases/32/Cloud/x86_64/images/Fedora-Cloud-Base-32-1.6.x86_64.qcow2 tags: \"hidden,kubevirt,virtualmachine,fedora\" iconClass: \"icon-fedora\" openshift.io/provider-display-name: \"KubeVirt\" openshift.io/documentation-url: \"https://github.com/kubevirt/common-templates\" openshift.io/support-url: \"https://github.com/kubevirt/common-templates/issues\" template.openshift.io/bindable: \"false\" template.kubevirt.io/version: v1alpha1 defaults.template.kubevirt.io/disk: rootdisk template.kubevirt.io/editable: | /objects[0].spec.template.spec.domain.cpu.sockets /objects[0].spec.template.spec.domain.cpu.cores /objects[0].spec.template.spec.domain.cpu.threads /objects[0].spec.template.spec.domain.resources.requests.memory /objects[0].spec.template.spec.domain.devices.disks /objects[0].spec.template.spec.volumes /objects[0].spec.template.spec.networks name.os.template.kubevirt.io/fedora32: Fedora 32 or higher name.os.template.kubevirt.io/fedora33: Fedora 32 or higher name.os.template.kubevirt.io/silverblue32: Fedora 32 or higher name.os.template.kubevirt.io/silverblue33: Fedora 32 or higher labels: os.template.kubevirt.io/fedora32: \"true\" os.template.kubevirt.io/fedora33: \"true\" os.template.kubevirt.io/silverblue32: \"true\" os.template.kubevirt.io/silverblue33: \"true\" workload.template.kubevirt.io/desktop: \"true\" flavor.template.kubevirt.io/large: \"true\" template.kubevirt.io/type: \"base\" template.kubevirt.io/version: \"0.11.3\" objects: - apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachine metadata: name: ${NAME} labels: vm.kubevirt.io/template: fedora-desktop-large vm.kubevirt.io/template.version: \"0.11.3\" vm.kubevirt.io/template.revision: \"45\" app: ${NAME} annotations: vm.kubevirt.io/os: \"fedora\" vm.kubevirt.io/workload: \"desktop\" vm.kubevirt.io/flavor: \"large\" vm.kubevirt.io/validations: | [ { \"name\": \"minimal-required-memory\", \"path\": \"jsonpath::.spec.domain.resources.requests.memory\", \"rule\": \"integer\", \"message\": \"This VM requires more memory.\", \"min\": 1073741824 } ] spec: dataVolumeTemplates: - apiVersion: cdi.kubevirt.io/v1beta1 kind: DataVolume metadata: name: ${NAME} spec: pvc: accessModes: - ReadWriteMany resources: requests: storage: 30Gi source: pvc: name: ${SRC_PVC_NAME} namespace: ${SRC_PVC_NAMESPACE} running: false template: metadata: labels: kubevirt.io/domain: ${NAME} kubevirt.io/size: large spec: domain: cpu: sockets: 2 cores: 1 threads: 1 resources: requests: memory: 8Gi devices: rng: {} networkInterfaceMultiqueue: true inputs: - type: tablet bus: virtio name: tablet disks: - disk: bus: virtio name: ${NAME} - disk: bus: virtio name: cloudinitdisk interfaces: - masquerade: {} name: default terminationGracePeriodSeconds: 180 networks: - name: default pod: {} volumes: - dataVolume: name: ${NAME} name: ${NAME} - cloudInitNoCloud: userData: |- #cloud-config user: fedora password: ${CLOUD_USER_PASSWORD} chpasswd: { expire: False } name: cloudinitdisk parameters: - description: VM name from: 'fedora-[a-z0-9]{16}' generate: expression name: NAME - name: SRC_PVC_NAME description: Name of the PVC to clone value: 'fedora' - name: SRC_PVC_NAMESPACE description: Namespace of the source PVC value: kubevirt-os-images - description: Randomized password for the cloud-init user fedora from: '[a-z0-9]{4}-[a-z0-9]{4}-[a-z0-9]{4}' generate: expression name: CLOUD_USER_PASSWORD Note that the template above defines free parameters ( NAME , SRC_PVC_NAME , SRC_PVC_NAMESPACE , CLOUD_USER_PASSWORD ) and the NAME parameter does not have specified default value. An OpenShift template has to be converted into the JSON file via oc process command, that also allows you to set the template parameters. A complete example can be found in the KubeVirt repository . !> You need to be logged in by oc login command. $ oc process -f cluster/vmi-template-fedora.yaml\\ -p NAME=testvmi \\ -p SRC_PVC_NAME=fedora \\ -p SRC_PVC_NAMESPACE=kubevirt \\ { \"kind\": \"List\", \"apiVersion\": \"v1\", \"metadata\": {}, \"items\": [ { The JSON file is usually applied directly by piping the processed output to oc create command. $ oc process -f cluster/examples/vm-template-fedora.yaml \\ -p NAME=testvm \\ -p SRC_PVC_NAME=fedora \\ -p SRC_PVC_NAMESPACE=kubevirt \\ | oc create -f - virtualmachine.kubevirt.io/testvm created The command above results in creating a Kubernetes object according to the specification given by the template \\(in this example it is an instance of the VirtualMachine object\\). It's possible to get list of available parameters using the following command: $ oc process -f dist/templates/fedora-desktop-large.yaml --parameters NAME DESCRIPTION GENERATOR VALUE NAME VM name expression fedora-[a-z0-9]{16} SRC_PVC_NAME Name of the PVC to clone fedora SRC_PVC_NAMESPACE Namespace of the source PVC kubevirt-os-images CLOUD_USER_PASSWORD Randomized password for the cloud-init user fedora expression [a-z0-9]{4}-[a-z0-9]{4}-[a-z0-9]{4}","title":"Example template"},{"location":"virtual_machines/templates/#starting-virtual-machine-from-the-created-object","text":"The created object is now a regular VirtualMachine object and from now it can be controlled by accessing Kubernetes API resources. The preferred way how to do this from within the OpenShift environment is to use oc patch command. $ oc patch virtualmachine testvm --type merge -p '{\"spec\":{\"running\":true}}' virtualmachine.kubevirt.io/testvm patched Do not forget about virtctl tool. Using it in the real cases instead of using kubernetes API can be more convenient. Example: $ virtctl start testvm VM testvm was scheduled to start As soon as VM starts, Kubernetes creates new type of object - VirtualMachineInstance. It has similar name to VirtualMachine. Example (not full output, it's too big): $ kubectl describe vm testvm name: testvm Namespace: myproject Labels: kubevirt-vm=vm-testvm kubevirt.io/os=fedora33 Annotations: <none> API Version: kubevirt.io/v1alpha3 Kind: VirtualMachine","title":"Starting virtual machine from the created object"},{"location":"virtual_machines/templates/#cloud-init-script-and-parameters","text":"Kubevirt VM templates, just like kubevirt VM/VMI yaml configs, supports cloud-init scripts","title":"Cloud-init script and parameters"},{"location":"virtual_machines/templates/#hack-use-pre-downloaded-image","text":"Kubevirt VM templates, just like kubevirt VM/VMI yaml configs, can use pre-downloaded VM image, which can be a useful feature especially in the debug/development/testing cases. No special parameters required in the VM template or VM/VMI yaml config. The main idea is to create Kubernetes PersistentVolume and PersistentVolumeClaim corresponding to existing image in the file system. Example: --- kind: PersistentVolume apiVersion: v1 metadata: name: mypv labels: type: local spec: storageClassName: manual capacity: storage: 10G accessModes: - ReadWriteOnce hostPath: path: \"/mnt/sda1/images/testvm\" --- kind: PersistentVolumeClaim apiVersion: v1 metadata: name: mypvc spec: storageClassName: manual accessModes: - ReadWriteOnce resources: requests: storage: 10G","title":"Hack - use pre-downloaded image"},{"location":"virtual_machines/templates/#using-datavolumes","text":"Kubevirt VM templates are using dataVolumeTemplates. Before using dataVolumes, CDI has to be installed in cluster. After that, source Datavolume can be created. --- apiVersion: cdi.kubevirt.io/v1beta1 kind: DataVolume metadata: name: fedora-datavolume-original namespace: kubevirt spec: source: registry: url: \"image_url\" pvc: accessModes: - ReadWriteOnce resources: requests: storage: 30Gi After import is completed, VM can be created: $ oc process -f cluster/examples/vm-template-fedora.yaml \\ -p NAME=testvmi \\ -p SRC_PVC_NAME=fedora-datavolume-original \\ -p SRC_PVC_NAMESPACE=kubevirt \\ | oc create -f - virtualmachine.kubevirt.io/testvm created","title":"Using DataVolumes"},{"location":"virtual_machines/templates/#additional-information","text":"You can follow Virtual Machine Lifecycle Guide for further reference.","title":"Additional information"},{"location":"virtual_machines/virtual_hardware/","text":"Virtual hardware \u00b6 Fine-tuning different aspects of the hardware which are not device related (BIOS, mainboard, etc.) is sometimes necessary to allow guest operating systems to properly boot and reboot. Machine Type \u00b6 QEMU is able to work with two different classes of chipsets for x86_64, so called machine types. The x86_64 chipsets are i440fx (also called pc) and q35. They are versioned based on qemu-system-${ARCH}, following the format pc-${machine_type}-${qemu_version} , e.g. pc-i440fx-2.10 and pc-q35-2.10 . KubeVirt defaults to QEMU's newest q35 machine type. If a custom machine type is desired, it is configurable through the following structure: metadata: name: myvmi spec: domain: machine: # This value indicates QEMU machine type. type: pc-q35-2.10 resources: requests: memory: 512M devices: disks: - name: myimage disk: {} volumes: - name: myimage persistentVolumeClaim: claimName: myclaim Comparison of the machine types' internals can be found at QEMU wiki . BIOS/UEFI \u00b6 All virtual machines use BIOS by default for booting. It is possible to utilize UEFI/OVMF by setting a value via spec.firmware.bootloader : apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance metadata: labels: special: vmi-alpine-efi name: vmi-alpine-efi spec: domain: devices: disks: - disk: bus: virtio name: containerdisk firmware: # this sets the bootloader type bootloader: efi: {} SecureBoot is not yet supported. SMBIOS Firmware \u00b6 In order to provide a consistent view on the virtualized hardware for the guest OS, the SMBIOS UUID can be set to a constant value via spec.firmware.uuid : metadata: name: myvmi spec: domain: firmware: # this sets the UUID uuid: 5d307ca9-b3ef-428c-8861-06e72d69f223 serial: e4686d2c-6e8d-4335-b8fd-81bee22f4815 resources: requests: memory: 512M devices: disks: - name: myimage disk: {} volumes: - name: myimage persistentVolumeClaim: claimName: myclaim In addition, the SMBIOS serial number can be set to a constant value via spec.firmware.serial , as demonstrated above. CPU \u00b6 Note : This is not related to scheduling decisions or resource assignment. Topology \u00b6 Setting the number of CPU cores is possible via spec.domain.cpu.cores . The following VM will have a CPU with 3 cores: metadata: name: myvmi spec: domain: cpu: # this sets the cores cores: 3 resources: requests: memory: 512M devices: disks: - name: myimage disk: {} volumes: - name: myimage persistentVolumeClaim: claimName: myclaim Enabling cpu compatibility enforcement \u00b6 To enable the CPU compatibility enforcement, the CPUNodeDiscovery feature gates must be enabled in the KubeVirt CR. This feature-gate allows kubevirt to take VM cpu model and cpu features and create node selectors from them. With these node selectors, VM can be scheduled on the node which can support VM cpu model and features. Labeling nodes with cpu models and cpu features \u00b6 To properly label the node, user can use Kubevirt Node-labeller, which creates all necessary labels or create node labels by himself. Kubevirt node-labeller creates 3 types of labels: cpu models, cpu features and kvm info. It uses libvirt to get all supported cpu models and cpu features on host and then Node-labeller creates labels from cpu models. Kubevirt can then schedule VM on node which has support for VM cpu model and features. Node-labeller supports obsolete list of cpu models and minimal baseline cpu model for features. Both features can be set via KubeVirt CR: apiVersion: kubevirt.io/v1alpha3 kind: Kubevirt metadata: name: kubevirt namespace: kubevirt spec: ... configuration: minCPUModel: \"Penryn\" obsoleteCPUModels: - \"486\" - \"pentium\" ... Obsolete cpus will not be inserted in labels. If KubeVirt CR doesn't contain obsoleteCPUModels or minCPUModel variables, Labeller sets default values (for obsoleteCPUModels \"pentium, pentium2, pentium3, pentiumpro, coreduo, n270, core2duo, Conroe, athlon, phenom, kvm32, kvm64, qemu32, qemu64\" and for minCPUModel \"Penryn\"). In minCPU user can set baseline cpu model. CPU features, which have this model, are used as basic features. These basic features are not in the label list. Feature labels are created as subtraction between set of newer cpu features and set of basic cpu features, e.g.: Haswell has: aes, apic, clflush Penryr has: apic, clflush subtraction is: aes. So label will be created only with aes feature. User can change obsoleteCPUModels or minCPUModel by adding / removing cpu model in config map. Kubevirt then update nodes with new labels. Model \u00b6 Note : Be sure that node CPU model where you run a VM, has the same or higher CPU family. Note : If CPU model wasn't defined, the VM will have CPU model closest to one that used on the node where the VM is running. Note : CPU model is case sensitive. Setting the CPU model is possible via spec.domain.cpu.model . The following VM will have a CPU with the Conroe model: apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance metadata: name: myvmi spec: domain: cpu: # this sets the CPU model model: Conroe ... You can check list of available models here . When CPUNodeDiscovery feature-gate is enabled and VM has cpu model, Kubevirt creates node selector with format: cpu-model.node.kubevirt.io/<cpuModel> , e.g. cpu-model.node.kubevirt.io/Conroe . When VM doesn\u2019t have cpu model, then no node selector is created. Enabling default cluster cpu model \u00b6 To enable the default cpu model, user may add the cpuModel field in the KubeVirt CR. apiVersion: kubevirt.io/v1alpha3 kind: KubeVirt metadata: name: kubevirt namespace: kubevirt spec: ... configuration: cpuModel: \"EPYC\" ... Default CPU model is set when vmi doesn't have any cpu model. When vmi has cpu model set, then vmi's cpu model is preferred. When default cpu model is not set and vmi's cpu model is not set too, host-model will be set. Default cpu model can be changed when kubevirt is running. When CPUNodeDiscovery feature gate is enabled Kubevirt creates node selector with default cpu model. CPU model special cases \u00b6 As special cases you can set spec.domain.cpu.model equals to: - host-passthrough to passthrough CPU from the node to the VM metadata: name: myvmi spec: domain: cpu: # this passthrough the node CPU to the VM model: host-passthrough ... host-model to get CPU on the VM close to the node one metadata: name: myvmi spec: domain: cpu: # this set the VM CPU close to the node one model: host-model ... See the CPU API reference for more details. Features \u00b6 Setting CPU features is possible via spec.domain.cpu.features and can contain zero or more CPU features : metadata: name: myvmi spec: domain: cpu: # this sets the CPU features features: # this is the feature's name - name: \"apic\" # this is the feature's policy policy: \"require\" ... Note : Policy attribute can either be omitted or contain one of the following policies: force, require, optional, disable, forbid. Note : In case a policy is omitted for a feature, it will default to require . Behaviour according to Policies: All policies will be passed to libvirt during virtual machine creation. In case the feature gate \"CPUNodeDiscovery\" is enabled and the policy is omitted or has \"require\" value, then the virtual machine could be scheduled only on nodes that support this feature. In case the feature gate \"CPUNodeDiscovery\" is enabled and the policy has \"forbid\" value, then the virtual machine would not be scheduled on nodes that support this feature. Full description about features and policies can be found here . When CPUNodeDiscovery feature-gate is enabled Kubevirt creates node selector from cpu features with format: cpu-feature.node.kubevirt.io/<cpuFeature> , e.g. cpu-feature.node.kubevirt.io/apic . When VM doesn\u2019t have cpu feature, then no node selector is created. Clock \u00b6 Guest time \u00b6 Sets the virtualized hardware clock inside the VM to a specific time. Available options are utc timezone See the Clock API Reference for all possible configuration options. utc \u00b6 If utc is specified, the VM's clock will be set to UTC. metadata: name: myvmi spec: domain: clock: utc: {} resources: requests: memory: 512M devices: disks: - name: myimage disk: {} volumes: - name: myimage persistentVolumeClaim: claimName: myclaim timezone \u00b6 If timezone is specified, the VM's clock will be set to the specified local time. metadata: name: myvmi spec: domain: clock: timezone: \"America/New York\" resources: requests: memory: 512M devices: disks: - name: myimage disk: {} volumes: - name: myimage persistentVolumeClaim: claimName: myclaim Timers \u00b6 pit rtc kvm hyperv A pretty common timer configuration for VMs looks like this: metadata: name: myvmi spec: domain: clock: utc: {} # here are the timer timer: hpet: present: false pit: tickPolicy: delay rtc: tickPolicy: catchup hyperv: {} resources: requests: memory: 512M devices: disks: - name: myimage disk: {} volumes: - name: myimage persistentVolumeClaim: claimName: myclaim hpet is disabled, pit and rtc are configured to use a specific tickPolicy . Finally, hyperv is made available too. See the Timer API Reference for all possible configuration options. Note : Timer can be part of a machine type. Thus it may be necessary to explicitly disable them. We may in the future decide to add them via cluster-level defaulting, if they are part of a QEMU machine definition. Random number generator (RNG) \u00b6 You may want to use entropy collected by your cluster nodes inside your guest. KubeVirt allows to add a virtio RNG device to a virtual machine as follows. metadata: name: vmi-with-rng spec: domain: devices: rng: {} For Linux guests, the virtio-rng kernel module should be loaded early in the boot process to acquire access to the entropy source. Other systems may require similar adjustments to work with the virtio RNG device. Note : Some guest operating systems or user payloads may require the RNG device with enough entropy and may fail to boot without it. For example, fresh Fedora images with newer kernels (4.16.4+) may require the virtio RNG device to be present to boot to login. Video and Graphics Device \u00b6 By default a minimal Video and Graphics device configuration will be applied to the VirtualMachineInstance. The video device is vga compatible and comes with a memory size of 16 MB. This device allows connecting to the OS via vnc . It is possible not attach it by setting spec.domain.devices.autoattachGraphicsDevice to false : metadata: name: myvmi spec: domain: devices: autoattachGraphicsDevice: false disks: - name: myimage disk: {} volumes: - name: myimage persistentVolumeClaim: claimName: myclaim VMIs without graphics and video devices are very often referenced as headless VMIs. If using a huge amount of small VMs this can be helpful to increase the VMI density per node, since no memory needs to be reserved for video. Features \u00b6 KubeVirt supports a range of virtualization features which may be tweaked in order to allow non-Linux based operating systems to properly boot. Most noteworthy are acpi apic hyperv A common feature configuration is shown by the following example: apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance metadata: name: myvmi spec: domain: # typical features features: acpi: {} apic: {} hyperv: relaxed: {} vapic: {} spinlocks: spinlocks: 8191 resources: requests: memory: 512M devices: disks: - name: myimage disk: {} volumes: - name: myimage persistentVolumeClaim: claimname: myclaim See the Features API Reference for all available features and configuration options. Resources Requests and Limits \u00b6 An optional resource request can be specified by the users to allow the scheduler to make a better decision in finding the most suitable Node to place the VM. apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance metadata: name: myvmi spec: domain: resources: requests: memory: \"1Gi\" cpu: \"2\" limits: memory: \"2Gi\" cpu: \"1\" disks: - name: myimage disk: {} volumes: - name: myimage persistentVolumeClaim: claimname: myclaim CPU \u00b6 Specifying CPU limits will determine the amount of cpu shares set on the control group the VM is running in, in other words, the amount of time the VM's CPUs can execute on the assigned resources when there is a competition for CPU resources. For more information please refer to how Pods with resource limits are run . Memory Overhead \u00b6 Various VM resources, such as a video adapter, IOThreads, and supplementary system software, consume additional memory from the Node, beyond the requested memory intended for the guest OS consumption. In order to provide a better estimate for the scheduler, this memory overhead will be calculated and added to the requested memory. Please see how Pods with resource requests are scheduled for additional information on resource requests and limits. Hugepages \u00b6 KubeVirt give you possibility to use hugepages as backing memory for your VM. You will need to provide desired amount of memory resources.requests.memory and size of hugepages to use memory.hugepages.pageSize , for example for x86_64 architecture it can be 2Mi . apiVersion: kubevirt.io/v1alpha1 kind: VirtualMachine metadata: name: myvm spec: domain: resources: requests: memory: \"64Mi\" memory: hugepages: pageSize: \"2Mi\" disks: - name: myimage disk: {} volumes: - name: myimage persistentVolumeClaim: claimname: myclaim In the above example the VM will have 64Mi of memory, but instead of regular memory it will use node hugepages of the size of 2Mi . Limitations \u00b6 a node must have pre-allocated hugepages hugepages size cannot be bigger than requested memory requested memory must be divisible by hugepages size Input Devices \u00b6 Tablet \u00b6 Kubevirt supports input devices. The only type which is supported is tablet . Tablet input device supports only virtio and usb bus. Bus can be empty. In that case, usb will be selected. apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachine metadata: name: myvm spec: domain: devices: inputs: - type: tablet bus: virtio name: tablet1 disks: - name: myimage disk: {} volumes: - name: myimage persistentVolumeClaim: claimname: myclaim","title":"Virtual hardware"},{"location":"virtual_machines/virtual_hardware/#virtual-hardware","text":"Fine-tuning different aspects of the hardware which are not device related (BIOS, mainboard, etc.) is sometimes necessary to allow guest operating systems to properly boot and reboot.","title":"Virtual hardware"},{"location":"virtual_machines/virtual_hardware/#machine-type","text":"QEMU is able to work with two different classes of chipsets for x86_64, so called machine types. The x86_64 chipsets are i440fx (also called pc) and q35. They are versioned based on qemu-system-${ARCH}, following the format pc-${machine_type}-${qemu_version} , e.g. pc-i440fx-2.10 and pc-q35-2.10 . KubeVirt defaults to QEMU's newest q35 machine type. If a custom machine type is desired, it is configurable through the following structure: metadata: name: myvmi spec: domain: machine: # This value indicates QEMU machine type. type: pc-q35-2.10 resources: requests: memory: 512M devices: disks: - name: myimage disk: {} volumes: - name: myimage persistentVolumeClaim: claimName: myclaim Comparison of the machine types' internals can be found at QEMU wiki .","title":"Machine Type"},{"location":"virtual_machines/virtual_hardware/#biosuefi","text":"All virtual machines use BIOS by default for booting. It is possible to utilize UEFI/OVMF by setting a value via spec.firmware.bootloader : apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance metadata: labels: special: vmi-alpine-efi name: vmi-alpine-efi spec: domain: devices: disks: - disk: bus: virtio name: containerdisk firmware: # this sets the bootloader type bootloader: efi: {} SecureBoot is not yet supported.","title":"BIOS/UEFI"},{"location":"virtual_machines/virtual_hardware/#smbios-firmware","text":"In order to provide a consistent view on the virtualized hardware for the guest OS, the SMBIOS UUID can be set to a constant value via spec.firmware.uuid : metadata: name: myvmi spec: domain: firmware: # this sets the UUID uuid: 5d307ca9-b3ef-428c-8861-06e72d69f223 serial: e4686d2c-6e8d-4335-b8fd-81bee22f4815 resources: requests: memory: 512M devices: disks: - name: myimage disk: {} volumes: - name: myimage persistentVolumeClaim: claimName: myclaim In addition, the SMBIOS serial number can be set to a constant value via spec.firmware.serial , as demonstrated above.","title":"SMBIOS Firmware"},{"location":"virtual_machines/virtual_hardware/#cpu","text":"Note : This is not related to scheduling decisions or resource assignment.","title":"CPU"},{"location":"virtual_machines/virtual_hardware/#topology","text":"Setting the number of CPU cores is possible via spec.domain.cpu.cores . The following VM will have a CPU with 3 cores: metadata: name: myvmi spec: domain: cpu: # this sets the cores cores: 3 resources: requests: memory: 512M devices: disks: - name: myimage disk: {} volumes: - name: myimage persistentVolumeClaim: claimName: myclaim","title":"Topology"},{"location":"virtual_machines/virtual_hardware/#enabling-cpu-compatibility-enforcement","text":"To enable the CPU compatibility enforcement, the CPUNodeDiscovery feature gates must be enabled in the KubeVirt CR. This feature-gate allows kubevirt to take VM cpu model and cpu features and create node selectors from them. With these node selectors, VM can be scheduled on the node which can support VM cpu model and features.","title":"Enabling cpu compatibility enforcement"},{"location":"virtual_machines/virtual_hardware/#labeling-nodes-with-cpu-models-and-cpu-features","text":"To properly label the node, user can use Kubevirt Node-labeller, which creates all necessary labels or create node labels by himself. Kubevirt node-labeller creates 3 types of labels: cpu models, cpu features and kvm info. It uses libvirt to get all supported cpu models and cpu features on host and then Node-labeller creates labels from cpu models. Kubevirt can then schedule VM on node which has support for VM cpu model and features. Node-labeller supports obsolete list of cpu models and minimal baseline cpu model for features. Both features can be set via KubeVirt CR: apiVersion: kubevirt.io/v1alpha3 kind: Kubevirt metadata: name: kubevirt namespace: kubevirt spec: ... configuration: minCPUModel: \"Penryn\" obsoleteCPUModels: - \"486\" - \"pentium\" ... Obsolete cpus will not be inserted in labels. If KubeVirt CR doesn't contain obsoleteCPUModels or minCPUModel variables, Labeller sets default values (for obsoleteCPUModels \"pentium, pentium2, pentium3, pentiumpro, coreduo, n270, core2duo, Conroe, athlon, phenom, kvm32, kvm64, qemu32, qemu64\" and for minCPUModel \"Penryn\"). In minCPU user can set baseline cpu model. CPU features, which have this model, are used as basic features. These basic features are not in the label list. Feature labels are created as subtraction between set of newer cpu features and set of basic cpu features, e.g.: Haswell has: aes, apic, clflush Penryr has: apic, clflush subtraction is: aes. So label will be created only with aes feature. User can change obsoleteCPUModels or minCPUModel by adding / removing cpu model in config map. Kubevirt then update nodes with new labels.","title":"Labeling nodes with cpu models and cpu features"},{"location":"virtual_machines/virtual_hardware/#model","text":"Note : Be sure that node CPU model where you run a VM, has the same or higher CPU family. Note : If CPU model wasn't defined, the VM will have CPU model closest to one that used on the node where the VM is running. Note : CPU model is case sensitive. Setting the CPU model is possible via spec.domain.cpu.model . The following VM will have a CPU with the Conroe model: apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance metadata: name: myvmi spec: domain: cpu: # this sets the CPU model model: Conroe ... You can check list of available models here . When CPUNodeDiscovery feature-gate is enabled and VM has cpu model, Kubevirt creates node selector with format: cpu-model.node.kubevirt.io/<cpuModel> , e.g. cpu-model.node.kubevirt.io/Conroe . When VM doesn\u2019t have cpu model, then no node selector is created.","title":"Model"},{"location":"virtual_machines/virtual_hardware/#enabling-default-cluster-cpu-model","text":"To enable the default cpu model, user may add the cpuModel field in the KubeVirt CR. apiVersion: kubevirt.io/v1alpha3 kind: KubeVirt metadata: name: kubevirt namespace: kubevirt spec: ... configuration: cpuModel: \"EPYC\" ... Default CPU model is set when vmi doesn't have any cpu model. When vmi has cpu model set, then vmi's cpu model is preferred. When default cpu model is not set and vmi's cpu model is not set too, host-model will be set. Default cpu model can be changed when kubevirt is running. When CPUNodeDiscovery feature gate is enabled Kubevirt creates node selector with default cpu model.","title":"Enabling default cluster cpu model"},{"location":"virtual_machines/virtual_hardware/#cpu-model-special-cases","text":"As special cases you can set spec.domain.cpu.model equals to: - host-passthrough to passthrough CPU from the node to the VM metadata: name: myvmi spec: domain: cpu: # this passthrough the node CPU to the VM model: host-passthrough ... host-model to get CPU on the VM close to the node one metadata: name: myvmi spec: domain: cpu: # this set the VM CPU close to the node one model: host-model ... See the CPU API reference for more details.","title":"CPU model special cases"},{"location":"virtual_machines/virtual_hardware/#features","text":"Setting CPU features is possible via spec.domain.cpu.features and can contain zero or more CPU features : metadata: name: myvmi spec: domain: cpu: # this sets the CPU features features: # this is the feature's name - name: \"apic\" # this is the feature's policy policy: \"require\" ... Note : Policy attribute can either be omitted or contain one of the following policies: force, require, optional, disable, forbid. Note : In case a policy is omitted for a feature, it will default to require . Behaviour according to Policies: All policies will be passed to libvirt during virtual machine creation. In case the feature gate \"CPUNodeDiscovery\" is enabled and the policy is omitted or has \"require\" value, then the virtual machine could be scheduled only on nodes that support this feature. In case the feature gate \"CPUNodeDiscovery\" is enabled and the policy has \"forbid\" value, then the virtual machine would not be scheduled on nodes that support this feature. Full description about features and policies can be found here . When CPUNodeDiscovery feature-gate is enabled Kubevirt creates node selector from cpu features with format: cpu-feature.node.kubevirt.io/<cpuFeature> , e.g. cpu-feature.node.kubevirt.io/apic . When VM doesn\u2019t have cpu feature, then no node selector is created.","title":"Features"},{"location":"virtual_machines/virtual_hardware/#clock","text":"","title":"Clock"},{"location":"virtual_machines/virtual_hardware/#guest-time","text":"Sets the virtualized hardware clock inside the VM to a specific time. Available options are utc timezone See the Clock API Reference for all possible configuration options.","title":"Guest time"},{"location":"virtual_machines/virtual_hardware/#utc","text":"If utc is specified, the VM's clock will be set to UTC. metadata: name: myvmi spec: domain: clock: utc: {} resources: requests: memory: 512M devices: disks: - name: myimage disk: {} volumes: - name: myimage persistentVolumeClaim: claimName: myclaim","title":"utc"},{"location":"virtual_machines/virtual_hardware/#timezone","text":"If timezone is specified, the VM's clock will be set to the specified local time. metadata: name: myvmi spec: domain: clock: timezone: \"America/New York\" resources: requests: memory: 512M devices: disks: - name: myimage disk: {} volumes: - name: myimage persistentVolumeClaim: claimName: myclaim","title":"timezone"},{"location":"virtual_machines/virtual_hardware/#timers","text":"pit rtc kvm hyperv A pretty common timer configuration for VMs looks like this: metadata: name: myvmi spec: domain: clock: utc: {} # here are the timer timer: hpet: present: false pit: tickPolicy: delay rtc: tickPolicy: catchup hyperv: {} resources: requests: memory: 512M devices: disks: - name: myimage disk: {} volumes: - name: myimage persistentVolumeClaim: claimName: myclaim hpet is disabled, pit and rtc are configured to use a specific tickPolicy . Finally, hyperv is made available too. See the Timer API Reference for all possible configuration options. Note : Timer can be part of a machine type. Thus it may be necessary to explicitly disable them. We may in the future decide to add them via cluster-level defaulting, if they are part of a QEMU machine definition.","title":"Timers"},{"location":"virtual_machines/virtual_hardware/#random-number-generator-rng","text":"You may want to use entropy collected by your cluster nodes inside your guest. KubeVirt allows to add a virtio RNG device to a virtual machine as follows. metadata: name: vmi-with-rng spec: domain: devices: rng: {} For Linux guests, the virtio-rng kernel module should be loaded early in the boot process to acquire access to the entropy source. Other systems may require similar adjustments to work with the virtio RNG device. Note : Some guest operating systems or user payloads may require the RNG device with enough entropy and may fail to boot without it. For example, fresh Fedora images with newer kernels (4.16.4+) may require the virtio RNG device to be present to boot to login.","title":"Random number generator (RNG)"},{"location":"virtual_machines/virtual_hardware/#video-and-graphics-device","text":"By default a minimal Video and Graphics device configuration will be applied to the VirtualMachineInstance. The video device is vga compatible and comes with a memory size of 16 MB. This device allows connecting to the OS via vnc . It is possible not attach it by setting spec.domain.devices.autoattachGraphicsDevice to false : metadata: name: myvmi spec: domain: devices: autoattachGraphicsDevice: false disks: - name: myimage disk: {} volumes: - name: myimage persistentVolumeClaim: claimName: myclaim VMIs without graphics and video devices are very often referenced as headless VMIs. If using a huge amount of small VMs this can be helpful to increase the VMI density per node, since no memory needs to be reserved for video.","title":"Video and Graphics Device"},{"location":"virtual_machines/virtual_hardware/#features_1","text":"KubeVirt supports a range of virtualization features which may be tweaked in order to allow non-Linux based operating systems to properly boot. Most noteworthy are acpi apic hyperv A common feature configuration is shown by the following example: apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance metadata: name: myvmi spec: domain: # typical features features: acpi: {} apic: {} hyperv: relaxed: {} vapic: {} spinlocks: spinlocks: 8191 resources: requests: memory: 512M devices: disks: - name: myimage disk: {} volumes: - name: myimage persistentVolumeClaim: claimname: myclaim See the Features API Reference for all available features and configuration options.","title":"Features"},{"location":"virtual_machines/virtual_hardware/#resources-requests-and-limits","text":"An optional resource request can be specified by the users to allow the scheduler to make a better decision in finding the most suitable Node to place the VM. apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance metadata: name: myvmi spec: domain: resources: requests: memory: \"1Gi\" cpu: \"2\" limits: memory: \"2Gi\" cpu: \"1\" disks: - name: myimage disk: {} volumes: - name: myimage persistentVolumeClaim: claimname: myclaim","title":"Resources Requests and Limits"},{"location":"virtual_machines/virtual_hardware/#cpu_1","text":"Specifying CPU limits will determine the amount of cpu shares set on the control group the VM is running in, in other words, the amount of time the VM's CPUs can execute on the assigned resources when there is a competition for CPU resources. For more information please refer to how Pods with resource limits are run .","title":"CPU"},{"location":"virtual_machines/virtual_hardware/#memory-overhead","text":"Various VM resources, such as a video adapter, IOThreads, and supplementary system software, consume additional memory from the Node, beyond the requested memory intended for the guest OS consumption. In order to provide a better estimate for the scheduler, this memory overhead will be calculated and added to the requested memory. Please see how Pods with resource requests are scheduled for additional information on resource requests and limits.","title":"Memory Overhead"},{"location":"virtual_machines/virtual_hardware/#hugepages","text":"KubeVirt give you possibility to use hugepages as backing memory for your VM. You will need to provide desired amount of memory resources.requests.memory and size of hugepages to use memory.hugepages.pageSize , for example for x86_64 architecture it can be 2Mi . apiVersion: kubevirt.io/v1alpha1 kind: VirtualMachine metadata: name: myvm spec: domain: resources: requests: memory: \"64Mi\" memory: hugepages: pageSize: \"2Mi\" disks: - name: myimage disk: {} volumes: - name: myimage persistentVolumeClaim: claimname: myclaim In the above example the VM will have 64Mi of memory, but instead of regular memory it will use node hugepages of the size of 2Mi .","title":"Hugepages"},{"location":"virtual_machines/virtual_hardware/#limitations","text":"a node must have pre-allocated hugepages hugepages size cannot be bigger than requested memory requested memory must be divisible by hugepages size","title":"Limitations"},{"location":"virtual_machines/virtual_hardware/#input-devices","text":"","title":"Input Devices"},{"location":"virtual_machines/virtual_hardware/#tablet","text":"Kubevirt supports input devices. The only type which is supported is tablet . Tablet input device supports only virtio and usb bus. Bus can be empty. In that case, usb will be selected. apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachine metadata: name: myvm spec: domain: devices: inputs: - type: tablet bus: virtio name: tablet1 disks: - name: myimage disk: {} volumes: - name: myimage persistentVolumeClaim: claimname: myclaim","title":"Tablet"},{"location":"virtual_machines/virtual_machine_instances/","text":"Virtual Machines Instances \u00b6 The VirtualMachineInstance type conceptionally has two parts: Information for making scheduling decisions Information about the virtual machine ABI Every VirtualMachineInstance object represents a single running virtual machine instance. API Overview \u00b6 With the installation of KubeVirt, new types are added to the Kubernetes API to manage Virtual Machines. You can interact with the new resources (via kubectl ) as you would with any other API resource. VirtualMachineInstance API \u00b6 Note: A full API reference is available at https://kubevirt.io/api-reference/ . Here is an example of a VirtualMachineInstance object: apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance metadata: name: testvmi-nocloud spec: terminationGracePeriodSeconds: 30 domain: resources: requests: memory: 1024M devices: disks: - name: containerdisk disk: bus: virtio - name: emptydisk disk: bus: virtio - disk: bus: virtio name: cloudinitdisk volumes: - name: containerdisk containerDisk: image: kubevirt/fedora-cloud-container-disk-demo:latest - name: emptydisk emptyDisk: capacity: \"2Gi\" - name: cloudinitdisk cloudInitNoCloud: userData: |- #cloud-config password: fedora chpasswd: { expire: False } This example uses a fedora cloud image in combination with cloud-init and an ephemeral empty disk with a capacity of 2Gi . For the sake of simplicity, the volume sources in this example are ephemeral and don't require a provisioner in your cluster. Additional Information \u00b6 More information about persistent and ephemeral volumes: Disks and Volumes How to access a VirtualMachineInstance via console or vnc : Console Access How to customize VirtualMachineInstances with cloud-init : Cloud Init","title":"Virtual Machines Instances"},{"location":"virtual_machines/virtual_machine_instances/#virtual-machines-instances","text":"The VirtualMachineInstance type conceptionally has two parts: Information for making scheduling decisions Information about the virtual machine ABI Every VirtualMachineInstance object represents a single running virtual machine instance.","title":"Virtual Machines Instances"},{"location":"virtual_machines/virtual_machine_instances/#api-overview","text":"With the installation of KubeVirt, new types are added to the Kubernetes API to manage Virtual Machines. You can interact with the new resources (via kubectl ) as you would with any other API resource.","title":"API Overview"},{"location":"virtual_machines/virtual_machine_instances/#virtualmachineinstance-api","text":"Note: A full API reference is available at https://kubevirt.io/api-reference/ . Here is an example of a VirtualMachineInstance object: apiVersion: kubevirt.io/v1alpha3 kind: VirtualMachineInstance metadata: name: testvmi-nocloud spec: terminationGracePeriodSeconds: 30 domain: resources: requests: memory: 1024M devices: disks: - name: containerdisk disk: bus: virtio - name: emptydisk disk: bus: virtio - disk: bus: virtio name: cloudinitdisk volumes: - name: containerdisk containerDisk: image: kubevirt/fedora-cloud-container-disk-demo:latest - name: emptydisk emptyDisk: capacity: \"2Gi\" - name: cloudinitdisk cloudInitNoCloud: userData: |- #cloud-config password: fedora chpasswd: { expire: False } This example uses a fedora cloud image in combination with cloud-init and an ephemeral empty disk with a capacity of 2Gi . For the sake of simplicity, the volume sources in this example are ephemeral and don't require a provisioner in your cluster.","title":"VirtualMachineInstance API"},{"location":"virtual_machines/virtual_machine_instances/#additional-information","text":"More information about persistent and ephemeral volumes: Disks and Volumes How to access a VirtualMachineInstance via console or vnc : Console Access How to customize VirtualMachineInstances with cloud-init : Cloud Init","title":"Additional Information"},{"location":"virtual_machines/windows_virtio_drivers/","text":"Windows virtio drivers \u00b6 Purpose of this document is to explain how to install virtio drivers for Microsoft Windows running in a fully virtualized guest. Do I need virtio drivers? \u00b6 Yes. Without the virtio drivers, you cannot use paravirtualized hardware properly. It would either not work, or will have a severe performance penalty. For more information about VirtIO and paravirtualization, see VirtIO and paravirtualization For more details on configuring your VirtIO driver please refer to Installing VirtIO driver on a new Windows virtual machine and Installing VirtIO driver on an existing Windows virtual machine . Which drivers I need to install? \u00b6 There are usually up to 8 possible devices that are required to run Windows smoothly in a virtualized environment. KubeVirt currently supports only: viostor , the block driver, applies to SCSI Controller in the Other devices group. viorng , the entropy source driver, applies to PCI Device in the Other devices group. NetKVM , the network driver, applies to Ethernet Controller in the Other devices group. Available only if a virtio NIC is configured. Other virtio drivers, that exists and might be supported in the future: Balloon, the balloon driver, applies to PCI Device in the Other devices group vioserial, the paravirtual serial driver, applies to PCI Simple Communications Controller in the Other devices group. vioscsi, the SCSI block driver, applies to SCSI Controller in the Other devices group. qemupciserial, the emulated PCI serial driver, applies to PCI Serial Port in the Other devices group. qxl, the paravirtual video driver, applied to Microsoft Basic Display Adapter in the Display adapters group. pvpanic, the paravirtual panic driver, applies to Unknown device in the Other devices group. Note Some drivers are required in the installation phase. When you are installing Windows onto the virtio block storage you have to provide an appropriate virtio driver. Namely, choose viostor driver for your version of Microsoft Windows, eg. does not install XP driver when you run Windows 10. Other drivers can be installed after the successful windows installation. Again, please install only drivers matching your Windows version. How to install during Windows install? \u00b6 To install drivers before the Windows starts its install, make sure you have virtio-win package attached to your VirtualMachine as SATA CD-ROM. In the Windows installation, choose advanced install and load driver. Then please navigate to loaded Virtio CD-ROM and install one of viostor or vioscsi, depending on whichever you have set up. Step by step screenshots: How to install after Windows install? \u00b6 After windows install, please go to Device Manager . There you should see undetected devices in \"available devices\" section. You can install virtio drivers one by one going through this list. For more details on how to choose a proper driver and how to install the driver, please refer to the Windows Guest Virtual Machines on Red Hat Enterprise Linux 7 . How to obtain virtio drivers? \u00b6 The virtio Windows drivers are distributed in a form of containerDisk , which can be simply mounted to the VirtualMachine. The container image, containing the disk is located at: https://hub.docker.com/r/kubevirt/virtio-container-disk and the image be pulled as any other docker container: docker pull kubevirt/virtio-container-disk However, pulling image manually is not required, it will be downloaded if not present by Kubernetes when deploying VirtualMachine. Attaching to VirtualMachine \u00b6 KubeVirt distributes virtio drivers for Microsoft Windows in a form of container disk. The package contains the virtio drivers and QEMU guest agent. The disk was tested on Microsoft Windows Server 2012. Supported Windows version is XP and up. The package is intended to be used as CD-ROM attached to the virtual machine with Microsoft Windows. It can be used as SATA CDROM during install phase or to provide drivers in an existing Windows installation. Attaching the virtio-win package can be done simply by adding ContainerDisk to you VirtualMachine. spec: domain: devices: disks: - name: virtiocontainerdisk # Any other disk you want to use, must go before virtioContainerDisk. # KubeVirt boots from disks in order ther are defined. # Therefore virtioContainerDisk, must be after bootable disk. # Other option is to choose boot order explicitly: # - https://kubevirt.io/api-reference/v0.13.2/definitions.html#_v1_disk # NOTE: You either specify bootOrder explicitely or sort the items in # disks. You can not do both at the same time. # bootOrder: 2 cdrom: bus: sata volumes: - containerDisk: image: kubevirt/virtio-container-disk name: virtiocontainerdisk Once you are done installing virtio drivers, you can remove virtio container disk by simply removing the disk from yaml specification and restarting the VirtualMachine.","title":"Windows virtio drivers"},{"location":"virtual_machines/windows_virtio_drivers/#windows-virtio-drivers","text":"Purpose of this document is to explain how to install virtio drivers for Microsoft Windows running in a fully virtualized guest.","title":"Windows virtio drivers"},{"location":"virtual_machines/windows_virtio_drivers/#do-i-need-virtio-drivers","text":"Yes. Without the virtio drivers, you cannot use paravirtualized hardware properly. It would either not work, or will have a severe performance penalty. For more information about VirtIO and paravirtualization, see VirtIO and paravirtualization For more details on configuring your VirtIO driver please refer to Installing VirtIO driver on a new Windows virtual machine and Installing VirtIO driver on an existing Windows virtual machine .","title":"Do I need virtio drivers?"},{"location":"virtual_machines/windows_virtio_drivers/#which-drivers-i-need-to-install","text":"There are usually up to 8 possible devices that are required to run Windows smoothly in a virtualized environment. KubeVirt currently supports only: viostor , the block driver, applies to SCSI Controller in the Other devices group. viorng , the entropy source driver, applies to PCI Device in the Other devices group. NetKVM , the network driver, applies to Ethernet Controller in the Other devices group. Available only if a virtio NIC is configured. Other virtio drivers, that exists and might be supported in the future: Balloon, the balloon driver, applies to PCI Device in the Other devices group vioserial, the paravirtual serial driver, applies to PCI Simple Communications Controller in the Other devices group. vioscsi, the SCSI block driver, applies to SCSI Controller in the Other devices group. qemupciserial, the emulated PCI serial driver, applies to PCI Serial Port in the Other devices group. qxl, the paravirtual video driver, applied to Microsoft Basic Display Adapter in the Display adapters group. pvpanic, the paravirtual panic driver, applies to Unknown device in the Other devices group. Note Some drivers are required in the installation phase. When you are installing Windows onto the virtio block storage you have to provide an appropriate virtio driver. Namely, choose viostor driver for your version of Microsoft Windows, eg. does not install XP driver when you run Windows 10. Other drivers can be installed after the successful windows installation. Again, please install only drivers matching your Windows version.","title":"Which drivers I need to install?"},{"location":"virtual_machines/windows_virtio_drivers/#how-to-install-during-windows-install","text":"To install drivers before the Windows starts its install, make sure you have virtio-win package attached to your VirtualMachine as SATA CD-ROM. In the Windows installation, choose advanced install and load driver. Then please navigate to loaded Virtio CD-ROM and install one of viostor or vioscsi, depending on whichever you have set up. Step by step screenshots:","title":"How to install during Windows install?"},{"location":"virtual_machines/windows_virtio_drivers/#how-to-install-after-windows-install","text":"After windows install, please go to Device Manager . There you should see undetected devices in \"available devices\" section. You can install virtio drivers one by one going through this list. For more details on how to choose a proper driver and how to install the driver, please refer to the Windows Guest Virtual Machines on Red Hat Enterprise Linux 7 .","title":"How to install after Windows install?"},{"location":"virtual_machines/windows_virtio_drivers/#how-to-obtain-virtio-drivers","text":"The virtio Windows drivers are distributed in a form of containerDisk , which can be simply mounted to the VirtualMachine. The container image, containing the disk is located at: https://hub.docker.com/r/kubevirt/virtio-container-disk and the image be pulled as any other docker container: docker pull kubevirt/virtio-container-disk However, pulling image manually is not required, it will be downloaded if not present by Kubernetes when deploying VirtualMachine.","title":"How to obtain virtio drivers?"},{"location":"virtual_machines/windows_virtio_drivers/#attaching-to-virtualmachine","text":"KubeVirt distributes virtio drivers for Microsoft Windows in a form of container disk. The package contains the virtio drivers and QEMU guest agent. The disk was tested on Microsoft Windows Server 2012. Supported Windows version is XP and up. The package is intended to be used as CD-ROM attached to the virtual machine with Microsoft Windows. It can be used as SATA CDROM during install phase or to provide drivers in an existing Windows installation. Attaching the virtio-win package can be done simply by adding ContainerDisk to you VirtualMachine. spec: domain: devices: disks: - name: virtiocontainerdisk # Any other disk you want to use, must go before virtioContainerDisk. # KubeVirt boots from disks in order ther are defined. # Therefore virtioContainerDisk, must be after bootable disk. # Other option is to choose boot order explicitly: # - https://kubevirt.io/api-reference/v0.13.2/definitions.html#_v1_disk # NOTE: You either specify bootOrder explicitely or sort the items in # disks. You can not do both at the same time. # bootOrder: 2 cdrom: bus: sata volumes: - containerDisk: image: kubevirt/virtio-container-disk name: virtiocontainerdisk Once you are done installing virtio drivers, you can remove virtio container disk by simply removing the disk from yaml specification and restarting the VirtualMachine.","title":"Attaching to VirtualMachine"}]}